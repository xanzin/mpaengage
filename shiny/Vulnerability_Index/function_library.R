#' Name: function_library.R
#' Purpose: This scripts stores some common functions used across the project.
#' Author: Juan Luis Herrera Cortijo (juan.luis.herrera.cortijo@gmail.com)
#' Date Created: 29/06/2020
#' Last update: 29/06/2020


#### LOAD PACKAGES AND INSTALL IF NEEDED ####

# 
# version
# _                           
# platform       x86_64-apple-darwin15.6.0   
# arch           x86_64                      
# os             darwin15.6.0                
# system         x86_64, darwin15.6.0        
# status                                     
# major          3                           
# minor          6.3                         
# year           2020                        
# month          02                          
# day            29                          
# svn rev        77875                       
# language       R                           
# version.string R version 3.6.3 (2020-02-29)
# nickname       Holding the Windsock        


if(!require(yaml)){
  install.packages("yaml",dependencies = TRUE,repos='http://cran.us.r-project.org')
}


require(yaml)
packageVersion("yaml")
# 2.2.1


if(!require(ipc)){
  install.packages("ipc",dependencies = TRUE,repos='http://cran.us.r-project.org')
}


require(ipc)
packageVersion("ipc")
# 1.18.0

if(!require(promises)){
  install.packages("promises",dependencies = TRUE,repos='http://cran.us.r-project.org')
}


require(promises)
packageVersion("promises")
# 1.1.1

if(!require(future)){
  install.packages("future",dependencies = TRUE,repos='http://cran.us.r-project.org')
}


require(future)
packageVersion("future")
# 1.18.0

# plan(multiprocess)


if(!require(zip)){
  install.packages("zip",dependencies = TRUE,repos='http://cran.us.r-project.org')
}


require(zip)
packageVersion("zip")
# 0.5.10

if(!require(extrafont)){
  install.packages("extrafont",dependencies = TRUE,repos='http://cran.us.r-project.org')
}


require(extrafont)
packageVersion("extrafont")
# 0.5.10
# if(!"Calibri" %in% fonts()){
#   font_import(pattern="Calibri",prompt = FALSE)
# }

# loadfonts(device="pdf")
if(!require(flextable)){
  install.packages("flextable",dependencies = TRUE,repos='http://cran.us.r-project.org')
}


require(flextable)
packageVersion("flextable")
# 0.5.10

if(!require(officer)){
  install.packages("officer",dependencies = TRUE,repos='http://cran.us.r-project.org')
}


require(officer)
packageVersion("officer")
# 0.3.14



if(!require(shinycssloaders)){
  install.packages("shinycssloaders",dependencies = TRUE,repos='http://cran.us.r-project.org')
}


require(shinycssloaders)
packageVersion("shinycssloaders")
# 1.0.0


if(!require(scales)){
  install.packages("scales",dependencies = TRUE,repos='http://cran.us.r-project.org')
}


require(scales)
packageVersion("scales")
# 1.1.1
if(!require(gridExtra)){
  install.packages("gridExtra",dependencies = TRUE,repos='http://cran.us.r-project.org')
}


require(gridExtra)
packageVersion("gridExtra")
# 2.3

if(!require(grid)){
  install.packages("grid",dependencies = TRUE,repos='http://cran.us.r-project.org')
}


require(grid)
packageVersion("grid")
# 4.0.2

if(!require(ggforce)){
  install.packages("ggforce",dependencies = TRUE,repos='http://cran.us.r-project.org')
}


require(ggforce)
packageVersion("ggforce")
# 1.1.1

if(!require(ggimage)){
  install.packages("ggimage",dependencies = TRUE,repos='http://cran.us.r-project.org')
}


require(ggimage)
packageVersion("ggimage")
# '0.2.8

if(!require(lubridate)){
  install.packages("lubridate",dependencies = TRUE,repos='http://cran.us.r-project.org')
}


require(lubridate)
packageVersion("lubridate")
# 1.7.9


if(!require(readxl)){
  install.packages("readxl",dependencies = TRUE,repos='http://cran.us.r-project.org')
}


require(readxl)
packageVersion("readxl")
# 1.3.1

if(!require(rlang)){
  install.packages("rlang",dependencies = TRUE,repos='http://cran.us.r-project.org')
}


require(rlang)
packageVersion("rlang")
# 0.4.7

if(!require(magrittr)){
  install.packages("magrittr",dependencies = TRUE,repos='http://cran.us.r-project.org')
}


require(magrittr)
packageVersion("magrittr")
# 1.5

if(!require(tidyverse)){
  install.packages("tidyverse",dependencies = TRUE,repos='http://cran.us.r-project.org')
}


require(tidyverse)
packageVersion("tidyverse")
# ── Attaching packages ────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.3.0 ──
# ✓ ggplot2 3.3.2     ✓ purrr   0.3.4
# ✓ tibble  3.0.3     ✓ dplyr   1.0.2
# ✓ tidyr   1.1.1     ✓ stringr 1.4.0
# ✓ readr   1.3.1     ✓ forcats 0.5.0


#### DEFINITIONS AND VALIDATIONS ####


#' column.definition: named list of lists. The names refer to the columns of a data.frame that will
#' be used in computing a new column. Each sublist includes the parameters required for such computations.
#' One parameter is scales which is a list of two vectors: scales_dep lists the scales that the column depends on,
#' scales_avg_ord lists the order from left to right in which the scales should be averaged out.
#' The remaining parameters will depend on the function applied.

#' list.of.column.definitions: named list of lists. The names represent the names of the 
#' new columns computed. Each sublist includes two named elements:
#' "column.definition" stores the column.definition used to compute the new column
#' "computing.function" stores the function applied to the column.definition.
#' "extra.parameters" other parameters passed to computing.function

#' model.structure: named list of lists. It has 4 levels depth: Dimension, Factor, Component,  Indicator and Scale.
#' Dimension, Factor, Component and Indicators levels are lists of lists, Scales are vectors of scale names relevant to compute the indicator.

#' factor.value.equivalences: named list the names are the indicator names, each list element is a 
#' named vector of numeric values The names of the vector are the factor levels and the values are the numeric values
#' assigned to each factor

#' model.data: data.frame with columns MPA, HB, SP, HZ, UG, and one numeric column per indicator

#' normalization.definitions.data.frame: data.frame with at least columns
#' name (character)
#' min (numeric)
#' max (numeric)

#' quality.indicator.display.definition: list with indicator display properties:
#' list(
#' title=list(value=(character),text_properties=list(geom_text properties passed to the function)),
#' subtitle=list(value=(character),text_properties=list(geom_text properties passed to the function)),
#' indicator=list(value=(character),text_properties=list(geom_text properties passed to the function)),
#' circle_color=(character)
#' )


quality.indicator.display.definition.constructor <- function(.title_value=character(0),.title_properties=list(),
                                                             .subtitle_value=character(0),.subtitle_properties=list(),
                                                             .indicator_value=character(0),.indicator_properties=list(),
                                                             .circle_color=character(0))
  list(title=list(value=.title_value,text_properties=.title_properties),
       subtitle=list(value=.subtitle_value,text_properties=.subtitle_properties),
       indicator=list(value=.indicator_value,text_properties=.indicator_properties),
       circle_color=.circle_color)


#' Validates a normalization.definitions.data.frame
#' if not valid, throws an normalization.definitions.data.frame.not.valid.error
#' if valid, returns the .df
validate_normalization.definitions.data.frame <- function(.df){
  
  if(all(c("name","min","max") %in% names(.df))){
    
    normalization_column_classes <- list("name"="character","min"=c("numeric","integer"),"max"=c("numeric","integer"))
    
    column_classes_OK <- c("name","min","max") %>% map(~.df %>% pull(.x) %>% inherits(normalization_column_classes[[.x]])) %>% unlist()
    
    if(!all(column_classes_OK)){
      
      .columns_failed <- c("name","min","max")[!column_classes_OK]
      .column_clases_failed <- .columns_failed %>% map(~normalization_column_classes[[.x]] %>% first) %>% unlist
      
      .msg <- paste0("Column(s) ",paste0(.columns_failed,collapse = ", ")," does not inherit from ",paste0(.column_clases_failed,collapse = ", "))
      .df <-  withRestarts(
        stop(normalization.definitions.data.frame_not_valid(.msg,"classes",.df)),
        fix_normalization.definitions.data.frame_columns_classes=function(.df){
          warning("Normalization table failed to validate, trying to recover by transforming the column clases")
          for(column_name in names(normalization_column_classes)){
            
            
            class(.df[,column_name]) <- normalization_column_classes[[column_name]][1]
            
          }
          
          return(.df)
        },
        use_value=function(x) {
          x
        },
        return_empty_normalization_data=function(.file){
          warning("Normalization failed to validate, using an empty table instead")
          empty_normalization_data
        }
        
      )
    }
    
  }else{
    .msg <- paste0("Columns ",setdiff(c("name","min","max"),names(.df))," not found")
    .df <- withRestarts(
      stop(normalization.definitions.data.frame_not_valid(.msg,"columns",.df)),
      fix_normalization.definitions.data.frame_columns=function(.df){
        warning("Normalization table failed to validate, trying to recover by fixing the column names")
        .df_names <- names(.df)  
        
        .df_names <-str_squish(tolower(.df_names))
        
        names(.df) <- .df_names
        
        if(all(c("name","min","max") %in% names(.df))){
          
          return(validate_normalization.definitions.data.frame(.df))
        }else{
          .msg <- paste0("Columns ",setdiff(c("name","min","max"),names(.df))," not found. Recovering failed.")
          
          stop(normalization.definitions.data.frame_columns_missing_error(.msg,"not recoverable",.df))
        }
        
        
        
      },
      use_value=function(x) x,
      return_empty_normalization_data=function(.file){
        warning("Normalization failed to validate, using an empty table instead")
        empty_normalization_data
      }
      
    )
    
  }
  
  return(.df)
}


#' Validates a data file and throws an error if something is wrong.
#' 
#' @param .data_file (character) path to the data file
#' @return (logic) TRUE if the file passed the validation process
validate_data_file <- function(.data_file){
  
  .out <- c()
  
  # For each scenario/sheet
  
  #Check that the scales columns are present
  
  # Check that the Values column is present
  
  # Check that the Year column is present
  
  # Check that the Quantitative column is present
  
  .out <- excel_sheets(.data_file)
  
  
  .out %<>% map(~{
    
    data <- read_excel(.data_file,.x)
    # missing_columns <- c("SP","HB","UG","HZ","Value","Year","Quantitative")
    # 
    # missing_columns <- setdiff(missing_columns,names(data))
    # 
    # if(length(missing_columns)>0){
    #   
    #   warning(data_file_column_missing(.data_file = .data_file,
    #                                    .columns = missing_columns,
    #                                    .scenario = .x))
    #   NULL
    # }else{
    #   .x
    # }
    
  } ) %>% compact() %>% unlist
  
  .out
}

#### MODEL FUNCTIONS ####

#' This function computes one new column from a column definition and 
#' a function
#' 
#' @param .df (data.frame) data used to compute the new column
#' @param .col_definition (column.definition) column definition used to compute the new column (see the section DEFINITIONS for more details)
#' @param .col_name (character) name of the new column
#' @param .computing_function (function) a function that takes the data and the col_definition and returns a vector of results
#' @param ... extra parameters passed to .computing_function
#' @return (data.frame) data.frame with the new column
compute_column_from_definition <- function(.df,.col_definition, .col_name, .computing_function,...){
  
  # Compute the new data
  # browser()
  
  .new_column_data <- do.call(.computing_function,c(list(.df,.col_definition),...)) %>% as.vector()#.computing_function(.df,.col_definition,...)
  
  .out <- data.frame(x=.new_column_data) %>% set_colnames(.col_name)
  
  # .out[,.col_name] <- .new_column_data
  # Create a new column with the new data
  
  # .out <- .df %>% add_column(new_column=.new_column_data)
  
  # Rename using the name given
  
  # .out %<>% rename_with(.cols=one_of("new_column"), .fn=~paste0(.col_name))
  
  # output
  
  .out
  
  
}


#' Given a data.frame and a column definition, this function averages out, for each column, the scales
#' in the order defined in the column definition.
#' 
#' @param .df (data.frame) data
#' @param .col_definition (colum.definition)
#' @return (data.frame) 1-row data.frame with the columns in .col_definition and no scales.
average_out_scales <- function(.df,.col_definition){
  
  .out <- .df
  
  
  .out <- .col_definition %>% map2(names(.col_definition),~{
    scales_avg_ord <- .x[["scales"]][["scales_avg_ord"]]
    res <- .out %>% select(one_of(c(scales_avg_ord,.y)))
    
    group_scales <- scales_avg_ord[-1]
    while(length(group_scales)>0){
      res %<>% group_by_at(vars(one_of(group_scales))) %>% summarise_at(vars(one_of(.y)),~mean(.,na.rm=TRUE)) %>% ungroup
      
      group_scales <- group_scales[-1]
    }
    res %<>%  summarise_at(vars(one_of(.y)),~mean(.,na.rm=TRUE))
    
  }) %>% reduce(bind_cols)
  
  .out
  
}

#' Given a list of column definitions, computes each of them and adds them to the
#' data.frame.
#' 
#' For each column definition, 
#' - it selects the columns and scales relevant for the definition. Included in the column.definition
#' - gets unique rows
#' - averages out the scales present in order of priority until only one row is remaining. Priority is included
#' in the column definition.
#' - applies the column_definition on the row.
#' 
#' 
#' @param .df (data.frame) data used to compute the new columns
#' @param .col_definitions (list.of.column.definition) column definitions used to compute the new columns (see the section DEFINITIONS for more details)
#' @return (data.frame) data.frame with the new columns
compute_columns_from_definitions <- function(.df,.col_definitions){
  
  
  .out <- .df
  
  
  .out <- .col_definitions %>%  reduce2(names(.col_definitions),~{
    
    .new_column_name <- ..3
    .col_definition <- .y[["column.definition"]]
    .computing.function <- .y[["computing.function"]]
    .extra.parameters <- .y[["extra.parameters"]]
    
    
    .new_val <- tibble(as.numeric(NA)) %>% set_names(.new_column_name)
    if(length(.col_definition) >0){
      # Keep only the columns needed
      
      .scales_dep <- .col_definition %>% map(~.x[["scales"]][["scales_dep"]]) %>% reduce(c) %>% unique()
      
      .names_to_get <- c(.scales_dep,names(.col_definition))
      .names_to_get <- .names_to_get[.names_to_get %in% names(.out)]
      .column_df <-.out[,.names_to_get,drop=FALSE] %>% distinct()
      
      if(ncol(.column_df)==0 & !is.null(.x)){
        .names_to_get <- c(.scales_dep,names(.col_definition))
        .names_to_get <- .names_to_get[.names_to_get %in% names(.x)]
        
        .column_df <-.x[,.names_to_get,drop=FALSE] %>% distinct()
      }
      
      if(ncol(.column_df)>0){
        
        # browser(expr = .new_column_name=="species integrity")
        
        .new_val <- compute_column_from_definition(.df=.column_df,
                                                   .col_definition = .col_definition,
                                                   .col_name = .new_column_name,
                                                   .computing_function = .computing.function,
                                                   .extra.parameters)
        
        .new_val %<>%  summarise(across(one_of(.new_column_name),~mean(.,na.rm = TRUE)))#average_out_scales(.col_definition=.col_definition)
        
        # .new_val%<>% 
        #   select(one_of(.new_column_name))
      }
    }
    bind_cols(.x,.new_val)
    
  },.init=NULL) 
  
  suppressWarnings(.out %<>% select(-one_of(names(.df))))
  
  .out
  
}


#' Computes the average of a set of columns
#' 
#' @param .df (data.frame) the data
#' @param .col_definition (column.definition) 
#' @param .na.rm (logical) if TRUE, then ignore NAs.
#' @param ... ignored
#' @return (numeric) weighted sums of the columns
compute_average <- function(.df,.col_definition,.na.rm=TRUE,...){
  
  
  .out <- rep(0,nrow(.df))
  
  .weighted_data_and_weights <- columns_origin(.col_definition) %>% map(~{
    
    
    .col_name <- .x
    
    
    if(.col_name %in% names(.df)){
      
      .result <- .df %>% pull(.col_name)
      .weights <- rep(1,nrow(.df))
      .which_nas <- is.na(.result)
      
      if(.na.rm){
        
        .weights[is.na(.result)] <- 0
        .result %<>% replace_na(0)
      }
      
      list(result=.result,weight=.weights,nas = .which_nas) 
    }else{
      withRestarts(
        {
          
          stop(computing_function_column_does_not_exist(.df,.col_definition,.col_name,"compute_average"))
        },
        
        use_constant=function(x){
          
          rlang::warn(paste0("Could not find ",.col_name," while computing average, using ",as.numeric(x)))
          list(result=rep(as.numeric(x),nrow(.df)),weight=rep(1,nrow(.df)),nas=rep(FALSE,nrow(.df)))
          
        },
        drop_column=function(cond,silent=FALSE){
          if(!silent){
          rlang::warn(paste0("Could not find ",.col_name," while computing average. Dropping it from the computation"))
          }
          list(result=rep(as.numeric(0),nrow(.df)),weight=rep(0,nrow(.df)),nas=rep(TRUE,nrow(.df)))
          
        }
      )
    }
    
  })
  
  
  .out <- .weighted_data_and_weights %>% reduce(function(x,y) x+y[["result"]],.init=rep(0,nrow(.df)))
  
  .weights_sum <- reduce(.weighted_data_and_weights,~.x + .y[["weight"]],.init=rep(0,nrow(.df)))
  .out <- .out/.weights_sum
  
  if(.na.rm){
    
    .all_NAs <- reduce(.weighted_data_and_weights,~ .x & .y[["nas"]],.init=rep(TRUE,nrow(.df)))
    .out[.all_NAs] <- NA
  }
  
  .out
  
}


#' Computes weighted data from a column definition
#' 
#' @param .df (data.frame) the data
#' @param .col_definition (column.definition) with a weight parameter for each column
#' @param .na.rm (logical) if TRUE, then ignore NAs, do not include their weights when normalizing.
#' @return (named list) with one item per column in the column definition, each with result,weight and nas
#' fields
compute_weighted_data <- function(.df,.col_definition,.na.rm=TRUE){
  
  # browser()
  
  .out <- columns_origin(.col_definition) %>% map(~{
    # print(.x)
    # browser()
    .w <- weight(.col_definition,selection = .x)
    .col_name <- .x
    # 
    # browser(expr=.col_name=="SP.DIS")
    if(.col_name %in% names(.df)){
      
      .result <- .df[,.col_name]*.w
      .weights <- rep(.w,nrow(.df))
      .which_nas <- is.na(.result)
      
      if(.na.rm){
        
        .weights[is.na(.result)] <- 0
        
      }
      
      list(result=.result,weight=.weights,nas = .which_nas) 
    }else{
      withRestarts(
        {
          
          stop(computing_function_column_does_not_exist(.df,.col_definition,.col_name,"compute_weighted_sum"))
        },
        
        use_constant=function(x){
          
          rlang::warn(paste0("Could not find ",.col_name," while computing weighted sum, using ",as.numeric(x)))
          list(result=rep(as.numeric(x),nrow(.df)) %>% multiply_by(.w),weight=rep(.w,nrow(.df)),nas=rep(FALSE,nrow(.df)))
          
        },
        drop_column=function(cond,silent=FALSE){
          rlang::warn(paste0("Could not find ",.col_name," while computing weighted sum. Dropping it from the computation"))
          
          list(result=rep(as.numeric(NA),nrow(.df)) ,weight=rep(0,nrow(.df)),nas=rep(TRUE,nrow(.df)))
          
        }
      )
    }
    
  })
  
  
  .out
}

#' Computes the weighted sum of set of columns
#' 
#' @param .df (data.frame) the data
#' @param .col_definition (column.definition) with a weight parameter for each column
#' @param .normalize (logical) if TRUE, it asumes that data scale is [0,1] and uses
#' the sum of weights to normalize the scale back to [0,1]
#' @param .na.rm (logical) if TRUE, then ignore NAs, do not include their weights when normalizing.
#' @param ... ignored
#' @return (numeric) weighted sums of the columns
compute_weighted_sum <- function(.df,.col_definition,.normalize=FALSE,.na.rm=TRUE){
  
  # browser()
  .out <- rep(0,nrow(.df))
  .weighted_data_and_weights <-compute_weighted_data(.df=.df,
                                                     .col_definition=.col_definition,
                                                     .na.rm=.na.rm)
  
  
  # browser()
  .out <- .weighted_data_and_weights %>% reduce(function(x,y){
    z <- y[["result"]] %>% unlist 
    if(.na.rm){
      z[is.na(z)] <- 0
    }
    x+z
  },.init=rep(0,nrow(.df)))
  
  if(.normalize){
    
    .pos_weights_sum <- reduce(.weighted_data_and_weights,~.x + unlist(map(.y[["weight"]],~max(0,.x,na.rm = FALSE))),.init=rep(0,nrow(.df)))
    .neg_weights_sum <- reduce(.weighted_data_and_weights,~.x + unlist(map(.y[["weight"]],~min(0,.x,na.rm = FALSE))),.init=rep(0,nrow(.df))) %>% abs()
    .weights_sum <- .pos_weights_sum + .neg_weights_sum
    
    .out <- (.out+ .neg_weights_sum)/.weights_sum
    
    #.neg_weight <- map(.weighted_data_and_weights,~ any(.x[["weight"]]<0)) %>% unlist %>% any
    
    #.out <- case_when(.out<0 & .neg_weight~ 0, TRUE ~.out)
    
  }
  
  if(.na.rm){
    
    .all_NAs <- reduce(.weighted_data_and_weights,~ .x & .y[["nas"]],.init=rep(TRUE,nrow(.df)))
    .out[.all_NAs] <- NA
  }
  
  .out
  
}


#' Computes the contribution of each indicator to the column definition
#' 
#' @param .df (data.frame) the data
#' @param .col_definition (column.definition) with a weight parameter for each column
#' @param .normalize (logical) if TRUE, it asumes that data scale is [0,1] and uses
#' the sum of weights to normalize the scale back to [0,1]
#' @param .na.rm (logical) if TRUE, then ignore NAs, do not include their weights when normalizing.
#' @param .previous_contributions (named list of data.frames) contributions to previous computations
#' @return (list of dtata.frames)  
compute_contribution <- function(.df,.col_definition,.normalize=FALSE,.na.rm=TRUE,.previous_contributions=list()){
  
  .out <- list(data.frame())
  
  
  # Keep only the relevant scales and columns for this definition
  
  .scales_dep <- .col_definition %>% map(~.x[["scales"]][["scales_dep"]]) %>% reduce(c) %>% unique()
  
  suppressWarnings(.col_df <-.df %>% select(one_of(.scales_dep,names(.col_definition))))
  
  .col_df %<>% distinct
  
  if(any(names(.col_definition) %in% names(.col_df))){
    # browser()
    # Compute weighted data
    .col_def <- .col_definition[names(.col_df)]
    .weighted_data <- compute_weighted_data(.df=.col_df,
                                            .col_definition = .col_definition,
                                            .na.rm=.na.rm)
    
    if(.normalize){
      
      # Compute total weights
      .wg_t <-  .weighted_data %>% reduce(~{
        
        .not_all_NAs <- as.numeric(!all(.y[["nas"]]))
        
        .x+abs(.y[["weight"]])*.not_all_NAs
        
      },.init = rep(0,nrow(.col_df)))
      
      # Compute total weights
      
      
      
    }
    
    # Compute contributions
    
    if(.na.rm){
      
      # Compute average weight
      
      .not_all_NAs <- .weighted_data %>% reduce(~ .x & .y[["nas"]],.init=rep(TRUE,nrow(.col_df))) %>% magrittr::not()
      
      .avg_weight <- sum(.not_all_NAs)  
      
      
      
    }else{
      .avg_weight <- nrow(.col_df)
      
    }
    
    
    
    .out <-   .weighted_data %>% map(~{
      
      # divide by the total number of rows as when the mean is computed
      .contrib <- .x[["result"]]/.avg_weight
      
      if(.normalize){
        .contrib <- .contrib/.wg_t
      }
      
      .contrib
    }) %>% data.frame()
    
    
    .out %<>% 
      summarise(across(everything(),~case_when(any(!is.na(.))~sum(.,na.rm=TRUE),TRUE~ as.numeric(NA)))) %>% 
      list()
    
  }
  if(length(.previous_contributions) >0){
    .previous_contrib_names <- names(.col_definition)
    
    .previous_contrib_names <- .previous_contrib_names[!.previous_contrib_names %in% names(.col_df) & .previous_contrib_names %in% names(.previous_contributions)  ]
    
    if(length(.previous_contrib_names)>0){
      if(.normalize){
        
        # browser(expr=any("EXPOSURE"==.previous_contrib_names))
        
        wt <- .previous_contrib_names %>% map(~{
          .w <- abs(.col_definition[[.x]][["weight"]])
          
          
        }) %>% unlist
        
        if(.na.rm){
          .not_all_NAs <- .previous_contributions[.previous_contrib_names] %>% map(~{
            
            .all_NAs <- .x %>% rowwise %>%  mutate(across(everything(),is.na)) %>% transmute(r=all(c_across(everything()))) %>% pull("r")
            !.all_NAs
          }) %>% unlist
          
          wt <- wt* .not_all_NAs
        }
        
        
        
        wt <- sum(wt)
        
        
        
      }
      # browser(expr = wneg>0)
      
      
      
      .prev_contributions_out <- .previous_contrib_names %>% map(~{
        # browser(expr = wneg>0)
        .w <- .col_definition[[.x]][["weight"]]        
        
        cols_weighted <- .previous_contributions[[.x]] %>% mutate(across(everything(), ~. * .w))
        
        if(.normalize){
          
          
          
          cols_weighted <- cols_weighted/wt
        }
        cols_weighted
        
      }) %>% bind_cols()
      
      # .prev_contributions_out %>% rowwise %>% transmute(sum(c_across(everything())))
      
      if(nrow(.out[[1]]) >0){
        .out[[1]] <- bind_cols(.prev_contributions_out,.out[[1]])
      }else{
        .out <- list(.prev_contributions_out)
      }
    }
  }
  
  
  
  # browser(expr = .out[[1]] %>% rowwise() %>% mutate(across(everything(),~.<0)) %>% unlist() %>% any)
  
  # .check_neg <- .out[[1]] %>% rowwise() %>% mutate(total=sum(across(everything()),na.rm=TRUE)) %>% ungroup
  
  # if(.check_neg$total <0){
  #   .check_neg <- .out[[1]] %>% rowwise() %>%
  #     mutate(across(everything(),~as.numeric(.<0)))  %>% 
  #     mutate(neg_count=sum(across(everything()),na.rm = TRUE)) %>%
  #     select(neg_count) %>%
  #     ungroup() %>%
  #     bind_cols(.check_neg,.)
  #   .check_neg %<>% mutate(reduction=total/neg_count)
  #   
  #   .check_neg %<>% mutate(across(-c(total,neg_count,reduction),~case_when(.<0~.-reduction,TRUE~.)))
  #   
  #   
  #   .check_neg %<>% select(-c(total,neg_count,reduction))
  #   
  #   .out[[1]] <- .check_neg %>%  as.data.frame()
  # }
  .out
}

#' @param .df (data.frame) the data
#' @param .column.definitions (.list.of.column.definition) columns to compute
#' @param .normalize (logical) if TRUE, it asumes that data scale is [0,1] and uses
#' the sum of weights to normalize the scale back to [0,1]
#' @param .na.rm (logical) if TRUE, then ignore NAs, do not include their weights when normalizing.
#' @return (list of data.frames) with one element for each colum_definitions element, that contains a df of each indicator contribution
compute_contribution_from_col_definitions <- function(.column_definitions,.df,.na.rm=TRUE,.normalize=FALSE){
  
  .out <- .column_definitions %>% reduce2(names(.),~{
    
    
    .out <- .x
    .col_def <- .y[["column.definition"]]
    
    # browser(expr=..3=="climate stressors")
    if(length(.col_def)>0){
      .res <- compute_contribution(.df=.df,
                                   .col_definition=.col_def,
                                   .normalize=.normalize,
                                   .na.rm=.na.rm,
                                   .previous_contributions = .out)  %>% set_names(..3)
      .out  <- c(.out,.res)
    }
    
    .out
  },.init=list())
  
  .out
}

#' Given a data.frame with scales and indicators data a list of column definitions and and the scales
#' this function computes the columns
#' 
#' @param .df (data.frame) data with columns scales columns, and one column per indicator
#' @param .column.definitions (.list.of.column.definition) columns to compute
#' @param .scales (character) name of the scales columns
compute_model <- function(.df,.column.definitions,.scales){
  
  .out <- .df
  
  # Which scales do we have in .df?
  .current_scales <- names(.out) %>% keep(~.x %in% .scales)
  
  # Update column definitions to consider only the scales present
  .column.definitions %<>% map(~{
    .x[["column.definition"]] %<>% map(~{
      .x[["scales"]][["scales_dep"]] %<>% keep(~.x %in% .current_scales)
      .x
    })
    .x
  })
  
  # Compute the results
  
  .out <- compute_columns_from_definitions(.df=.out,.col_definitions = .column.definitions)
  
  
  .out
  
}


#' Given the data, a list of column definitions and a set of scales
#' this function computes the index for each scale groups.
#' 
#' @param .df (data.frame) data with columns MPA, HB, SP, HZ, UG, and one columnper indicator
#' @param .column.definitions (.list.of.column.definition) columns to compute
#' @param .scales (character) name of the scales columns
#' @param .group_by_scales (character) a vector with one or more of these strings: "MPA","HB","SP","HZ","UG"
compute_model_for_scales <- function(.df,.column.definitions,.scales,.group_by_scales){
  
  .out <- .df
  
  # Group the data by the group scales and compute the model for each group
  .out %<>% nest(scale_groups=-one_of(.group_by_scales)) %>% 
    rowwise() %>% mutate(
      compute_model(.df=scale_groups,
                    .column.definitions = .column.definitions,
                    .scales=.scales)
    )
  
  .out %<>% select(-scale_groups)
  
  
  .out
  
}

##### Model inputs #####


###### FOL_model_info ######

computing_function <- function(x) switch(x,"weighted sum"="compute_weighted_sum","average"="compute_average",error(paste0("Computing method ",x," not found.")))

new_FOL_model_info <- function(name="FOL_model",
                               structure=do.call(new_FOL_model_structure,make_FOL_model_structure_parameters()),
                               weights=do.call(new_FOL_model_weights,make_FOL_model_weights_parameters()),
                               factors_to_values_equiv=do.call(new_FOL_factor_values_equiv,make_FOL_factor_values_equiv_parameters()),
                               normalization_info=do.call(new_FOL_normalization_info,make_FOL_normalization_info_parameters()),
                               indexes=do.call(new_FOL_model_index_list,make_FOL_model_index_list_parameters()),
                               data_columns_specifications=do.call(new_FOL_data_columns_specifications,make_FOL_data_columns_specifications_parameters()),
                               input_data=data.frame(),
                               log=character(),
                               ...){
  
  stopifnot(is.character(name))
  stopifnot(inherits(weights,"FOL_model_weights"))
  stopifnot(inherits(factors_to_values_equiv,"FOL_factor_values_equiv"))
  stopifnot(inherits(normalization_info,"FOL_normalization_info"))
  stopifnot(inherits(indexes,"FOL_model_index_list"))
  stopifnot(inherits(structure,"FOL_model_structure"))
  stopifnot(inherits(data_columns_specifications,"FOL_data_columns_specifications"))
  
  
  
  
  
  
  
  
  .object <- structure(name,
                       class=c("FOL_model_info",class(name)),
                       structure=structure,
                       weights=weights,
                       factors_to_values_equiv=factors_to_values_equiv,
                       normalization_info=normalization_info,
                       indexes=indexes,
                       data_columns_specifications=data_columns_specifications,
                       input_data=input_data,
                       log=log,
                       ...
  )
  
  
  
  .object %<>% compute_column_definitions(model_info = .object)
  
  .object
  
}


normalization_info <- function(object,...){
  UseMethod("normalization_info")
}

normalization_info.FOL_model_info <- function(object,...){
  attr(object,"normalization_info")
}


input_data <- function(object,...){
  UseMethod("input_data")
}

input_data.FOL_model_info <- function(object,...){
  attr(object,"input_data")
}

load_data <- function(x,...){
  UseMethod("load_data")
}

load_data.FOL_model_info <- function(x,file,sheet,...){
  
  stopifnot(file.exists(file))
  
  try(.sheet <- readxl::read_excel(file,sheet=sheet) )
  
  
  .sheet %<>% mutate(across(where(is.character),str_squish))
  
  
  attr(x,"input_data") <- .sheet
  
  
  attr(x,"indexes") <- load_data(x=indexes(x),model_info=x)
  
  
  x
  
}

model_run <- function(x,...){
  UseMethod("model_run")
}

model_run.FOL_model_info <- function(x,...){
  
  
  # browser()
  attr(x,"indexes") <- model_run(x=indexes(x),model_info=x)
  
  
  x
  
}

compute_column_definitions <- function(object,model_info,...){
  UseMethod("compute_column_definitions")
}
compute_column_definitions.FOL_model_info <- function(object,model_info,...){
  
  
  attr(object,"indexes") <- compute_column_definitions(indexes(object),model_info,...)
  
  object
  
}

data_columns_specifications<- function(object,...){
  UseMethod("data_columns_specifications")
}

data_columns_specifications.FOL_model_info <- function(object,...){
  attr(object,"data_columns_specifications")
}


factors_to_values_equiv<- function(object,...){
  UseMethod("factors_to_values_equiv")
}

factors_to_values_equiv.FOL_model_info <- function(object,...){
  attr(object,"factors_to_values_equiv")
}

indexes <- function(object,...){
  UseMethod("indexes")
}

indexes.FOL_model_info <- function(object,...){
  
  
  attr(object,"indexes")
}




weights.FOL_model_info <- function(object,...){
  attr(object,"weights")
}

model_structure <- function(object,...){
  UseMethod("model_structure")
}

model_structure.FOL_model_info <- function(object,...){
  attr(object,"structure")
}

scales <- function(object){
  
  UseMethod("scales")
}

scales.FOL_model_info <- function(object,...){
  get_scales(model_structure(object))
}




FOL_model_info <- function(x,...){
  UseMethod("FOL_model_info")
}


FOL_model_info.character <- function(x,name=NULL,...){
  
  stopifnot(file.exists(x))
  
  .ms <- FOL_model_structure(x)
  .w <- FOL_model_weights(x)
  .equiv <- FOL_factor_values_equiv(x)
  .norm <- FOL_normalization_info(x)
  .indx <- FOL_model_index_list(x)
  .col_spc <- FOL_data_columns_specifications(x)
  
  if(is.null(name)){
    name <- basename(x)
  }
  
  new_FOL_model_info(name=name,
                     structure = .ms,
                     weights = .w,
                     factors_to_values_equiv = .equiv,
                     normalization_info = .norm,
                     indexes = .indx,
                     file=x,
                     data_columns_specifications = .col_spc,
                     ...
  )
  
  
}

get_results <- function(object,...){
  
  UseMethod("get_results")
}

get_results.FOL_model_info <- function(object,vars=character(),columns=character(),indexes=character(),...){
  
  .indxs <- indexes(object)
  
  if(length(indexes)>0){
    
    .indxs %<>% keep(~name(.x) %in% indexes)
  }
  
  .results <- .indxs %>% map(~{
    
    .indx <- .x
    
    .scales_x <- as.character(scales(.indx))
    .d_out <- column_definitions(.x) %>% keep(~ !name(.x) %in% as.character(indicators(.indx))) %>% map(~{
      
      attr(.x,"data_output")
    })
    .comps <- data_columns_specifications(object) %>% names
    .d_comps <- .comps%>% map(~{
      .comp <- .x
      .d_out %>% map(~ pluck(.x,.comp)) %>% compact
    }) %>% set_names(.comps)
    
    .res_names <- c("data","coverage","contribution")
    .res_funs <- c("data"="get_data","coverage"="get_coverage","contribution"="get_contribution")
    
    .out <- .d_comps %>% map(~{
      
      .d_out <- .x %>% compact()
      
      .res_names %>% map(~{
        
        .fun<-get(.res_funs[.x])
        
        .out <- .d_out %>% map(~{
          
          .r <- .fun(.x)
          
          
          
        }) %>% keep(~nrow(.x)>0)
        
        if(!.x=="contribution"){
          
          if(length(.out) >1){
            # suppressMessages(.out %<>% reduce(full_join))
            
            .out <- .out[-1] %>% reduce(~{
              
              # browser()
              .accuml <- .x
              .new_d <- .y
              
              .full_scales <- .scales_x %>% map(~{
                .scle_accuml <- character()
                if(.x %in% names(.accuml)){
                  .scle_accuml <- .accuml %>% pull(.x)
                }
                
                .scle_new_d <- character()
                if(.x %in% names(.new_d)){
                  .scle_new_d <- .new_d %>% pull(.x)
                }
                
                c(.scle_accuml,.scle_new_d) %>% discard(is.na) %>% unique
                
              }) %>%
                set_names(.scales_x)
              
              .fill_scales <- function(x) .scales_x %>% reduce(~{
                .out <- .x
                .scl <- .y
                if(.scl %in% names(.out)){
                  if(any(is.na(.out %>% pull(.scl)))){
                    # browser()  
                    .out <- seq_len(nrow(.out)) %>% map(~{
                      .r <- .out %>% slice(.x)
                      if(is.na(.r %>% pull(.scl))){
                        .r %<>% mutate(across(one_of(.scl),~.full_scales[.scl])) %>% unnest(cols=.scl)
                      }
                      .r
                    }) %>% bind_rows()
                    
                  }
                }
                .out
                
              },.init=x)
              
              .accuml %<>% .fill_scales
              
              .new_d %<>% .fill_scales
              .r <- suppressMessages(full_join(.accuml,.new_d))
              
              
              
              
            },.init=.out[[1]])
            
          }else{
            
            .out <- pluck(.out,1)
          }
          
          if(!is.null(.out)){
            .out %<>% distinct()
          }else{
            .out <- data.frame()
          }
        }
        .out
      }) %>% set_names(.res_names) 
      
    })
    
    if(length(columns)>0){
      
      .out %<>% map(~ .x %>% map2(names(.),~{
        if(.y!="contribution"){
          
        
        .x %>% select(one_of(c(intersect(scales(.indx),names(.)),intersect(columns,names(.)))))
        }else{
         .x[names(.x) %in% columns]
        }
      }))
    }
    .out %<>% map(~ .x %>% map2(names(.),~{
      if(.y!="contribution"){
      .x %>% select(one_of(intersect(scales(.indx),names(.))),everything())
      }else{
        .x
      }
    }))
  })
  
  if(length(vars)>0){
    .results %<>% map(~.x[names(.x) %in% vars])
  }
  
  
  .results
}


column_definitions.FOL_model_info <- function(object,columns=character(),indexes=character(),...){
  
  .indxs <- indexes(object)
  
  if(length(indexes)>0){
    .indxs <- .indxs[as.character(.indxs) %in% indexes]
  }
  
  .out <- .indxs %>% map(~ column_definitions(.x,columns=columns))
  
  
  
  .out
  
}

###### FOL_data_columns_specifications ######

make_FOL_data_columns_specifications_parameters <- function(specifications=list(),
                                                            ...){
  
  
  .options <- list(...)
  .options$FOL_object_class <- pluck(.options,"FOL_object_class",.default = "FOL_data_columns_specifications")
  
  
  
  c(list(specifications=specifications),
    .options)
}

new_FOL_data_columns_specifications <- function(specifications=list(),
                                                ...){
  
  stopifnot(is.list(specifications))
  
  structure(specifications,
            class=c("FOL_data_columns_specifications",class(specifications)),
            ...)
  
}

FOL_data_columns_specifications <- function(x,...){
  UseMethod("FOL_data_columns_specifications")
}


FOL_data_columns_specifications.character <- function(x,sheet="Computations",...){
  
  .sheet <- readxl::read_excel(x,sheet = sheet)
  
  .spec <- seq_len(nrow(.sheet)) %>% map(~{
    
    list(column=.sheet$Column[.x],
         fun=computing_function(.sheet$Function[.x]),
         contribution=.sheet$Contribution[.x]=="YES",
         coverage=.sheet$Coverage[.x]=="YES",
         normalize=.sheet$Normalize[.x]=="YES"
    )
    
  }) %>% set_names(.sheet$Column)
  
  new_FOL_data_columns_specifications(specifications = .spec,
                                      file=x,
                                      sheet=sheet,
                                      ...)
  
  
}
###### FOL_model_index_list ######

make_FOL_model_index_list_parameters <- function(indexes=list(),
                                                 ...){
  
  .options <- list(...)
  .options$FOL_object_class <- pluck(.options,"FOL_object_class",.default = "FOL_model_index_list")
  
  
  
  c(list(structure=structure),
    .options)
  
}

new_FOL_model_index_list <-function(indexes=list(),...){
  
  stopifnot(is.list(indexes))
  stopifnot(all(map(indexes,~inherits(.x,"FOL_model_index")) %>% unlist))
  
  
  
  structure(indexes,
            class=c("FOL_model_index_list",class(indexes)),
            ...)
  
}


FOL_model_index_list <- function(x,...){
  
  UseMethod("FOL_model_index_list")
}

FOL_model_index_list.character <- function(x,...){
  
  stopifnot(file.exists(x))
  
  .sheet <- readxl::read_excel(x,sheet = "Indices_Structure")
  
  .index_names <- names(.sheet)[(which(names(.sheet)=="Indicator")+1):ncol(.sheet)]
  
  .idx_strct <- .index_names %>% map(~{
    .sheet %>% select(Indicator,one_of(.x)) %>% filter(across(one_of(.x),~.==1)) %>% pull("Indicator") %>%
      FOL_index_indicators(index=.x)
  }) %>% set_names(.index_names)
  
  .sheet <- readxl::read_excel(x,sheet = "Indices_Scales")
  
  
  .index_names <- .sheet$Index
  
  .idx_scales <- .index_names %>% map(~{
    .keep <- .sheet %>% filter(Index==.x) %>% select(where(~.>0),-Index) %>% names()
    .drop <- .sheet %>% filter(Index==.x) %>% select(where(~.<0),-Index) %>% names()
    FOL_index_scales(scales=.keep,scales_drop=.drop,index=.x)
    
  }) %>% set_names(.index_names)
  
  .index_names <- unique(names(.idx_scales),names(.idx_strct)) 
  .idx <- .index_names%>% map(~{
    FOL_model_index(index=.x,scales = .idx_scales[[.x]],indicators = .idx_strct[[.x]])
  }) %>% set_names(.index_names)
  
  
  
  .sheet <- readxl::read_excel(x,sheet = "Filtering")
  
  .idx <- names(.idx) %>% map(~{
    
    .indx <- pluck(.idx,.x)
    if(.x %in% names(.sheet)){
      .filters <- .sheet %>% pull(.x) %>% discard(is.na)
      if(length(.filters)>0)
      .indx %<>% set_scale_filters(.filters)
    }
    .indx
  }) %>% set_names(names(.idx))
  
  new_FOL_model_index_list(indexes=.idx,file=x)
  
}



compute_column_definitions.FOL_model_index_list <- function(object,model_info,...){
  
  for(i in seq_along(object)){
    object[[i]] <- compute_column_definitions(object[[i]],model_info,...)
  }
  
  object
}


load_data.FOL_model_index_list <- function(x,model_info,...){
  
  for(i in seq_along(x)){
    # print(as.character(x[[i]]))
    x[[i]] <- load_data(x=x[[i]],model_info=model_info,...)
  }
  
  x
}


model_run.FOL_model_index_list <- function(x,model_info,...){
  
  for(i in seq_along(x)){
    # print(as.character(x[[i]]))
    # browser()
    x[[i]] <- model_run(x=x[[i]],model_info=model_info,...)
  }
  
  x
}

###### FOL_model_index ######

make_FOL_model_index_parameters <- function(indicators=do.call(new_FOL_index_indicators,make_FOL_index_indicators_parameters()),
                                            scales=do.call(new_FOL_index_scales,make_FOL_index_scales_parameters()),
                                            index=character(),
                                            column_definitions=list(),
                                            ...){
  
  .options <- list(...)
  .options$FOL_object_class <- pluck(.options,"FOL_object_class",.default = "FOL_model_index")
  
  
  
  c(list(
    index_indicators=index_indicators,
    scales=scales,
    index=index),
    .options)
  
}

new_FOL_model_index <- function(index=character(),
                                indicators=do.call(new_FOL_index_indicators,make_FOL_index_indicators_parameters()),
                                scales=do.call(new_FOL_index_scales,make_FOL_index_scales_parameters()),
                                column_definitions=list(),
                                scales_filters=character(),
                                ...){
  
  stopifnot(inherits(indicators,"FOL_index_indicators"))
  stopifnot(inherits(scales,"FOL_index_scales"))
  stopifnot(is.character(index))
  stopifnot(is.character(scales_filters))
  
  structure(index,
            class=c("FOL_model_index",class(index)),
            scales=scales,
            indicators=indicators,
            column_definitions=column_definitions,
            scales_filters=scales_filters,
            ...)
  
}




FOL_model_index <- function(index=character(),
                            indicators=do.call(new_FOL_index_indicators,make_FOL_index_indicators_parameters()),
                            scales=do.call(new_FOL_index_scales,make_FOL_index_scales_parameters()),
                            column_definitions=list(),
                            scales_filters=character(),
                            ...){
  
  
  new_FOL_model_index(index=index,
                      indicators = indicators,
                      scales=scales,
                      column_definitions = column_definitions,
                      scales_filters=character(),
                      ...)  
}


scales.FOL_model_index <- function(object,...){
  
  attr(object,"scales")
  
}

name.FOL_model_index <- function(object,...){
  
  as.character(object)
}

indicators <- function(object,...){
  
  UseMethod("indicators")
}

indicators.FOL_model_index <- function(object,...){
  
  attr(object,"indicators")
  
}

column_definitions <- function(object,...){
  
  
  UseMethod("column_definitions")
  
}

column_definitions.FOL_model_index <- function(object,columns=character(),...){
  
  .out <- attr(object,"column_definitions")  
  
  if(length(columns) >0){
    
    .out <- .out[as.character(.out) %in% columns]
  }
  
  .out
  
}




compute_column_definitions.FOL_model_index <- function(object,model_info,...){
  
  
  
  .indx <- object
  .structure <- model_structure(model_info)
  .str <- .structure
  .col_defs <- list()
  .indx_scales <- scales(.indx)
  .indx_scales_drop <- scales_drop(.indx_scales)
  
  
  # indicators
  for(i in seq_len(vec_depth(.structure)-2)){
    .str <- flatten(.str)
  }
  
  
  .str <- .str[names(.str) %in% indicators(.indx)]
  
  .col_defs <- names(.str) %>% map(~{
    
    
    .elements <- .str[[.x]] 
    
    .elements <- .elements[!.elements  %in% .indx_scales_drop]
    
    if(length(.elements) >0){
      
      .w <- weights(weights(model_info),selection=.x)
      new_FOL_column_definition(name=.x,
                                weights = .w,
                                columns_origin = .x,
                                computations = data_columns_specifications(model_info),
                                scales=.elements
      )
    }
  })  %>% set_names(names(.str)) %>% compact() %>% c(.col_defs)
  
  
  
  
  for(i in rev(seq_len(vec_depth(.structure)-3))){
    .str <- .structure
    for(j in seq_len(i)){
      .str <- flatten(.str)
    }
    
    .col_defs <- names(.str) %>% map(~{
      
      .elements <- .str[[.x]]
      # browser(expr="SOCIAL ADAPTIVE CAPACITY SUBDIMENSION" %in% .x)
      .elements <- .elements[names(.elements)  %in% names(.col_defs)]
      if(length(.elements) >0){
        .w <- weights(weights(model_info),selection=names(.elements))
        new_FOL_column_definition(name=.x,
                                  weights = .w,
                                  columns_origin = names(.elements),
                                  computations = data_columns_specifications(model_info),
                                  scales=.indx_scales
        )
      }
    })  %>% set_names(names(.str))  %>% compact%>% c(.col_defs,.)
    
    
  }
  
  # Top level
  
  .str <- .structure
  
  
  .col_defs <- names(.str) %>% map(~{
    .elements <- .str[[.x]]
    .elements <- .elements[names(.elements)  %in% names(.col_defs)]
    .w <- weights(weights(model_info),selection=names(.elements))
    new_FOL_column_definition(name=.x,
                              weights = .w,
                              columns_origin = names(.elements),
                              computations = data_columns_specifications(model_info),
                              scales=.indx_scales
    )
  })  %>% set_names(names(.str)) %>% c(.col_defs,.)
  
  attr(.indx,"column_definitions") <- .col_defs
  
  
  .indx
  
}

scale_filters <- function(x,...){
 UseMethod("scale_filters") 
}

scale_filters.FOL_model_index <- function(x,...){
  
  attr(x,"scale_filters")
}

set_scale_filters <- function(x,...){
  UseMethod("set_scale_filters")
}

set_scale_filters.FOL_model_index <- function(x,filters,...){
  
  attr(x,"scale_filters") <- filters
  
  x
}

  
  
load_data.FOL_model_index <- function(x,model_info,...){
  
  .col_defs <- column_definitions(x)
  
  print(name(x))
  
  .data <- input_data(model_info)
  
  .scale_filters <- attr(x,"scale_filters")
  if(length(.scale_filters) >0){
    
    .idx_scales <- scales(model_info) %>% as.character() %>% intersect(names(.data))
    
    .not_in_filters <-function(y) {
      
      ! y  %in% .scale_filters
      
    }
    
    .data %<>% rowwise() %>% filter(all(.not_in_filters(c_across(one_of(.idx_scales))))) %>% ungroup
  }
  
  for(i in seq_along(.col_defs)){
    # print(as.character(.col_defs[[i]]))
    
    
    if(name(.col_defs[[i]]) %in% as.character(indicators(x))){
      .col_defs[[i]] <- load_data(x=.col_defs[[i]],model_info=model_info,index=x,data=.data,...)
    }
  }
  
  
  attr(x,"column_definitions") <- .col_defs
  
  x
}

model_run.FOL_model_index <- function(x,model_info,...){
  
  .col_defs <- column_definitions(x)
  
  
  print(name(x))
  for(i in seq_along(.col_defs)){
    # print(as.character(.col_defs[[i]]))
    
    
    if(!name(.col_defs[[i]]) %in% as.character(indicators(x))){
      # browser()
      .col_defs[[i]] <- model_run(x=.col_defs[[i]],model_info=model_info,index=x,col_defs=.col_defs,...)
    }
  }
  
  
  attr(x,"column_definitions") <- .col_defs
  
  x
}


###### FOL_model_result ######

new_FOL_model_result <- function(name=character(),
                                 df=data.frame(),
                                 coverage=data.frame(),
                                 contribution=data.frame(),
                                 ...){
  
  
  structure(name,
            class=c("FOL_model_result",class(name)),
            data=df,
            coverage=coverage,
            contribution=contribution,
            ...)
  
}


FOL_model_results <- function(name=character(),
                              df=data.frame(),
                              coverage=data.frame(),
                              contribution=data.frame(),
                              ...){
  
  new_FOL_model_result(name=name,
                       df=df,
                       coverage=coverage,
                       contribution=contribution,
                       ...)  
  
}

name <- function(object,...){
  UseMethod("name")
}

name.FOL_model_result <- function(object,...){
  
  as.character(object)
}

get_data <- function(object,...){
  
  UseMethod("get_data")
  
}

get_data.FOL_model_result <- function(object,...){
  
  attr(object,"data")
  
}


get_contribution <- function(object,...){
  
  UseMethod("get_contribution")
  
}

get_contribution.FOL_model_result <- function(object,...){
  
  attr(object,"contribution")
  
}


get_coverage <- function(object,...){
  
  UseMethod("get_coverage")
  
}

get_coverage.FOL_model_result <- function(object,...){
  
  attr(object,"coverage")
  
}


###### FOL_column_definition ######


new_FOL_column_definition <- function(name=character(),
                                      weights=do.call(new_FOL_model_weights,make_FOL_model_weights_parameters()),
                                      data_input=data.frame(),
                                      computations=do.call(new_FOL_data_columns_specifications,make_FOL_data_columns_specifications_parameters()),
                                      columns_origin=character(),
                                      data_output=data.frame(),
                                      scales=character(),
                                      ...){
  
  
  structure(name,
            class=c("FOL_column_definition",class(name)),
            weights=weights,
            data_input=data_input,
            computations=computations,
            columns_origin=columns_origin,
            data_output=data_output,
            scales=scales,
            ...)
}


name.FOL_column_definition <- function(object,...){
  as.character(object)
}

columns_origin <- function(object,...){
  
  UseMethod("columns_origin")
  
}

columns_origin.FOL_column_definition <- function(object,...){
  
  attr(object,"columns_origin")
  
}


data_input <- function(object,...){
  
  UseMethod("data_input")
  
}

data_input.FOL_column_definition <- function(object,...){
  
  attr(object,"data_input")
  
}


data_output <- function(object,...){
  
  UseMethod("data_output")
  
}

data_output.FOL_column_definition <- function(object,...){
  
  attr(object,"data_output")
  
}


weight <- function(object,selection,...){
  
  UseMethod("weight")
  
}

weight.FOL_column_definition <- function(object,selection,...){
  
  weights(object) %>% filter(str_squish(tolower(name))==str_squish(tolower(selection))) %>% pull("weight")
  
}

weights.FOL_column_definition <- function(object,...){
  
  attr(object,"weights")
  
}
computations <- function(object,...){
  
  UseMethod("computations")
  
}

computations.FOL_column_definition <- function(object,...){
  
  attr(object,"computations")
  
}


scales.FOL_column_definition <- function(object,...){
  
  attr(object,"scales")
  
}


load_data.FOL_column_definition <- function(x,model_info,index,data,...){
  print(name(x))
  
  # browser(expr= as.character(x)=="HB.INV" & as.character(index)=="MPA INDEX")
  .data <- data
  
  .data %<>% filter(Indicator %in% columns_origin(x) ) 
  if(nrow(.data)==0){
    .data %<>% bind_rows(data.frame(Indicator=name(x)))
  }
  
  .scales_x <- as.character(scales(x))
  
  .missing_scales <- setdiff(c(.scales_x),names(.data))
  
  
  for(.scale in .missing_scales){
    .data[,.scale] <- .scale
  }
  
  
  
  .data %<>% select(one_of(.scales_x),everything())

  
  .data %<>% rowwise() %>% filter(all(is.na(c_across(one_of(scales_drop(scales(index))))))) %>% ungroup
  .data %<>%  select(-one_of(scales_drop(scales(index)))) 
  # browser(expr= as.character(x)=="MON.NSP" & as.character(index)=="SP INDEX")
  if(nrow(.data)>0){
    
    columns_origin(x) %>% walk(~{
      
      dt <- .data %>% filter(Indicator==.x)
      
      
      dt %<>%  summarise(across(one_of(intersect(.scales_x,names(.))),~any(is.na(.))))
      
      failed_check <- dt %>% as.data.frame %>% unlist %>% keep(~.x) %>% names
      
      if(length(failed_check)>0){
        
        warning(read_MPA_scale_missing_for_indicator(.indicator = .x,.scales = failed_check))
      }
      
      
      
    })
    
    
    .data <-columns_origin(x) %>% reduce(~{
      
      
      
      res <- .x
      indic <- .y
      dt <- res %>% filter(Indicator==indic) %>% select(-Indicator,-one_of(names(computations(x))))
      
      .scales_to_use <- setdiff(names(dt),.scales_x)
      dt %<>%  summarise(across(one_of(.scales_to_use),~any(!is.na(.))))
      
      failed_check <- dt %>% as.data.frame %>% unlist %>% keep(~.x) %>% names
      
      if(length(failed_check)>0){
        
        warning(read_MAP_unexpected_scale_for_indicator(.indicator=indic,.scales=failed_check))
        
        res %<>% mutate(across(one_of(failed_check),~case_when(Indicator==indic ~ as.character(NA),TRUE ~.)))
        
      }
      
      res
      
    },.init=.data) %>% distinct
    
    attr(x,"data_input") <- .data
    
    .vars_data <- names(computations(x)) %>% map(~{
      # browser()
      .comp <- .x
      .d <- .data %>% select(one_of(c(intersect(.scales_x,names(.)),"Indicator",.comp)))
      
      .factor_columns <- .d$Indicator %>% keep(~.x %in% names(factors_to_values_equiv(model_info))) %>% unique
      
     
      
      
      
      if(length(.factor_columns)>0){
        # browser()
        
        .d <- .factor_columns %>% reduce(~{
# browser()
          .out <- .x
          .ind <- .y
          .fc <- factors_to_values_equiv(model_info)[.ind] %>% pluck(.ind)
          

          # browser(expr=name(x)=="SP.POP")
          .out %<>% mutate(as_numeric=across(one_of(.comp),~suppressWarnings(as.numeric(.)))) %>% mutate(as_numeric=as_numeric %>% pull(.comp))
          .out %<>% mutate(lower_val=across(one_of(.comp),~str_squish(tolower(.)))) %>% mutate(lower_val=lower_val %>% pull(.comp))
            if(!.ind %in% .out$Indicator){
              warning(indicator_factors_to_numeric_indicator_missing(.ind))
            }



          
              .column_values <- .out %>% filter(Indicator==.ind) %>% pull("lower_val")
              
              .not_numeric_values <- .out %>% filter(Indicator==.ind) %>% pull("as_numeric") %>% is.na

              # suppressWarnings(.not_numeric_values <- is.na(as.numeric(.column_values)))
              .wrng_val_test <- .not_numeric_values & !.column_values %in% names(.fc)
              if(any(!is.na(.column_values) & .wrng_val_test) ){

                wrong_value <- unique(.column_values[ .wrng_val_test]) %>% purrr::discard(is.na)


                warning(unexpected_qualitative_values_for_indicator(.indicator = .ind,.values=wrong_value))
              }

          
          .out %>% mutate(across(one_of(.comp),~case_when(
            is.na(as_numeric) & Indicator==.ind & lower_val %in% names(.fc) ~.fc[lower_val],
            !is.na(as_numeric) ~ as_numeric,
            TRUE ~ NA_real_
          ))) %>% select(-c(as_numeric,lower_val))


        },.init=.d)
        
        # .d <- indicator_factors_to_numeric(.df = .d,
        #                                    .factor_value_equivalences = factors_to_values_equiv(model_info)[.factor_columns],
        #                                    .scales = .scales_x)
      }else{
        .d %<>% mutate(across(one_of(.comp),~suppressWarnings(as.numeric(.))))
      }
      
      
      
      
      
      
      .non_na_rows <- any(!is.na(.d %>% pull(.comp)))# filter(across(one_of(.comp),~any(!is.na(.)))) %>% nrow
      
      if(computations(x)[[.x]]$normalize & .non_na_rows){
        .normalization_limits <- normalization_info(model_info)
        
        .missing_scales <- setdiff(.scales_x,names(.normalization_limits))
        
        for(.scale in .missing_scales){
          .normalization_limits[,.scale] <- .scale
        }
        
        .normalization_limits %<>% select(one_of(intersect(.scales_x,names(.))),name,where(is.numeric))
        
        if("Quantitative" %in% names(.data)){
          .quant <- .data$Quantitative
          .d  %<>% mutate(quantitative=.quant)
        
          
        }else{
          .d  %<>% mutate(quantitative=0)
        
          
        }
        
        
        .missing <- .d %>% filter(!Indicator %in% .normalization_limits$name) %>% pull("Indicator")
        
        if(length(.missing)>0){
          
          stop(normalization_limits_missing_error(.missing_limits =.missing))
          
        }
        
        check_lims <- function(.indicator,.min_scale,.max_scale,.col_data){
          
          # browser(expr=.indicator=="U.SCI")
          .column_data <-withRestarts({ 
            
            # Check if the column values are within the normalization limits.
            if(any(!is.na(.col_data))){
              if(!all((.col_data>=.min_scale & .col_data <=.max_scale),na.rm = TRUE)){
                
                
                which_outside <- which(replace_na(!(.col_data>=.min_scale & .col_data <=.max_scale),FALSE))
                values_outside <- .col_data[which_outside]
                
                warning(data_outside_normalization_limits(.indicator=.indicator,.values=values_outside,.range=list(min=.min_scale[which_outside],max=.max_scale[which_outside])))
                
              }
            }
            
            .col_data
          },
          cut_to_range=function(cond){
            # warning(paste0("Cutting ",.indicator," values to its normalization limits."))
           
            # browser(expr=.indicator=="SP.POP")
            .which_lower <- replace_na(!(.col_data >=.min_scale & .col_data <=.max_scale),FALSE) & replace_na(.col_data < .min_scale ,FALSE)  
            .which_higher <- replace_na(!(.col_data >=.min_scale & .col_data <=.max_scale),FALSE) & replace_na(.col_data > .max_scale ,FALSE)  
            .col_data[.which_lower] <- .min_scale[.which_lower]
            .col_data[.which_higher] <- .max_scale[.which_higher]
            .col_data
            
          }
          )
          .column_data
        }
        
        
        
        .full_scales <- .scales_x%>% map(~{
          .scle_accuml <- character()
          if(.x %in% names(.d)){
            .scle_accuml <- .d %>% pull(.x)
          }
          
          .scle_new_d <- character()
          if(.x %in% names(.normalization_limits)){
            .scle_new_d <- .normalization_limits %>% pull(.x)
          }
          
          .out <- c(.scle_accuml,.scle_new_d) %>% discard(is.na) %>% unique
          
          if(length(.out)==0){
            .out <- NA_character_
          }
          .out
          
        }) %>%
          set_names(.scales_x)
        
        .fill_scales <- function(x) .scales_x %>% reduce(~{
          .out <- .x
          .scl <- .y
          if(.scl %in% names(.out)){
            if(any(is.na(.out %>% pull(.scl)))){
              # browser()  
              .out <- seq_len(nrow(.out)) %>% map(~{
                .r <- .out %>% slice(.x)
                if(is.na(.r %>% pull(.scl))){
                  .r %<>% mutate(across(one_of(.scl),~.full_scales[.scl])) %>% unnest(cols=.scl)
                }
                .r
              }) %>% bind_rows()
              
            }
          }
          .out
          
        },.init=x)

        .normalization_limits %<>% filter(name==name(x)) %>% .fill_scales()
        # browser(expr=name(x)=="U.SCI")
        .d %<>% mutate(.id=1:nrow(.))
        .d_lims <-  .d%>% .fill_scales()
        
        suppressMessages(.d_lims %<>% left_join(.normalization_limits %>% rename(Indicator=name)))
        
        .d%<>% left_join(.d_lims %>% select(.id,min,max,reverse) %>% distinct,by=".id") %>% select(-.id)
        
        .d %<>% group_by(Indicator) %>% mutate(across(one_of(.comp),~check_lims(unique(Indicator),min,max,.)))
        
        .d %<>% mutate(across(one_of(.comp),~normalize_vector(.,min,max))) %>% mutate(across(one_of(.comp),~case_when(reverse==1 ~ 1-., TRUE ~ .)))
        
        
        # .quantitative_flags %<>% pivot_wider(names_from = "Indicator",values_from = "Quantitative")
        
        # .d <- normalize_df(.df = .d,.df_quantitative = .quantitative_flags,.normalization.limits = .normalization_limits,.scales = intersect(.scales_x,names(.d)))
        
        .d %<>% select(-c(quantitative,min,max,reverse))
        
      }
      
      .d %<>% pivot_wider(names_from = "Indicator",values_from = .comp)
      
      .cov <- data.frame()
      if(pluck(computations(x),.x,"coverage")){
        
        
        
        .cov_data <- .d %>% mutate(across(-one_of(c(intersect(.scales_x,names(.)))),~as.numeric(!is.na(.))))
        # browser(expr=nrow(.cov_data)>1)
        
        .cov <-  .cov_data  %>% rowwise() %>%
          mutate(.coverage=sum(c_across(where(is.numeric)),na.rm=TRUE),.n=length(columns_origin(x))) %>%
          select(one_of(c(intersect(.scales_x,names(.)))),.coverage,.n) %>% 
          group_by(across(one_of(c(intersect(.scales_x,names(.)))))) %>%
          summarise(across(everything(),~sum(.,na.rm = TRUE)),.groups = "drop") %>%
          mutate(.coverage=.coverage/.n) %>%
          select(one_of(c(intersect(.scales_x,names(.)))),.coverage) %>% rename_with(~name(x),.coverage)
        
        # .cov <- .d %>% mutate(across(-one_of(c(intersect(.scales_x,names(.)))),~as.numeric(!is.na(.)))) %>% 
        #   mutate(coverage=sum(c_across(-one_of(c(intersect(.scales_x,names(.)))))/length(columns_origin(x)),na.rm=TRUE)) %>%
        #   select(one_of(c(intersect(.scales_x,names(.)))),coverage) %>% rename_with(~name(x),coverage)
      }
      .cont <- data.frame()
      if(pluck(computations(x),.x,"contribution")){
        .cont <- .d
      }
      
      FOL_model_results(name=as.character(x),df = .d,coverage = .cov,contribution = .cont)
    }) %>% set_names(names(computations(x)))
    
    
    
    attr(x,"data_output") <- .vars_data
    
  }
  
  x
}

model_run.FOL_column_definition <- function(x,model_info,index,col_defs,...){
  # print(name(x))
  # browser(expr=name(x)=="Social Organization" & name(index)=="MPA INDEX")
  .data <- col_defs[as.character(col_defs) %in% columns_origin(x)] %>% map(~attr(.x,"data_output"))
  # browser(expr=name(x)=="species integrity threats")
  .scales_x <- as.character(scales(x))
  
  attr(x,"data_input") <- .data
  
  .d_input <- names(computations(x)) %>% map(~{
    
    .var <- .x
    .d <- .data %>% map(~pluck(.x,.var)) %>% compact()
    
    if(length(.d)>1){
      .d <- .d[-1] %>% reduce(~{
        
        # browser()
        .accuml <- .x
        .new_d <- get_data(.y)
        
        .full_scales <- .scales_x %>% map(~{
          .scle_accuml <- character()
          if(.x %in% names(.accuml)){
            .scle_accuml <- .accuml %>% pull(.x)
          }
          
          .scle_new_d <- character()
          if(.x %in% names(.new_d)){
            .scle_new_d <- .new_d %>% pull(.x)
          }
          
          c(.scle_accuml,.scle_new_d) %>% discard(is.na) %>% unique
          
        }) %>%
          set_names(.scales_x)
        
        .fill_scales <- function(x) .scales_x %>% reduce(~{
          .out <- .x
          .scl <- .y
          if(.scl %in% names(.out)){
            if(any(is.na(.out %>% pull(.scl)))){
              # browser()  
              .out <- seq_len(nrow(.out)) %>% map(~{
                .r <- .out %>% slice(.x)
                if(is.na(.r %>% pull(.scl))){
                  .r %<>% mutate(across(one_of(.scl),~.full_scales[.scl])) %>% unnest(cols=.scl)
                }
                .r
              }) %>% bind_rows()
              
            }
          }
          .out
          
        },.init=x)
        
        .accuml %<>% .fill_scales
        
        .new_d %<>% .fill_scales
        .r <- suppressMessages(full_join(.accuml,.new_d))
        
        
        
        
      },.init=get_data(.d[[1]]))
    }else{
      .d <- get_data(.d[[1]])
    }
    
    .d
  }) %>% set_names(names(computations(x)))
  
  
  
  
  .result <- names(computations(x)) %>% map(~{
    # browser()
    # browser(expr=length(columns_origin(x))>1)
    .comp <- .x
    .df <- .d_input %>% pluck(.comp)
    .fun <- get(pluck(computations(x),.comp,"fun"))
    .normalize <- pluck(computations(x),.comp,"normalize")
    # browser(expr=.comp=="Year")
    .data_out <- compute_column_from_definition(.df=.df,
                                                .col_definition=x, 
                                                .col_name=name(x), 
                                                .computing_function=.fun,
                                                .normalize=.normalize
    )
    
    .data_out <- .df %>% select(one_of(intersect(.scales_x,names(.)))) %>% 
      bind_cols(.data_out) %>% 
      group_by(across(one_of(intersect(.scales_x,names(.)))))%>% 
      summarise(across(everything(),~mean(.,na.rm=TRUE)),.groups = "drop")
    
    .cov <- data.frame()
    if(pluck(computations(x),.comp,"coverage")){
      
      .cov_data <- .data %>% map(~{
        
        .r <- pluck(.x,.comp)
        if(!is.null(.r)){
        get_coverage(.r)
        }
      }) %>% compact()
      
      if(length(.cov_data)>1){
        .cov_data <- .cov_data[-1] %>% reduce(~{
          
          # browser()
          .accuml <- .x
          .new_d <- .y
          
          .full_scales <- .scales_x %>% map(~{
            .scle_accuml <- character()
            if(.x %in% names(.accuml)){
              .scle_accuml <- .accuml %>% pull(.x)
            }
            
            .scle_new_d <- character()
            if(.x %in% names(.new_d)){
              .scle_new_d <- .new_d %>% pull(.x)
            }
            
            c(.scle_accuml,.scle_new_d) %>% discard(is.na) %>% unique
            
          }) %>%
            set_names(.scales_x)
          
          .fill_scales <- function(x) .scales_x %>% reduce(~{
            .out <- .x
            .scl <- .y
            if(.scl %in% names(.out)){
              if(any(is.na(.out %>% pull(.scl)))){
                # browser()  
                .out <- seq_len(nrow(.out)) %>% map(~{
                  .r <- .out %>% slice(.x)
                  if(is.na(.r %>% pull(.scl))){
                    .r %<>% mutate(across(one_of(.scl),~.full_scales[.scl])) %>% unnest(cols=.scl)
                  }
                  .r
                }) %>% bind_rows()
                
              }
            }
            .out
            
          },.init=x)
          
          .accuml %<>% .fill_scales
          
          .new_d %<>% .fill_scales
          .r <- suppressMessages(full_join(.accuml,.new_d))
          
          
          
          
        },.init=.cov_data[[1]])
      }else{
        .cov_data %<>% pluck(1)
      }
      
      
      # browser(expr=nrow(.cov_data)>1)
      # browser() 
      .cov <-  .cov_data  %>% rowwise() %>%
        mutate(.coverage=sum(c_across(where(is.numeric)),na.rm=TRUE),.n=length(columns_origin(x))) %>%
        select(one_of(c(intersect(.scales_x,names(.)))),.coverage,.n) %>% 
        group_by(across(one_of(c(intersect(.scales_x,names(.)))))) %>%
        summarise(across(everything(),~sum(.,na.rm = TRUE)),.groups = "drop") %>%
        mutate(.coverage=.coverage/.n) %>%
        select(one_of(c(intersect(.scales_x,names(.)))),.coverage) %>% rename_with(~name(x),.coverage)
      
    }
    .cont <- data.frame()
    if(pluck(computations(x),.x,"contribution")){
      # browser(expr=name(x)=="EXPOSURE")
      # browser(expr=length(columns_origin(x))>1)
      # browser(expr=name(x)=="EXPOSURE" & name(index)=="SP INDEX")
      
      .cont_data <-.data %>% map(~{
        
        .r <- pluck(.x,.comp)
        if(!is.null(.r)){
          get_contribution(.r)
        }
      }) %>% compact()
      
      
      if(pluck(computations(x),.comp,"fun")=="compute_weighted_sum"){
        .cont <- .cont_data %>% map2(names(.),~{
          
          .w <- weight(x,selection=.y)
          .x %>% mutate(across(where(is.numeric),~.*.w))
        })
        
        
        
        if(length(.cont)>1){
          .cont <- .cont[-1] %>% reduce(~{
            
            # browser()
            .accuml <- .x
            .new_d <- .y
            
            .full_scales <- .scales_x %>% map(~{
              .scle_accuml <- character()
              if(.x %in% names(.accuml)){
                .scle_accuml <- .accuml %>% pull(.x)
              }
              
              .scle_new_d <- character()
              if(.x %in% names(.new_d)){
                .scle_new_d <- .new_d %>% pull(.x)
              }
              
              c(.scle_accuml,.scle_new_d) %>% discard(is.na) %>% unique
              
            }) %>%
              set_names(.scales_x)
            
            .fill_scales <- function(x) .scales_x %>% reduce(~{
              .out <- .x
              .scl <- .y
              if(.scl %in% names(.out)){
                if(any(is.na(.out %>% pull(.scl)))){
                  # browser()  
                  .out <- seq_len(nrow(.out)) %>% map(~{
                    .r <- .out %>% slice(.x)
                    if(is.na(.r %>% pull(.scl))){
                      .r %<>% mutate(across(one_of(.scl),~.full_scales[.scl])) %>% unnest(cols=.scl)
                    }
                    .r
                  }) %>% bind_rows()
                  
                }
              }
              .out
              
            },.init=x)
            
            .accuml %<>% .fill_scales
            
            .new_d %<>% .fill_scales
            .r <- suppressMessages(full_join(.accuml,.new_d))
            
            
            
            
          },.init=.cont[[1]])
          
          
          .cont  %<>% select(one_of(intersect(.scales_x,names(.))),where(is.numeric)) %>%   group_by(across(one_of(intersect(.scales_x,names(.)))))%>% 
            summarise(across(everything(),~mean(.,na.rm=TRUE)),.groups = "drop")
          
          
          if(.normalize){
            
            .weights_sums <- weights(x) %>% mutate(pos=factor(as.character(weight>=0),levels=c("TRUE","FALSE"))) %>% group_by(pos) %>% summarise(weight=sum(weight),.groups = "drop")
            .weights_sums %<>% complete(pos,fill=list(weight=0))
            
            
            .pos_weights_sum <- .weights_sums %>% filter(pos=="TRUE") %>% pull("weight")
            .neg_weights_sum <- .weights_sums %>% filter(pos=="FALSE") %>% pull("weight") %>% abs()
            .weights_sum <- .pos_weights_sum + .neg_weights_sum
            
            .cont %<>% mutate(across(where(is.numeric),~./.weights_sum))
            
            
            
          }
          
        }else{
          .cont %<>% pluck(1)
        }
      }else if(pluck(computations(x),.comp,"fun")=="compute_weighted_sum"){
        if(length(.cont_data)>1){
          .cont_data <- .cont_data[-1] %>% reduce(~{
            
            # browser()
            .accuml <- .x
            .new_d <- .y
            
            .full_scales <- .scales_x %>% map(~{
              .scle_accuml <- character()
              if(.x %in% names(.accuml)){
                .scle_accuml <- .accuml %>% pull(.x)
              }
              
              .scle_new_d <- character()
              if(.x %in% names(.new_d)){
                .scle_new_d <- .new_d %>% pull(.x)
              }
              
              c(.scle_accuml,.scle_new_d) %>% discard(is.na) %>% unique
              
            }) %>%
              set_names(.scales_x)
            
            .fill_scales <- function(x) .scales_x %>% reduce(~{
              .out <- .x
              .scl <- .y
              if(.scl %in% names(.out)){
                if(any(is.na(.out %>% pull(.scl)))){
                  # browser()  
                  .out <- seq_len(nrow(.out)) %>% map(~{
                    .r <- .out %>% slice(.x)
                    if(is.na(.r %>% pull(.scl))){
                      .r %<>% mutate(across(one_of(.scl),~.full_scales[.scl])) %>% unnest(cols=.scl)
                    }
                    .r
                  }) %>% bind_rows()
                  
                }
              }
              .out
              
            },.init=x)
            
            .accuml %<>% .fill_scales
            
            .new_d %<>% .fill_scales
            .r <- suppressMessages(full_join(.accuml,.new_d))
            
            
            
            
          },.init=.cont_data[[1]])
        }else{
          .cont_data %<>% pluck(1)
        }
        
        
        
        .cont <- .cont_data %>% select(one_of(intersect(.scales_x,names(.))),where(is.numeric)) %>% rowwise() %>% 
          mutate(n=sum(!is.na(c_across(-one_of(intersect(.scales_x,names(.))))))) %>%
          mutate(across(-one_of(intersect(.scales_x,names(.))),~./n)) %>% select(-n)
        
        .cont %<>% group_by(across(one_of(intersect(.scales_x,names(.)))))%>% 
          summarise(across(everything(),~mean(.,na.rm=TRUE)),.groups = "drop")
        
      }
      
      
      
    }
    
    FOL_model_results(name=name(x),df=.data_out,contribution = .cont,coverage = .cov)
    
  }) %>% set_names(names(computations(x)))
  
  
  
  attr(x,"data_output") <- .result
  x
}

#' Creates a list of column.definitions from a model structure
#' 
#' @param .model.structure (model.structure) structure to be converted
#' @param .weights (data.frame) columns are name and weight
#' @param .computing_function (function) used to compute the columns
#' @param .extra.parameters (list) extra parameters included in each definition
#' @param .extra.definitions (list.of.column.definition) other column definitions, 
#' appended after processing the model structure
#' @return (list.of.colum.definitions)
model.structure_to_column.definitions <- function(.model.structure,.weights,.computing_function,.extra.parameters=list(),.extra.definitions=list()){
  
  
  
  .out <- list()
  
  
  if(.model.structure  %>% set_names(NULL) %>% flatten %>% map(is_list) %>% reduce(any)){
    .results <- .model.structure  %>% set_names(NULL) %>% flatten %>% model.structure_to_column.definitions(.weights,.computing_function,.extra.parameters,.extra.definitions=list())
    
    
    .out <- .model.structure %>% map(~{
      
      input.columns <- names(.x)
      
      if(!all(tolower(input.columns) %in% tolower(.weights$name))){
        missing_weights <- input.columns[!tolower(input.columns) %in% tolower(.weights$name)]
        stop(weight_missing_in_model_definition(.missing_weights=missing_weights))
      }
      
      col_def <- .weights %>% filter(tolower(name) %in% tolower(input.columns)) %>%  mutate(result=map(weight,~ list(weight=.x,scales=list(scales_dep=NULL)))%>% set_names(input.columns[match(tolower(name),tolower(input.columns))]) ) %>% pull("result")
      
      list(column.definition=col_def,computing.function=.computing_function,extra.parameters=.extra.parameters)
      
    })
    
    .out <- c(.results,.out)
  }else{
    
    .out <-  .model.structure %>% map(~{
      
      col_structure <- .x
      input.columns <- names(col_structure)
      
      
      if(!all(tolower(input.columns) %in% tolower(.weights$name))){
        missing_weights <- input.columns[!tolower(input.columns) %in% tolower(.weights$name)]
        stop(weight_missing_in_model_definition(.missing_weights=missing_weights))
      }
      
      col_def <- .weights %>% filter(tolower(name) %in% tolower(names(col_structure))) %>%  mutate(result=map(weight,~ list(weight=.x))%>% set_names(input.columns[match(tolower(name),tolower(input.columns))]) ) %>% pull("result")
      
      col_def %<>% map2(names(col_def),~{
        
        scales_dep <- col_structure[[.y]]
        .x$scales <- list(scales_dep=scales_dep)
        
        .x
        
      })
      
      list(column.definition=col_def,computing.function=.computing_function,extra.parameters=.extra.parameters)
      
    }) %>% c(.extra.definitions)
    
  }
  
  c(.out,.extra.definitions)
  
}



###### FOL_index_scales ######
make_FOL_index_scales_parameters <- function(scales=character(),
                                             scales_drop=character(),
                                             index=character(),
                                             ...){
  
  .options <- list(...)
  .options$FOL_object_class <- pluck(.options,"FOL_object_class",.default = "FOL_index_scales")
  
  
  
  c(list(scales=scales,
         scales_drop=scales_drop,
         index=index),
    .options)
  
}

new_FOL_index_scales <- function(scales=character(),
                                 scales_drop=character(),
                                 index=character(),
                                 ...){
  
  stopifnot(is.character(scales))
  stopifnot(is.character(scales_drop))
  stopifnot(is.character(index))
  
  structure(scales,
            class=c("FOL_index_scales",class(scales)),
            scales_drop=scales_drop,
            index=index,
            ...)
  
}




FOL_index_scales <- function(scales=character(),
                             scales_drop=character(),
                             index=character,
                             ...){
  
  
  new_FOL_index_scales(scales=scales,scales_drop=scales_drop,index=index,...)  
}

scales_drop <- function(object,...){
  UseMethod("scales_drop")
}

scales_drop.FOL_index_scales <- function(object,...){
  attr(object,"scales_drop")
}

###### FOL_index_indicators ######

make_FOL_index_indicators_parameters <- function(indicators=character(),
                                                 index=character(),
                                                 ...){
  
  .options <- list(...)
  .options$FOL_object_class <- pluck(.options,"FOL_object_class",.default = "FOL_index_indicators")
  
  
  
  c(list(indicators=indicators,
         index=index),
    .options)
  
}

new_FOL_index_indicators <- function(indicators=character(),
                                     index=character(),
                                     ...){
  
  stopifnot(is.character(indicators))
  stopifnot(is.character(index))
  
  structure(indicators,
            class=c("FOL_index_indicators",class(indicators)),
            index=index,
            ...)
  
}




FOL_index_indicators <- function(indicators=chaaracter(),
                                 index=character(),
                                 ...){
  
  
  new_FOL_index_indicators(indicators=indicators,index=index,...)  
}





#' Reads the icons from a xlsx model definition
#' 
#' @params .file (character) path to excel file
#' @return (data.frame) with colum Name (character), Icon (character)
read_icons <- function(.file,.sheet = "Aesthetics"){
  
  
  # Read the sheet
  
  .out <- read_xlsx(.file,sheet = "Aesthetics")
  
  # Get the columns
  
  .out %<>% select(name=Name,icon=Icon)  %>%filter(complete.cases(.))
  
  .out
}

#' Given a model definition file (xlsx) read the labels
#'
#' @params .file (character) path to excel file
#' @return (model.structure)
read_labels <- function(.file){
  
  
  # Read the sheet
  
  .out <- read_xlsx(.file,sheet = "Aesthetics")
  
  # Get the columns
  
  .out %<>% select(name=Name,label=Label)  %>%filter(complete.cases(.))
  
  
  
  .out
}

###### FOL_model_structure ######

make_FOL_model_structure_parameters <- function(structure=list(),
                                                ...){
  
  .options <- list(...)
  .options$FOL_object_class <- pluck(.options,"FOL_object_class",.default = "FOL_model_structure")
  
  
  
  c(list(structure=structure),
    .options)
  
}

new_FOL_model_structure <-function(structure=list(),...){
  
  stopifnot(is.list(structure))
  
  
  structure(structure,
            class=c("FOL_model_structure",class(structure)),
            ...)
  
}


FOL_model_structure <- function(x,...){
  
  UseMethod("FOL_model_structure")
}

FOL_model_structure.character <- function(x,sheet="Model Structure",...){
  
  .ms <- read_model_structure(.file=x,.sheet = sheet)
  
  new_FOL_model_structure(structure = .ms,file=x,sheet=sheet)
  
}


scales.FOL_model_structure <- function(object,...){
  get_scales(object)
}

#' Given a model definition file (xlsx) read the model structure:
#' Dimensions,factors, components and indicators, and build a model.structure
#' as a result
#'
#' @params .file (character) path to excel file
#' @return (model.structure)
read_model_structure <- function(
  .file,
  .sheet="Model Structure"
){
  
  stopifnot(file.exists(.file))
  
  
  .out <- list()
  
  # Read the sheet
  
  .model.structure.excel <- read_xlsx(.file,sheet = .sheet)
  
  stopifnot("Indicator" %in% names(.model.structure.excel))
  
  # Which columns are scales?
  
  .names_model <- names(.model.structure.excel)
  
  .ind_position <- which(.names_model=="Indicator")
  
  .scales <- .names_model[(.ind_position+1):length(.names_model)]
  
  .estructure_names <- .names_model[1:(.ind_position-1)]
  
  # Get the columns
  
  # .model.structure.excel %<>% select(Dimension,Factor,Component,Indicator,MPA,HB,SP,HZ,UG)
  
  # Nest scales into indicators
  
  .nested.data <- .model.structure.excel %>% 
    pivot_longer(cols=one_of(.scales),names_to = "Scale",values_to = "Scale_used") %>%
    filter(Scale_used==1) %>% select(-Scale_used) %>% nest(Scale=Scale) %>%
    mutate(Scale=map(Scale,~.x %>% pull(Scale)))
  
  .nested.data %<>% nest(Indicator=c(Indicator,Scale)) %>%
    mutate(Indicator=map(Indicator,~pull(.x,"Scale") %>% set_names(pull(.x,"Indicator"))))
  
  .prev_estructure_name <- "Indicator"
  
  for(.estructure_name in rev(.estructure_names)){
    
    
    .nested.data %<>% nest(Estructure=one_of(.estructure_name,.prev_estructure_name)) %>%
      mutate(Estructure=map(Estructure,~pull(.x,.prev_estructure_name) %>% set_names(pull(.x,.estructure_name)))) 
    
    .nested.data%<>%
      rename_with(.cols=Estructure,.fn=~.estructure_name)
    .prev_estructure_name <- .estructure_name
  }
  
  
  
  .out <- .nested.data %>%pull(first(.estructure_names)) %>% extract2(1)
  
  
  
  .out
}

#' Extracts the scales from a model structure
#' 
#' @params .model_structure (model.structure)
#' @return (character) vector of scales
get_scales <- function(.model_structure){
  
  .out <- character(0)
  
  .ms <- .model_structure
  
  while(.ms  %>% set_names(NULL) %>% flatten %>% map(is_list) %>% reduce(any)){
    
    .ms %<>%  set_names(NULL) %>% flatten()
  }
  
  .out <- .ms %>% set_names(NULL) %>% flatten() %>% unlist %>% unique
  
  .out
  
}

###### FOL_model_weights ######

make_FOL_model_weights_parameters <- function(weights=data.frame(),
                                              ...){
  
  .options <- list(...)
  .options$FOL_object_class <- pluck(.options,"FOL_object_class",.default = "FOL_model_weights")
  
  
  
  c(list(weights=weights),
    .options)
  
}

new_FOL_model_weights <- function(weights=data.frame(),
                                  ...){
  
  
  structure(weights,
            class=c("FOL_model_weights",class(weights)),
            ...)
}

FOL_model_weights <- function(x,...){
  
  UseMethod("FOL_model_weights")
}

FOL_model_weights.character <- function(x,sheet="Weights",...){
  
  .w <- read_weights(.file=x,.sheet = sheet)
  
  new_FOL_model_weights(weights = .w,file=x,sheet=sheet)
  
}


weights.FOL_model_weights <- function(object,selection=object$name,...){
  
  .w <- object %>% filter(str_squish(tolower(name)) %in% str_squish(tolower(selection)))
  
  new_FOL_model_weights(weights = .w)
  
  
  
}

#' Reads the weights from a xlsx model definition
#' 
#' @params .file (character) path to excel file
#' @return (data.frame) with colum name (character) and weight (numeric)
read_weights <- function(.file,.sheet = "Weights"){
  
  
  
  .out <- readxl::read_xlsx(.file,sheet =.sheet) %>% set_colnames(c("name","weight"))
  
  .out
}

###### read habitat, use and species info ######

#' Reads the species groups from a xlsx model definition
#' 
#' @params .file (character) path to excel file
#' @return (data.frame) with colum species_group (character) and species (numeric)
read_species_groups <- function(.model_definitions_file,.sheet = "Species Groups"){
  
  
  .out <- read_excel(path = .model_definitions_file,sheet = .sheet)%>% fill(1,.direction = "down") %>% rename(species_group= 1)
  
  
  
  .out %<>% pivot_longer(cols=-1,names_to="species_vul",values_to="species") %>% filter(!is.na(species))
  
  .out
}


#' Reads the users groups from a xlsx model definition
#' 
#' @params .file (character) path to excel file
#' @return (data.frame) with colum users_group (character)
read_user_groups <- function(.model_definitions_file,.sheet = "User Groups"){
  
  
  .out <- read_excel(path = .model_definitions_file,sheet = .sheet) %>% rename(user_groups=1)
  
  
  
  .out
}

#' Reads the habitats from a xlsx model definition
#' 
#' @params .file (character) path to excel file
#' @return (data.frame) with colum habitats (character)
read_habitats <- function(.model_definitions_file,.sheet = "Habitats"){
  
  
  .out <- read_excel(path = .model_definitions_file,sheet = .sheet) %>% rename(habitats=1)
  
  
  
  .out
}

###### FOL_normalization_info ######

make_FOL_normalization_info_parameters <- function(limits=do.call(new_FOL_normalization_limits,make_FOL_normalization_limits_parameters()),
                                                   index=character(),
                                                   ...){
  
  .options <- list(...)
  .options$FOL_object_class <- pluck(.options,"FOL_object_class",.default = "FOL_normalization_info")
  
  
  
  c(list(limits=limits,
         index=index),
    .options)
  
}

new_FOL_normalization_info <- function(limits,...){
  
  stopifnot(inherits(limits,"FOL_normalization_limits"))
  
  structure(
    limits,
    class=c("FOL_normalization_info",class(limits)),
    ...
    
  )
  
}


FOL_normalization_info <- function(x,...){
  UseMethod("FOL_normalization_info")
}

FOL_normalization_info.character <- function(x,.sheet = "Normalization",...){
  
  
  
  
  .limits <- FOL_normalization_limits(x,.sheet=.sheet)
  
  new_FOL_normalization_info(limits=.limits,file=x,sheet=.sheet,...)
  
}

#' Reads the normalization sheet from a xlsx model definition file
#' 
#' Reads from a xlsx model definition file the normalization limits
#' for each indicator
#' 
#' @params .file (character) path to excel file
#' @params .sheet (character) 
#' @return (data.frame) normalization.definitions.data.frame
read_normalization_sheet <- function(.file,.sheet = "Normalization"){
  
  
  .df <- tryCatch({
    
    # Get all definitions
    .norm_data <- read_xlsx(.file,.sheet,skip = 1) %>% rename_with(~"reverse",matches("reverse"))
    
    # Get quantitative limits definitions
    .quan_data <- .norm_data %>%select(1:which(str_detect(names(.norm_data),"reverse")))
    
    
    
    # Get common column names (indicator names and scales)
    .qual_info_cols <- .quan_data %>% select(-min,-max,-reverse) %>% names()
    
    # Compute qualitative limits
    .qual_data <- .norm_data %>%  select(-min,-max,-reverse) %>% rowwise %>% filter(any(!is.na(c_across(-one_of(.qual_info_cols))))) %>% ungroup
    
    # quantitative values corresponding the the qualitative labels
    .qual_values <- .qual_data %>% select(-one_of(.qual_info_cols)) %>% names %>% as.numeric()
    
    # Compute replace labels with values
    .qual_data <- 1:(ncol(.qual_data)-length(.qual_info_cols)) %>% map(~{
      
      .qual_valu <- .qual_values[.x]
      .qual_data %>% select(-one_of(.qual_info_cols)) %>% select(.x) %>% mutate(across(everything(),~case_when(is.na(.)~as.numeric(NA),TRUE~.qual_valu)))#mutate(across(1,~case_when(!is.na(.) ~ qual_values[.x],TRUE ~ as.numeric(NA))))
      
    }) %>% bind_cols(.qual_data %>% select(one_of(.qual_info_cols)),.)
    
    # Compute min and max
    .qual_data %<>% rowwise() %>% 
      mutate(min=min(c_across(-one_of(.qual_info_cols)),na.rm=TRUE),
             max=max(c_across(-one_of(.qual_info_cols)),na.rm=TRUE))  %>%
      ungroup %>%
      select(one_of(.qual_info_cols),min,max) %>%
      mutate(reverse=0,quantitative=0) # these limits are for qualitative values, they are not reversed
    
    .quan_data %<>% mutate(quantitative=1) # flag quantitative values
    
    .qual_data <- .quan_data %>% filter(!name %in% .qual_data$name) %>% mutate(quantitative=0) %>% bind_rows(.qual_data)
    
    bind_rows(.quan_data,.qual_data)
    
  },
  
  
  
  error=function(.cond) {
    withRestarts(
      stop(read_model_definition_normalization_sheet_error(.file,.cond)),
      return_empty_normalization_data= function(.file) {
        warning(paste0("There was a problem reading the Normalization sheet on file: ",.file," using empty normalization data"))
        empty_normalization_data
      }
    )
    
  })
  
  # .df <- validate_normalization.definitions.data.frame(.df)
  
  .df  
}

###### FOL_normalization_limits ######

make_FOL_normalization_limits_parameters <- function(df=data.frame(),
                                                     ...){
  
  .options <- list(...)
  .options$FOL_object_class <- pluck(.options,"FOL_object_class",.default = "FOL_normalization_limits")
  
  
  
  c(list(df=df),
    .options)
  
}

new_FOL_normalization_limits <- function(df=data.frame(),...){
  
  structure(df,
            class=c("FOL_normalization_limits",class(df)),
            ...)
}

FOL_normalization_limits <- function(x,...){
  UseMethod("FOL_normalization_limits")
}

FOL_normalization_limits.character <- function(x,.sheet = "Normalization",...){
  
  
  
  .df <- read_normalization_limits(.file=x,
                                   .sheet=.sheet)
  
  
  
  new_FOL_normalization_limits(df=.df,file=x,sheet=.sheet,...)
  
}

#' Reads from a xlsx model definition file the normalization limits
#' for each indicator
#' 
#' @params . file (character) path to excel file
#' @return (data.frame) with colum name (character) and min and max (numeric)
read_normalization_limits <- function(.file,.sheet = "Normalization"){
  
  
  .out <- withCallingHandlers(
    # Read the data
    read_normalization_sheet(.file,.sheet = .sheet),
    # If there is a problem with the file, use an empty normalization data
    read_model_definition_normalization_sheet_error=function(text){
      return_empty_normalization_data(.file)
    },
    normalization.definitions.data.frame_not_valid=function(cond){
      if(cond$normalization.definitions.data.frame_not_valid_type=="columns"){
        fix_normalization.definitions.data.frame_columns(cond$df)
      }else{
        fix_normalization.definitions.data.frame_columns_classes(cond$df)
      }
    }
  ) 
  
  
  
  
  .out
}

###### FOL_factor_values_equiv ######

make_FOL_factor_values_equiv_parameters <- function(equivs=list(),
                                                    ...){
  
  .options <- list(...)
  .options$FOL_object_class <- pluck(.options,"FOL_object_class",.default = "FOL_factor_values_equiv")
  
  
  
  c(list(equivs=equivs),
    .options)
  
}

new_FOL_factor_values_equiv <- function(equivs=list(),
                                        ...){
  
  structure(equivs,
            class="FOL_factor_values_equiv",
            ...)
  
}

FOL_factor_values_equiv <- function(x,...){
  UseMethod("FOL_factor_values_equiv")
}

FOL_factor_values_equiv.character <- function(x,.sheet = "Normalization",...){
  
  
  
  .equivs <- read_factor_value_equivalences(.file=x,
                                            .sheet=.sheet)
  
  
  
  new_FOL_factor_values_equiv(equivs = .equivs,file=x,sheet=.sheet,...)
  
}

#' Reads from a xlsx model definition file the factor values
#' for each indicator
#' 
#' @params . file (character) path to excel file
#' @return (factor.value.equivalences)
read_factor_value_equivalences <- function(.file,.sheet = "Normalization"){
  
  .out <- list()
  
  # .df <- withCallingHandlers({
  #   # Read the data
  #  
  #   
  #   read_normalization_sheet(.file=.file)
  # }
  #   ,
  #   # If there is a problem with the file, use an empty normalization data
  #   read_model_definition_normalization_sheet_error=function(text){
  #     return_empty_normalization_data(.file)
  #   },
  #   normalization.definitions.data.frame_not_valid=function(cond){
  #     
  #     if(cond$normalization.definitions.data.frame_not_valid_type=="columns"){
  #       fix_normalization.definitions.data.frame_columns(cond$df)
  #     }else if(cond$normalization.definitions.data.frame_not_valid_type=="classes"){
  #       fix_normalization.definitions.data.frame_columns_classes(cond$df)
  #     }
  #   }) 
  # 
  
  .df <- read_excel(.file,sheet = .sheet,skip=1)  %>% rename_with(~"reverse",matches("reverse"))
  
  .df %<>%select(name,(which(stringr::str_detect(names(.df),"reverse"))+1):ncol(.df)) 
  
  # .df %<>% select(-one_of("min","max"))
  
  .indicators <- .df %>% pull("name")
  .df %<>%  select(-name)
  
  if(ncol(.df)==0 | nrow(.df)==0){
    return(withRestarts(
      warning(normalization_factor_values_empty(.df=.df)),
      return_empty_list=function() list()
    ))
  }
  
  
  # Get the values
  .values <- suppressWarnings(names(.df) %>% str_squish() %>% as.numeric())
  
  
  # Check alternative numeric formats
  if(any(is.na(.values))){
    
    .values[is.na(.values)] <-   suppressWarnings(as.numeric(gsub(",",".",names(.df)[is.na(.values)])))
  }
  
  if(any(is.na(.values))){
    
    .values[is.na(.values)] <-   suppressWarnings(as.numeric(gsub("\\.",",",names(.df)[is.na(.values)])))
  }
  
  if(all(is.na(.values))){
    return(withRestarts(
      warning(normalization_factor_values_empty(.df=.df)),
      return_empty_list=function() list()
    ))
  }
  
  .out <- 1:nrow(.df) %>% map(~ {
    
    .factor_levels <- .df %>% slice(.x) %>% as.vector %>% tolower() %>% str_squish()
    
    .result <- .values %>% set_names(.factor_levels)
    
    # Keep only valid factors
    
    .result <- .result[!is.na(names(.result))]
    
    # Drop NA values
    
    .result %>% purrr::discard(is.na)
    
  }) %>% set_names(.indicators) %>% compact
  
  
  
  .out
}

#' Given a min and max limits, it normalices a vector to 0-1
#' 
#' @param .data (numeric) data to be normalized
#' @param .min (numeric) min value of the scale
#' @param .max (numeric) max valur of the scale
#' @return (numeric) normalizd data
normalize_vector <- function(.data,.min,.max){
  
  .out <- .data
  
  .out <- (.out-.min)/(.max-.min)
  
  .out
  
  
}

#' Given a data.frame including indicators and a data.frame of normalization
#' limits for those indicators, normalize each indicator.
#' 
#' @param .df (data.frame) 
#' @param .df_quantitative (data.frame) data.frame of indicators. 1 if the value is expected to be quantiative, 0 otherwise.
#' @param .normalization.limits (data.frame) as returned by read_normalization_limits
#' @param .factor_value_equivalences (factor.value.equivalences) like the one produced by read_factor_value_equivlalences
#' @param .scales (character) which columns are scales
#' @param .silent (logical) wether to throw warnings if a problem is found
#' @return (data.frame) with indicators normalized
normalize_df <- function(.df,.df_quantitative,.normalization.limits,.factor_value_equivalences,.scales,.silent=FALSE){
  
  
  .out <- .df
  
  .indicator_names <- setdiff(names(.df),.scales)
  if(!all(.indicator_names %in% .normalization.limits$name)){
    .missing <- setdiff(.indicator_names,.normalization.limits$name)
    stop(normalization_limits_missing_error(.missing_limits =.missing))
    
  }
  
  
  .limits_long <- .normalization.limits %>% pivot_longer(cols=-c(name,min,max,reverse,quantitative),names_to="scale",values_to="scale_val")
  
  .limits_scale_dep <- .limits_long %>% group_by(name) %>% filter(!all(is.na(scale_val))) %>% ungroup %>% filter(!is.na(scale_val)) %>% distinct()
  
  .limits_scale_indep <- .limits_long %>% group_by(name) %>% filter(all(is.na(scale_val))) %>% ungroup  %>% select(-scale,-scale_val) %>% distinct()
  
  .quant_long <- .df_quantitative  %>% pivot_longer(cols=-one_of(c(.scales))) %>% rename(quantitative=value) 
  
  .data_long <- .out %>% arrange(across(one_of(.scales)))  %>% mutate(.id=1:nrow(.))  %>% pivot_longer(cols=-one_of(c(".id",.scales)))  
  
  
  
  .out <- .data_long %>% left_join(.quant_long,by=c(.scales,"name") ) %>% filter(!is.na(quantitative) & !is.na(value)) %>%
    mutate(.tid =1:nrow(.))
  
  
  .out %<>% pivot_longer(cols=-c(.id,.tid,name,value,quantitative),names_to="scale",values_to="scale_val") %>% distinct 
  
  
  .out_dep <- .out %>% left_join(.limits_scale_dep,by=c("name","scale","scale_val","quantitative"))
  .out_dep %<>% group_by(.tid) %>% filter(!all(is.na(min)) & !all(is.na(max)) & !all(is.na(reverse))) %>% mutate(across(one_of(c("min","max","reverse")),~ keep(.,~!is.na(.x)))) %>% ungroup 
  
  .out_indep <- .out %>% filter(!.tid %in% .out_dep$.tid) %>% left_join(.limits_scale_indep,by=c("name","quantitative")) #%>% filter(!is.na(min) & !is.na(max) & !is.na(rev)) %>% distinct()
  # target_indep %<>% pivot_wider(names_from = "scale",values_from="scale_val")  
  
  
  .out <- bind_rows(.out_dep,.out_indep) %>% pivot_wider(names_from = "scale",values_from="scale_val") 
  
  check_lims <- function(.indicator,.min_scale,.max_scale,.col_data){
    
    # browser(expr=.indicator=="SP.POP")
    .column_data <-withRestarts({ 
      
      # Check if the column values are within the normalization limits.
      if(any(!is.na(.col_data))){
        if(!all((.col_data>=.min_scale & .col_data <=.max_scale),na.rm = TRUE)){
          
          
          which_outside <- which(replace_na(!(.col_data>=.min_scale & .col_data <=.max_scale),FALSE))
          values_outside <- .col_data[which_outside]
          
          warning(data_outside_normalization_limits(.indicator=.indicator,.values=values_outside,.range=list(min=.min_scale[which_outside],max=.max_scale[which_outside])))
          
        }
      }
      
      .col_data
    },
    cut_to_range=function(cond){
      # warning(paste0("Cutting ",.indicator," values to its normalization limits."))
      if(.silent){
        cnd_muffle(cond)
      }
      # browser(expr=.indicator=="SP.POP")
      .which_lower <- replace_na(!(.col_data >=.min_scale & .col_data <=.max_scale),FALSE) & replace_na(.col_data < .min_scale ,FALSE)  
      .which_higher <- replace_na(!(.col_data >=.min_scale & .col_data <=.max_scale),FALSE) & replace_na(.col_data > .max_scale ,FALSE)  
      .col_data[.which_lower] <- .min_scale[.which_lower]
      .col_data[.which_higher] <- .max_scale[.which_higher]
      .col_data
      
    }
    )
    .column_data
  }
  
  .out %<>% group_by(name) %>% mutate(value=check_lims(unique(name),min,max,value))
  
  .out %<>% mutate(norm=normalize_vector(value,min,max)) %>% mutate(norm=case_when(reverse==1 ~ 1-norm, TRUE ~ norm))
  
  .out %<>% select(.id,name,norm)
  
  .out <- .data_long %>%  
    left_join(.out,by=c(".id","name")) %>% 
    mutate(value=norm) %>% select(-norm) %>% 
    pivot_wider(names_from = "name",values_from="value") %>% 
    select(-.id)
  
  
  
  
  # 
  # .out <- 1:nrow(.normalization.limits) %>% reduce(~{
  #   browser()
  #   .indicator <- .normalization.limits %>% slice(.y) %>% pull("name")
  #   .min_scale <-  .normalization.limits %>% slice(.y) %>% pull("min")
  #   .max_scale <- .normalization.limits %>% slice(.y) %>% pull("max")
  #   
  #   # Check whether the column is in the data.frame
  #   
  #   if(.indicator %in% names(.x)){
  #     
  #     .column_data <- withRestarts({ .col_data <- .x %>% pull(.indicator)
  #     
  #     # Check if the column values are within the normalization limits.
  #     if(any(!is.na(.col_data))){
  #       if(!all(between(.col_data,.min_scale,.max_scale),na.rm = TRUE) & !.silent){
  #         
  #         values_outside <- .col_data[replace_na(!between(.col_data,.min_scale,.max_scale),FALSE)]
  #         warning(data_outside_normalization_limits(.indicator=.indicator,.values=values_outside,.range=c(.min_scale,.max_scale)))
  #         
  #       }
  #     }
  #     
  #     .col_data
  #     },
  #     cut_to_range=function(cond){
  #       # warning(paste0("Cutting ",.indicator," values to its normalization limits."))
  #       # cnd_muffle(cond)
  #       .col_data <-.x %>% pull(cond$indicator)
  #       ind_min <- cond$range[1]
  #       ind_max <- cond$range[2]
  #       
  #       .col_data[replace_na(.col_data > ind_max ,FALSE)] <- ind_max
  #       .col_data[replace_na(.col_data < ind_min ,FALSE)] <- ind_min
  #       .col_data
  #       
  #     }
  #     )
  #     
  #     # Normalize the data
  #     .normalized_data <- normalize_vector(.column_data,.min=.min_scale,.max=.max_scale)
  #     
  #     .x[,.indicator] <- .normalized_data
  #   }
  #   .x
  #   
  # },.init=.out)
  # 
  
  .out
  
}


#' This function read a excel file containing the data used in the model
#' It formats the data to be ready for use in other functions.
#' 
#' @param .file (character) path to excel file
#' @param .sheet (numeric) sheet number to read
#' @param .value_column (character) which column holds the value to be imported?
#' @param .model.structure (model.structure) used to check scales dependence
#' @param .MPA_name (character)
#' @param .checks (logical) whether to perform extra data checks or not.
#' @param .silent (logical) wether to throw warnings if a problem is found
#' @param ... other parameters passed to read_excel
#' @return (data.frame) a data.frame with columns of scales and one column of values for each indicator.
read_MPA_data <- function(.file,.sheet=1,.value_column,.model.structure,.MPA_name="MPA",.checks=TRUE,.silent=FALSE,...){
  # browser()
  
  # Read the data
  .out <- read_excel(path = .file,sheet = .sheet,trim_ws = TRUE,...)
  
  .model_scales <- get_scales(.model.structure)
  
  .ms <- .model.structure
  
  while(.ms  %>% set_names(NULL) %>% flatten %>% map(is_list) %>% reduce(any)){
    
    .ms %<>%  set_names(NULL) %>% flatten()
  }
  # browser()
  
  # Check if there is an MPA column already, if not, create one filled with the MPA name
  if(!"MPA" %in% names(.out)){
    
    
    
    MPA_indicators <-  .ms %>% set_names(NULL) %>% flatten() %>% map(~{
      "MPA" %in% .x
    }) %>% unlist %>% keep(~.x) %>% names
    
    .out %<>% mutate(MPA=case_when(Indicator %in% MPA_indicators ~ .MPA_name,TRUE ~as.character(NA)))
    
  }
  
  # Get structure at indicator level
  
  .indicators <- .ms %>%   set_names(NULL) %>%flatten %>% names
  
  if(.checks){
    
    .out %>% pull("Indicator") %>% walk(~{
      .ind <- .x
      if(!.ind %in% .indicators & !.silent){
        warning(read_MPA_unknown_indicator(.ind))
      }
      
    })
  }
  # Drop indicators not in model definition
  
  .out %<>% filter(Indicator %in% .indicators)
  
  
  if(.checks){
    
    .ms  %>% set_names(NULL) %>% flatten %>% walk2(names(.),~{
      
      dt <- .out %>% filter(Indicator==.y)
      
      
      dt %<>%  summarise(across(one_of(.x),~any(is.na(.))))
      
      failed_check <- dt %>% as.data.frame %>% unlist %>% keep(~.x) %>% names
      
      if(length(failed_check)>0 & ! .silent){
        
        warning(read_MPA_scale_missing_for_indicator(.indicator = .y,.scales = failed_check))
      }
      
      
      
    })
    
    .out <- .ms  %>% set_names(NULL) %>% flatten %>% map2(names(.),~{
      
      dt <- .out %>% filter(Indicator==.y)
      
      dt %>% mutate(across(one_of(.x),~case_when(is.na(.) ~ "Other",TRUE ~.)))
    }) %>% bind_rows()
    
  }
  .out <- .ms  %>% set_names(NULL) %>% flatten %>% reduce2(names(.),~{
    
    # browser(expr=..3=="SP.SEN")
    # browser()
    res <- .x
    indic <- ..3
    dt <- res %>% filter(Indicator==indic)
    .scales_to_use <- setdiff(c("MPA","HB","SP","HZ","UG"),.y)
    dt %<>%  summarise(across(one_of(.scales_to_use),~any(!is.na(.))))
    
    failed_check <- dt %>% as.data.frame %>% unlist %>% keep(~.x) %>% names
    
    if(length(failed_check)>0){
      if(!.silent & .checks){
        warning(read_MAP_unexpected_scale_for_indicator(.indicator=indic,.scales=failed_check))
      }
      res %<>% mutate_at(vars(one_of(failed_check)),~case_when(Indicator==indic ~ as.character(NA),TRUE ~.))
      
    }
    
    res
    
  },.init=.out) %>% distinct
  
  # Select scale columns, the indicator names and the values column selected.
  .out %<>% select(one_of(c("MPA","HB","SP","HZ","UG","Indicator",.value_column)))
  
  .out %<>% mutate(across(one_of(c("MPA","HB","SP","HZ","UG","Indicator")),str_squish))
  
  # Pivot the data and fill with NAs
  
  keep_first <- list(first)
  
  names(keep_first)  <- .value_column
  
  .out %<>%pivot_wider(names_from = "Indicator",values_from = .value_column,values_fill = list(Value=NA),
                       values_fn = keep_first)
  
  # .out %>% rowwise() %>% mutate(across(everything(),length))
  # Create groups of variables depending on the same set of scales
  
  
  MPA <- .out$MPA %>% unique %>% purrr::discard(is.na)
  
  HB <- .out$HB %>% unique %>% purrr::discard(is.na) 
  
  SP <- .out$SP %>% unique %>% purrr::discard(is.na) 
  
  # HZ <- .out$HZ %>% unique %>% purrr::discard(is.na) 
  
  UG <- .out$UG %>% unique %>% purrr::discard(is.na) 
  
  
  
  .groups <-.out %>%
    select(MPA,HB,SP,UG) %>% # Select scales
    mutate_all(~as.integer(!is.na(.))) %>% # Which scales have values in each row?
    transmute(group=paste0(MPA,SP,HB,UG), # Create a string code to identify a group
              group_sum=MPA+SP+HB+UG)  %>%
    bind_cols(.out)  %>% # Paste the data
    nest(data=-c(group,group_sum)) %>% # Arrange the data by group of scales and by the number of scales involved
    mutate(data=data %>% map(~.x %>% select_if(~!all(is.na(.)))))
  
  
  
  
  
  # Create a grid of scales that we will fill with data later
  
  .scales_grid <- expand.grid(MPA=MPA,HB=HB,SP=SP,UG=UG,stringsAsFactors = FALSE)  
  
  # Fill the grid with the data.
  
  # First arrange the groups so that we join the groups with less scales first.
  
  .groups %<>% mutate(group=stringi::stri_reverse(group)) %>% # Reverse group code to sort
    arrange(group_sum,group) %>% # Sort by number of scales and by code
    mutate(group=stringi::stri_reverse(group))
  
  
  .out <- c(list(.scales_grid),.groups$data) %>% reduce(~{
    
    
    scales_present <- names(.y) %>% keep(~.x %in% c("MPA","HB","SP","UG"))
    
    
    
    left_join(.x,.y,by=scales_present)
    
    
    
  }) %>%  # Join
    select(one_of(c("MPA","HB","SP","HZ","UG")),everything())  %>%  # Arrange vars
    arrange_at(vars(one_of(c("MPA","HB","SP","HZ","UG")))) # Arrange by scales
  
  
  .missing_cols <- setdiff(.indicators,names(.out))
  .out <- .missing_cols %>% map(~NA) %>% set_names(.missing_cols) %>% as.data.frame() %>% bind_cols(.out,.)
  
  
  .out
  
  
}

#' Computes the percentage of quantitative indicators from a quantittive flags dataframe and an indicators data.frame
#' 
#' @param .df_quantitative (data.frame) quantitative flags data
#' @param .df_indicators (data.frame) indicators data
#' @param .column_definitions (list of data.definitions) used to compute the index
#' @param .all_scales (character) scales in .df
#' @param .group_by_scales (character) scales to group the data
#' @return (data.frame) of data coverage
compute_quantitative_percent <- function(.df_quantitative,.df_indicators,.column_definitions,.all_scales,.group_by_scales){
  
  .out <- .df_quantitative
  
  
  .column_definitions_quantitative <- .column_definitions %>% map(~{
    
    .x[["extra.parameters"]][[".normalize"]] <- FALSE
    .x[["computing.function"]] <- compute_weighted_sum
    .x[["column.definition"]] %<>%map(~{
      .x[["weight"]] <-1
      .x
    })
    .x
  })
  
  .df_indicators %<>% pivot_longer(cols=-one_of(.all_scales),names_to="Indicator",values_to="Values")
  
  
  .out %<>% pivot_longer(cols=-one_of(.all_scales),names_to="Indicator",values_to="Quantitative")
  
  suppressMessages(.out %<>% left_join(.df_indicators))
  
  .out %<>% mutate(Quantitative=case_when(is.na(Values)~as.numeric(NA), TRUE ~Quantitative))
  
  .out %<>% select(-Values) %>% pivot_wider(names_from = "Indicator",values_from="Quantitative")
  
  .quantitative_count <- withCallingHandlers(compute_model_for_scales(.df=.out,
                                                                      .column.definitions = .column_definitions_quantitative,
                                                                      .scales = .all_scales,
                                                                      .group_by_scales =.group_by_scales),
                                             
                                             computing_function_column_does_not_exist=function(cond){
                                               drop_column(cond)
                                               
                                               
                                             })
  
  
  
  
  .out %<>% mutate_at(vars(-one_of(.all_scales)),~1)
  
  .indicators_count <- withCallingHandlers(compute_model_for_scales(.df=.out,
                                                                    .column.definitions = .column_definitions_quantitative,
                                                                    .scales = .all_scales,
                                                                    .group_by_scales = .group_by_scales),
                                           
                                           computing_function_column_does_not_exist=function(cond){
                                             use_constant(1)
                                             
                                             
                                           })
  
  
  
  .quantitative_count %<>% pivot_longer(-one_of(.all_scales),values_to = "quantitative_count")
  
  .indicators_count %<>% pivot_longer(-one_of(.all_scales),values_to="indicators_count")
  
  .out <- full_join(.indicators_count,.quantitative_count)
  
  .out %<>% mutate(quantitative_percent=quantitative_count/indicators_count)
  
  .out %<>% select(-indicators_count,-quantitative_count) %>% pivot_wider(names_from = "name",values_from = "quantitative_percent")
  
  
  
  .out
  
  
}

#' Computes the average indocators years from a years dataframe and an indicators data.frame
#' 
#' @param .df_years (data.frame) years data
#' @param .df_indicators (data.frame) indicators data
#' @param .column_definitions (list of data.definitions) used to compute the index
#' @param .all_scales (character) scales in .df
#' @param .group_by_scales (character) scales to group the data
#' @return (data.frame) of data coverage
compute_indicator_years <- function(.df_years,.df_indicators,.column_definitions,.all_scales,.group_by_scales){
  
  .out <- .df_years
  
  
  .column_definitions_years <- .column_definitions %>% map(~{
    
    .x[["extra.parameters"]][[".normalize"]] <- FALSE
    .x[["computing.function"]] <- compute_average
    .x[["column.definition"]] %<>%map(~{
      .x[["weight"]] <-1
      .x
    })
    .x
  })
  
  .df_indicators %<>% pivot_longer(cols=-one_of(.all_scales),names_to="Indicator",values_to="Values")
  
  
  .out %<>% pivot_longer(cols=-one_of(.all_scales),names_to="Indicator",values_to="Year")
  
  suppressMessages(.out %<>% left_join(.df_indicators))
  
  .out %<>% mutate(Year=case_when(is.na(Values)~as.numeric(NA), TRUE ~Year))
  
  .out %<>% select(-Values) %>% pivot_wider(names_from = "Indicator",values_from="Year")
  
  .out <- withCallingHandlers(compute_model_for_scales(.df=.out,
                                                       .column.definitions = .column_definitions_years,
                                                       .scales = .all_scales,
                                                       .group_by_scales = .group_by_scales),
                              
                              computing_function_column_does_not_exist=function(cnd){
                                drop_column(cnd)
                              })
  
  
  .out
  
  
}


#' Computes the data coverage for a indicators data.frame
#' 
#' @param .df (data.frame) indicators data
#' @param .column_definitions (list of data.definitions) used to compute the index
#' @param .all_scales (character) scales in .df
#' @param .group_by_scales (character) scales to group the data
#' @return (data.frame) of data coverage
compute_data_coverage <- function(.df,.column_definitions,.all_scales,.group_by_scales){
  
  
  
  
  .out <- .df
  
  
  .column_definitions_coverage <- .column_definitions %>% map(~{
    
    .x[["extra.parameters"]][[".normalize"]] <- FALSE
    .x[["computing.function"]] <- compute_weighted_sum
    .x[["column.definition"]] %<>%map(~{
      .x[["weight"]] <-1
      .x
    })
    .x
  })
  
  # Count non NA values
  
  .nas_df <- .df %>% mutate_at(vars(-one_of(.all_scales)),~case_when(!is.na(.)~1,TRUE~as.numeric(NA)))
  
  
  .non_na_count <- withCallingHandlers(compute_model_for_scales(.df=.nas_df,
                                                                .column.definitions = .column_definitions_coverage,
                                                                .scales = .all_scales,
                                                                .group_by_scales = .group_by_scales),
                                       
                                       computing_function_column_does_not_exist=function(cnd){
                                         drop_column(cnd)
                                       })
  
  # Count indicators
  
  
  .indicators_df <- .df %>% mutate_at(vars(-one_of(.all_scales)),~1)
  
  
  .indicators_count <- withCallingHandlers(compute_model_for_scales(.df=.indicators_df,
                                                                    .column.definitions = .column_definitions_coverage,
                                                                    .scales = .all_scales,
                                                                    .group_by_scales = .group_by_scales),
                                           
                                           computing_function_column_does_not_exist=function(cnd){
                                             use_constant(1)
                                           })
  
  # Compute coverage
  
  .non_na_count %<>% pivot_longer(-one_of(.all_scales),values_to = "na_count")
  
  .indicators_count %<>% pivot_longer(-one_of(.all_scales),values_to="indicators_count")
  
  suppressMessages(.out <- full_join(.indicators_count,.non_na_count))
  
  .out %<>% mutate(coverage=na_count/indicators_count)
  
  .out %<>% select(-indicators_count,-na_count) %>% pivot_wider(names_from = "name",values_from = "coverage")
  
  .out
  
  .out
  
}


#' Applies a set of factor to numeric equivalences to the indicators in a data.frame
#' 
#' @param .df (data.frame) data where we have column scales and columns of indicators. Like the one
#' produced by read_MPA_data
#' @param .factor_value_equivalences (factor.value.equivalences) like the one produced by read_factor_value_equivlalences
#' @param .scales (character) names of scale columns
#' @param .silent (logical) wether to throw warnings if a problem is found
#' @return (data.frame) model.data data.frame
indicator_factors_to_numeric <- function(.df,.factor_value_equivalences,.scales,.silent=FALSE){
  .out <- .df
  
  .out <- .factor_value_equivalences %>% names %>% reduce(~{
    
    
    var_name <- .y
    dt <- .x
    
    if(!var_name %in% names(dt)){
      if(!.silent){
        warning(indicator_factors_to_numeric_indicator_missing(var_name))
      }
    }else{
      
      
      column_values <- dt %>% pull(var_name) 
      f_equiv <- .factor_value_equivalences[[var_name]]
      
      
      suppressWarnings(not_numeric_values <- is.na(as.numeric(column_values)))
      
      if(any(!is.na(column_values) & not_numeric_values & !str_squish(tolower(column_values)) %in% names(f_equiv)) & !.silent){
        
        wrong_value <- unique(column_values[ not_numeric_values & !str_squish(tolower(column_values)) %in% names(f_equiv)]) %>% purrr::discard(is.na)
        
        
        warning(unexpected_qualitative_values_for_indicator(.indicator = var_name,.values=wrong_value))
      }
      
      
      
      
      new_val <- .factor_value_equivalences[[var_name]][str_squish(tolower(column_values))] %>% as.numeric()
      
      column_values[not_numeric_values] <- new_val[not_numeric_values]
      
      
      
      
      suppressWarnings(dt %<>% mutate_at(vars(one_of(var_name)), ~column_values))
      
      
    }
    
    
    dt
  },.init=.out)
  
  
  suppressWarnings(.out %<>% mutate(across(-one_of(.scales), as.numeric)))
  .out
}


#' Computes the indicator coverage for each component, factor, dimension and
#' for the index..
#' 
#' For each component, factor, dimension and the index, the number of indicators with no values are 
#' added and then divided by the total number of indicators required.
#' 
#' @param .data_file (character) path to file with the data
#' @param .sheet sheet in the data file where the data is located
#' @param .model_definition_file (character) path to model definition file
#' @param .scales (character) scales at which the index will be computed.
#' @param ... parameters passed to read_excel when reading .data_file
#' @return (data.frame) components, factors, dimensions and index coverage
vulnerability_index_coverage <- function(.data_file,.sheet,.model_definition_file,.scales,...){
  
  suppressWarnings({
    
    .out <- data.frame()
    
    # Get the weights
    
    .weights <- read_weights(.model_definition_file)
    
    .weights %<>% mutate(weight=1)
    
    # Get the model structure
    
    .model_structure <- read_model_structure(.model_definition_file)
    
    
    
    # ECOLOGICAL VULNERABILITY and index definitions.
    
    
    
    # Create column definitions
    
    .column_definitions <- model.structure_to_column.definitions(.model.structure = .model_structure,
                                                                 .weights = .weights,
                                                                 .computing_function = compute_weighted_sum,
                                                                 .extra.parameters = list(.na.rm=TRUE,.normalize=FALSE))
    
    # Read the data
    .df <- read_MPA_data(.file = .data_file,
                         .sheet = .sheet,
                         .value_column = "Value",
                         .model.structure = .model_structure,
                         ...)
    
    
    # Define my model scales
    .all_scales <- c("MPA","HB","SP","UG","HZ")
    
    # Compute coverage
    
    .out <- compute_data_coverage(.df=.df,
                                  .column_definitions = .column_definitions,
                                  .all_scales = .all_scales,
                                  .group_by_scales = .scales)
    
    .out 
    
  })
  
}

#' Reads a indicators data file and preprocess them
#' 
#' @param .data_file (character) path to file with the data
#' @param .sheet sheet in the data file where the data is located
#' @param .model_definition_file (character) path to model definition file
#' @param .normalize (logical) 
#' @param .checks (logical) whether to perform extra data checks or not.
#' @param .silent (logical) wether to throw warnings if a problem is found
#' @return (data.frame) indicators
read_and_preprocess_indicators <- function(
  .data_file,
  .sheet,
  .model_definition_file,
  .normalize=FALSE,
  .checks=TRUE,
  .silent=FALSE,
  .index_indicators=do.call(FOL_index_indicators,make_FOL_index_indicators_parameters())
){
  
  .out <- data.frame()
  
  
  .factor_values <- read_factor_value_equivalences(.model_definition_file)
  
  .model_structure <- read_model_structure(.model_definition_file,.index_indicators = .index_indicators)
  
  .all_scales <- get_scales(.model_structure)
  
  .out <-read_MPA_data(.data_file,.sheet,"Value",.model_structure,"MPA",.checks = .checks,.silent = .silent)
  
  
  .quantitative_flags <- read_MPA_data(.data_file,.sheet,"Quantitative",.model_structure,"MPA",.checks = FALSE,.silent = TRUE)
  
  
  
  .out <- indicator_factors_to_numeric(.df = .out,
                                       .factor_value_equivalences = .factor_values,
                                       .scales = .all_scales,
                                       .silent = .silent)
  
  if(.normalize){
    .normalization_limits <- read_normalization_limits(.model_definition_file)
    
    .out <-  normalize_df(.df = .out,.df_quantitative = .quantitative_flags,.normalization.limits = .normalization_limits,.scales = .all_scales,.silent=FALSE)
  }
  
  
  .out
  
}


#' Computes the percentage of quantitative data for each component, factor, dimension and
#' for the index..
#' 
#' Missing columns are dropped from the computation. NAs are ignored.
#' 
#' @param .data_file (character) path to file with the data
#' @param .sheet sheet in the data file where the data is located
#' @param .model_definition_file (character) path to model definition file
#' @param .scales (character) scales at which the index will be computed.
#' @param ... parameters passed to read_excel when reading .data_file
#' @return (data.frame) components, factors, dimensions and index quantitative data percentage
vulnerability_index_quantitative_perc <- function(.data_file,.sheet,.model_definition_file,.scales,...){
  
  suppressWarnings({
    
    .out <- data.frame()
    
    # Get the weights
    
    .weights <- read_weights(.model_definition_file)
    
    .weights %<>% mutate(weight=1)
    
    # Get the model structure
    
    .model_structure <- read_model_structure(.model_definition_file)
    
    
    
    
    
    # Create column definitions
    
    .column_definitions <- model.structure_to_column.definitions(.model.structure = .model_structure,
                                                                 .weights = .weights,
                                                                 .computing_function = compute_average,
                                                                 .extra.parameters = list(.na.rm=TRUE))
    
    # Read the data
    .df <- read_MPA_data(.file = .data_file,
                         .sheet = .sheet,
                         .value_column = "Quantitative",
                         .model.structure = .model_structure,
                         .checks = FALSE,
                         .silent = TRUE,
                         ...)
    
    
    
    # Define my model scales
    .all_scales <- get_scales(.model_structure)
    
    .df_indicators <- read_and_preprocess_indicators(.data_file = .data_file,
                                                     .sheet = .sheet,
                                                     .model_definition_file = .model_definition_file,
                                                     .normalize = TRUE,
                                                     .silent = TRUE,
                                                     .checks = FALSE
    )
    
    .out <- compute_quantitative_percent(.df_quantitative  = .df,
                                         .df_indicators= .df_indicators,
                                         .column_definitions = .column_definitions,
                                         .all_scales=.all_scales, 
                                         .group_by_scales = .scales)
    
    
    .out
    
  })  
  
}

#' Computes the number of years of measurement eccumulated for each component, factor, dimension and
#' for the index..
#' 
#' Missing columns are dropped from the computation. NAs are ignored. Years are averaged across scales 
#' no being computed and then added together when computing each component, factor, dimension or index
#' 
#' @param .data_file (character) path to file with the data
#' @param .sheet sheet in the data file where the data is located
#' @param .model_definition_file (character) path to model definition file
#' @param .scales (character) scales at which the index will be computed.
#' @param ... parameters passed to read_excel when reading .data_file
#' @return (data.frame) components, factors, dimensions and index accumulated years
vulnerability_index_years <- function(.data_file,.sheet,.model_definition_file,.scales,...){
  
  suppressWarnings({
    
    .out <- data.frame()
    
    # Get the weights
    
    .weights <- read_weights(.model_definition_file)
    
    .weights %<>% mutate(weight=1)
    
    # Get the model structure
    
    .model_structure <- read_model_structure(.model_definition_file)
    
    
    
    
    
    # Create column definitions
    
    .column_definitions <- model.structure_to_column.definitions(.model.structure = .model_structure,
                                                                 .weights = .weights,
                                                                 .computing_function = compute_average,
                                                                 .extra.parameters = list(.na.rm=TRUE))
    
    # Read the data
    .df <- read_MPA_data(.file = .data_file,
                         .sheet = .sheet,
                         .value_column = "Year",
                         .model.structure = .model_structure,
                         ...)
    
    
    
    # Define my model scales
    .all_scales <- get_scales(.model_structure)
    
    .df_indicators <- read_and_preprocess_indicators(.data_file = .data_file,
                                                     .sheet = .sheet,
                                                     .model_definition_file = .model_definition_file,
                                                     .normalize = TRUE
    )
    
    .out <- compute_indicator_years(.df_years = .df,
                                    .df_indicators= .df_indicators,
                                    .column_definitions = .column_definitions,
                                    .all_scales=.all_scales, 
                                    .group_by_scales = .scales)
    
    
    .out
    
  })
  
}



#' Computes the vulnerability index from a model definition file and a data file.
#' 
#' Missing columns are dropped from the computation. NAs are ignored.
#' 
#' @param .data_file (character) path to file with the data
#' @param .sheet sheet in the data file where the data is located
#' @param .model_definition_file (character) path to model definition file
#' @param .scales (character) scales at which the index will be computed.
#' @param .normalize (logical) if true, the results at each step are normalized.
#' @param ... parameters passed to read_excel when reading .data_file
#' @return (data.frame) indicators contribution to each components, factors, dimensions and index
vulnerability_index_contributions <- function(.data_file,.sheet,.model_definition_file,.normalize=FALSE,.scales,...){
  suppressWarnings({
    .out <- data.frame()
    
    # Get the weights
    
    .weights <- read_weights(.model_definition_file)
    
    # Get the model structure
    
    .model_structure <- read_model_structure(.model_definition_file)
    
    # TODO: Create a model.structure from the definitions of ECOLOGICAL VULNERABILITY and the index and then run
    # model.structure_to_column.definitions. Update other functions too.
    
    
    
    # Create column definitions
    
    .column_definitions <- model.structure_to_column.definitions(.model.structure = .model_structure,
                                                                 .weights = .weights,
                                                                 .computing_function = compute_weighted_sum,
                                                                 .extra.parameters = list(.na.rm=TRUE,.normalize=.normalize))
    
    # # Read the data
    # .df <- read_MPA_data(.file = .data_file,
    #                      .sheet = .sheet,
    #                      .value_column = "Value",
    #                      .model.structure = .model_structure,
    #                      ...)
    
    .quantitative_flags <- read_MPA_data(.file = .data_file,
                                         .sheet = .sheet,
                                         .value_column = "Quantitative",
                                         .model.structure = .model_structure,
                                         ...)
    
    .df <- read_and_preprocess_indicators(.data_file = .data_file,
                                          .sheet = .sheet,
                                          .model_definition_file = .model_definition_file,
                                          .normalize = .normalize,
                                          .silent = TRUE,
                                          .checks = FALSE
    )
    
    # # Read factor to numeric equivalences for qualitative indicators
    # 
    # .factor_values <- read_factor_value_equivalences(.model_definition_file)
    # 
    # # Define my model scales
    .all_scales <- get_scales(.model_structure)
    # 
    # # Convert indicator factors to numeric values
    # .df <- indicator_factors_to_numeric(.df = .df,
    #                                     .df_quantitative = .quantitative_flags,
    #                                     .factor_value_equivalences = .factor_values,
    #                                     .scales = .all_scales)
    # 
    # # Normalization
    # if(.normalize){
    #   # Get normalization limits
    #   .normalization_limits <- read_normalization_limits(.model_definition_file)
    #   
    #   .df <- normalize_df(.df = .df,.normalization.limits = .normalization_limits,.scales = .all_scales)  
    # }
    # 
    
    .out <- .df %<>% nest(scale_groups=-one_of(.scales)) %>% 
      rowwise() %>% 
      mutate(contributions=
               list(
                 withCallingHandlers(compute_contribution_from_col_definitions(.column_definitions = .column_definitions,
                                                                               .df=scale_groups,
                                                                               .na.rm = TRUE,
                                                                               .normalize = .normalize),
                                     computing_function_column_does_not_exist=function(cnd) drop_column(cnd)
                 )
               )
      )
    
    .out %<>% select(-scale_groups)
    
    
  })
  .out
  
}

#' Computes the vulnerability index from a model definition file and a data file.
#' 
#' Missing columns are dropped from the computation. NAs are ignored.
#' 
#' @param .data_file (character) path to file with the data
#' @param .sheet sheet in the data file where the data is located
#' @param .model_definition_file (character) path to model definition file
#' @param .scales (character) scales at which the index will be computed.
#' @param .normalize (logical) if true, the results at each step are normalized.
#' @param ... parameters passed to read_excel when reading .data_file
#' @return (data.frame) components, factors, dimensions and index
vulnerability_index <- function(.data_file,.sheet,.model_definition_file,.normalize=FALSE,.scales,...){
  
  .out <- data.frame()
  
  # Get the weights
  
  .weights <- read_weights(.model_definition_file)
  
  # Get the model structure
  
  .model_structure <- read_model_structure(.model_definition_file)
  
  # Define my model scales
  .all_scales <- get_scales(.model_structure)
  
  # Create column definitions
  
  .column_definitions <- model.structure_to_column.definitions(.model.structure = .model_structure,
                                                               .weights = .weights,
                                                               .computing_function = compute_weighted_sum,
                                                               .extra.parameters = list(.na.rm=TRUE,.normalize=.normalize))
  
  # Read the data
  # .df <- read_MPA_data(.file = .data_file,
  #                      .sheet = .sheet,
  #                      .value_column = "Value",
  #                      .model.structure = .model_structure,
  #                      ...)
  
  .quantitative_flags <- read_MPA_data(.file = .data_file,
                                       .sheet = .sheet,
                                       .value_column = "Quantitative",
                                       .model.structure = .model_structure,
                                       ...)
  
  
  .df <- read_and_preprocess_indicators(.data_file = .data_file,
                                        .sheet = .sheet,
                                        .model_definition_file = .model_definition_file,
                                        .normalize = .normalize,
                                        .silent = TRUE,
                                        .checks = FALSE
  )
  # Read factor to numeric equivalences for qualitative indicators
  
  # .factor_values <- read_factor_value_equivalences(.model_definition_file)
  # 
  # 
  # 
  # # Convert indicator factors to numeric values
  # .df <- indicator_factors_to_numeric(.df = .df,
  #                                     .df_quantitative = .quantitative_flags,
  #                                     .factor_value_equivalences = .factor_values,
  #                                     .scales = .all_scales)
  # 
  # # Normalization
  # if(.normalize){
  #   # Get normalization limits
  #   .normalization_limits <- read_normalization_limits(.model_definition_file)
  #   
  #   .df <- normalize_df(.df = .df,.normalization.limits = .normalization_limits,.scales = .all_scales)  
  # }
  # 
  
  
  .out <- withCallingHandlers(compute_model_for_scales(.df=.df,
                                                       .column.definitions = .column_definitions,
                                                       .scales = .all_scales,
                                                       .group_by_scales = .scales),
                              
                              computing_function_column_does_not_exist=function(cnd){
                                
                                drop_column(cnd)
                              })
  
  
  .out
  
}

#' Checks correlation between indicators at the dimension levels and randomly
#' drops those with strong correlation from the column definitions
#' 
#' @param .df (data.frame) data
#' @param .model_structure (model.structure) model structure
#' @param .column_definitions (list.of.colum.definitions) column definitions
#' @param .corr_lim (numeric) absolute correlation limit
#' @return (list.of.colum.definitions)
drop_correlated_indicators <- function(.df,.model_structure,.column_definitions,.corr_lim=0.8){
  
  .out <- .column_definitions
  
  
  .correlated_indicators <- .model_structure %>% 
    set_names(NULL) %>% flatten %>% 
    set_names(NULL) %>% flatten %>% 
    map(~{
      
      .ind_names <- .x %>% set_names(NULL) %>% flatten %>% set_names(NULL) %>% flatten %>% names()
      .corr_df <- .df %>% select(one_of(.ind_names))
      
      .corr_df %<>% select(sample(1:ncol(.corr_df),ncol(.corr_df),replace = FALSE))
      
      .correlations <-  .corr_df %>% cor(use="pairwise.complete.obs",method="pearson")
      .correlated_features <- c()
      for(i in 2:ncol(.correlations)){
        for(j in 1:(i-1)){
          if(!is.na(.correlations[j,i])){
            if(abs(.correlations[j,i])>.corr_lim){
              
              .correlated_features <- unique(c(.correlated_features,colnames(.correlations)[i]))
            }
          }
        }
      }
      
      .correlated_features
      
      
    }) %>% compact() %>% reduce(c)
  
  
  
  .out %<>% map(~{
    
    .x$column.definition <- .x$column.definition[!names(.x$column.definition) %in%  .correlated_indicators]
    .x
  })
  
  
}

#' Processes one scenario in one sheet of the data file
#' 
#' @param .data_file (character) path to file with the data
#' @param .sheet sheet in the data file where the data is located
#' @param .model_definition_file (character) path to model definition file
#' @param .normalize (logical) if true, the results at each step are normalized.
#' @param .drop_correlated_indicators (logical)
#' @return (list)
process_scenario <- function(.data_file,.sheet,.model_definition_file,.normalize,.drop_correlated_indicators=FALSE,.na.rm=TRUE){
  
  
  
  # browser()
  print("----- Read Model -----")
  .mdl <- FOL_model_info(.model_definition_file)
  
  # 
  # 
  # .index_indicators <- read_model_definitions_indices_structure(.model_definition_file)
  # 
  # 
  # .weights <- read_weights(.model_definition_file)
  # 
  # 
  # .indexes_to_compute <- .index_indicatorss %>% map(~attr(.x,"index")) %>% unlist
  # 
  # names(.index_indicatorss) <- .indexes_to_compute
  # 
  # 
  # 
  
  # # .scenario_out <- .indexes_to_compute %>% map(~{
  # #   
  # #   
  # #   .out <- list(index=data.frame(),
  # #                coverage=data.frame(),
  # #                years=data.frame(),
  # #                quantitative_percent=data.frame(),
  # #                indicators_contributions=data.frame(),
  # #                log=c(),
  # #                scenario=.sheet)
  # #   
  # #   .index_indicators <- .index_indicatorss[[.x]]
  # #   
  # #   
  # #   .model_structure <- read_model_structure(.file=.model_definition_file,.index_indicators = .index_indicators)
  # #   
  # #   
  # #   
  # #   .column_definitions <- model.structure_to_column.definitions(.model.structure = .model_structure,
  # #                                                                .weights = .weights,
  # #                                                                .computing_function = compute_weighted_sum,
  # #                                                                .extra.parameters = list(.na.rm=.na.rm,
  # #                                                                                         .normalize=.normalize))
  # #   
  # #   
  # # })
  # 
  # .all_scales <- get_scales(.model_structure)
  
  # Read the data, preprocess it and store warnings in a log.
  # browser()
  read_and_preprocess_indicators_log <-  c()
  
  .mdl <- withCallingHandlers({
    # read_and_preprocess_indicators(.data_file=.data_file,
    #                                .sheet=.sheet,
    #                                .model_definition_file = .model_definition_file,
    #                                .normalize = .normalize),
    print("----- Load Data -----")
      load_data(.mdl,file=.data_file,sheet=.sheet)
      
    },
    read_MPA_unknown_indicator=function(cond){
      
      .log <- paste0("Unknown indicator '",cond$indicator,"' found. Ignoring it.")
      
      read_and_preprocess_indicators_log <<- append(read_and_preprocess_indicators_log,.log)
      cnd_muffle(cond)
      
    },
    read_MPA_scale_missing_for_indicator=function(cond){
      .log <- paste0("Indicator '",cond$indicator,"' is expected to depend on scale(s) ",paste0(cond$scales,collapse = ", "),", but some values don't. Please check your data.")
      
      read_and_preprocess_indicators_log <<- append(read_and_preprocess_indicators_log,.log)
      cnd_muffle(cond)
    },
    
    read_MAP_unexpected_scale_for_indicator=function(cond){
      .log <- paste0("Indicator '",cond$indicator,"' is NOT expected to depend on scale(s) ",paste0(cond$scales,collapse = ", "),", but some values do. Ignoring that/those scale(s) for that indicator.")
      
      read_and_preprocess_indicators_log <<- append(read_and_preprocess_indicators_log,.log)
      cnd_muffle(cond)
      
    },
    indicator_factors_to_numeric_indicator_missing=function(cond){
      .log <- paste0("Indicator '",cond$indicator,"' is missing.")
      
      read_and_preprocess_indicators_log <<- append(read_and_preprocess_indicators_log,.log)
      cnd_muffle(cond)
      
    },
    unexpected_qualitative_values_for_indicator=function(cond){
      .log <- paste0("Unexpected values '",paste0(cond$values,collapse = ", "),"' for indicator '",cond$indicator,"'. Removing from data.")
      
      read_and_preprocess_indicators_log <<- append(read_and_preprocess_indicators_log,.log)
      cnd_muffle(cond)
    },
    data_outside_normalization_limits=function(cond){
      .log <- paste0("Values '",paste0(unique(cond$values),collapse = ", "),"' of indicator '",cond$indicator,"' are outside their normalization range: ",paste0(sort(unique(unlist(cond$range))),collapse = ", "),". Replacing them with the range limits.")
      
      read_and_preprocess_indicators_log <<- append(read_and_preprocess_indicators_log,.log)
      cut_to_range(cond)
      cnd_muffle(cond)
    }
  )
  
  
  # Check species
  
  species_log <- c()
  
  .species_groups <- read_species_groups(.model_definition_file)
  
  
  .species_in_data  <- unique(input_data(.mdl)$SP %>% discard(is.na))
  if(!all(.species_in_data %in% c(unique(.species_groups$species)))){
    
    unknown_species <- setdiff(.species_in_data,c(unique(.species_groups$species)))
    withCallingHandlers(  
      warning(species_not_listed_in_model(unknown_species)),
      species_not_listed_in_model=function(cond){
        
        species_log <<- append(species_log,cond$message)
        
      }
      
    )
    
    
  }
  
  
  users_log <- c()
  
  .users_groups <- read_user_groups(.model_definition_file)
  
  .users_in_data  <- unique(c(input_data(.mdl)$RA %>% discard(is.na),input_data(.mdl)$PF %>% discard(is.na)))
  
  if(!all(.users_in_data %in% c(.users_groups$user_groups))){
    
    .unknown_user_groups <- setdiff(.users_in_data,c(.users_groups$user_groups))
    
    
    withCallingHandlers(  
      warning(user_groups_not_listed_in_model(.unknown_user_groups)),
      user_groups_not_listed_in_model=function(cond){
        
        users_log <<- append(users_log,cond$message)
        
      }
      
    )
    
  }
  
  
  
  
  habitats_log <- c()
  
  .habitats <- read_habitats(.model_definition_file)
  
  .habitats_in_data  <- unique(input_data(.mdl)$HB %>% discard(is.na))
  
  if(!all(.habitats_in_data %in% c(.habitats$habitats))){
    
    .unknown_habitats <- setdiff(.habitats_in_data,c(.habitats$habitats))
    
    
    withCallingHandlers(  
      warning(habitats_not_listed_in_model(.unknown_habitats)),
      habitats_not_listed_in_model=function(cond){
        
        habitats_log <<- append(habitats_log,cond$message)
        
      }
      
    )
    
    
    
    
    
    
    
  }
  
  
  # Drop correlated indicators within each dimension
  
  set.seed(2000)
  # if(.drop_correlated_indicators & !is.null(.model_structure)){
  #   .column_definitions <- drop_correlated_indicators(.df=.df_indicators,
  #                                                     .model_structure = .model_structure,
  #                                                     .column_definitions = .column_definitions,
  #                                                     .corr_lim = 0.8
  #   )
  # }
  # Compute the indexes
  
  compute_model_log <- c()
  print("----- Run Model -----")
  .mdl <- withCallingHandlers(model_run(.mdl),
                                             
                                             computing_function_column_does_not_exist=function(cond){
                                               
                                               # .log <- paste0("Indicator '",cond$missing_column,"' is missing.")
                                               
                                               # compute_model_log <<- append(compute_model_log,.log)
                                               drop_column(cond,silent=pluck(cond,"computing_function")=="compute_average")
                                               cnd_muffle(cond)
                                               
                                             })
  
  
  # .out[["MPA_SP_index"]] <- withCallingHandlers(compute_model_for_scales(.df=.df_indicators,
  #                                                                        .column.definitions = .column_definitions,
  #                                                                        .scales = .all_scales,
  #                                                                        .group_by_scales = c("MPA","SP")),
  #                                               
  #                                               computing_function_column_does_not_exist=function(cond){
  #                                                 
  #                                                 drop_column(cond)
  #                                                 cnd_muffle(cond)
  #                                                 
  #                                               })
  # 
  # .out[["MPA_HB_index"]] <- withCallingHandlers(compute_model_for_scales(.df=.df_indicators,
  #                                                                        .column.definitions = .column_definitions,
  #                                                                        .scales = .all_scales,
  #                                                                        .group_by_scales = c("MPA","HB")),
  #                                               
  #                                               computing_function_column_does_not_exist=function(cond){
  #                                                 
  #                                                 drop_column(cond)
  #                                                 cnd_muffle(cond)
  #                                                 
  #                                               })
  # 
  # .out[["MPA_UG_index"]] <- withCallingHandlers(compute_model_for_scales(.df=.df_indicators,
  #                                                                        .column.definitions = .column_definitions,
  #                                                                        .scales = .all_scales,
  #                                                                        .group_by_scales = c("MPA","UG")),
  #                                               
  #                                               computing_function_column_does_not_exist=function(cond){
  #                                                 
  #                                                 drop_column(cond)
  #                                                 cnd_muffle(cond)
  #                                                 
  #                                               })
  # 
  # .out[["MPA_HZ_index"]] <- withCallingHandlers(compute_model_for_scales(.df=.df_indicators,
  #                                                                        .column.definitions = .column_definitions,
  #                                                                        .scales = .all_scales,
  #                                                                        .group_by_scales = c("MPA","HZ")),
  #                                               
  #                                               computing_function_column_does_not_exist=function(cond){
  #                                                 
  #                                                 drop_column(cond)
  #                                                 cnd_muffle(cond)
  #                                                 
  #                                               })
  # 
  
  # Compute quality indicators
  
  # 
  # .model_structure <- read_model_structure(.model_definition_file)
  # 
  # # Years
  # 
  # .df_years <- read_MPA_data(.data_file,.sheet,"Year",.model_structure,"MPA")
  # 
  # .out[["MPA_years"]] <- compute_indicator_years(.df_years =.df_years,
  #                                                .df_indicators = .df_indicators,
  #                                                .column_definitions = .column_definitions,
  #                                                .all_scales = .all_scales,
  #                                                .group_by_scales="MPA")
  # 
  # 
  # # Coverage
  # 
  # .out[["MPA_coverage"]] <- compute_data_coverage(.df=.df_indicators,
  #                                                 .column_definitions = .column_definitions,
  #                                                 .all_scales = .all_scales,
  #                                                 .group_by_scales = "MPA")
  # 
  # # Quantitative indicators percent
  # 
  # 
  # .df_quantitative <- read_MPA_data(.data_file,.sheet,"Quantitative",.model_structure,"MPA")
  # 
  # .out[["MPA_quantitative_percent"]] <- compute_quantitative_percent(.df_quantitative = .df_quantitative,
  #                                                                    .df_indicators = .df_indicators,
  #                                                                    .column_definitions = .column_definitions,
  #                                                                    .all_scales=.all_scales,
  #                                                                    .group_by_scales = "MPA")
  # 
  # 
  # # Compute indicators contributions to the index
  # 
  # 
  # .out[["MPA_indicators_contributions"]] <- .df_indicators %<>% nest(scale_groups=-one_of("MPA")) %>% 
  #   rowwise() %>% 
  #   mutate(contributions=
  #            list(
  #              withCallingHandlers(compute_contribution_from_col_definitions(.column_definitions = .column_definitions,
  #                                                                            .df=scale_groups,
  #                                                                            .na.rm = TRUE,
  #                                                                            .normalize = .normalize),
  #                                  computing_function_column_does_not_exist=function(cnd) drop_column(cnd)
  #              )
  #            )
  #   )
  # 
  # .out[["MPA_indicators_contributions"]] %<>% select(-scale_groups)
  # 
  # Logs
  
  attr(.mdl,"log") <- unique(c(read_and_preprocess_indicators_log,species_log,users_log,habitats_log,compute_model_log))
  
  .mdl
  
}


#' Processes all the scenarios in one data file and returns a list of scenarios with their data
#' 
#' @param .data_file (character) path to file with the data
#' @param .model_definition_file (character) path to model definition file
#' @param .normalize (logical) if true, the results at each step are normalized.
#' @param .drop_correlated_indicators (logical)
#' @param .updateProgress (function) called to update a progress object after each scenario is processed
#' @return (list)
process_data_file <- function(
  .data_file,
  .model_definition_file,
  .normalize,
  .drop_correlated_indicators=FALSE,
  updateProgress=NULL,
  .na.rm=TRUE){
  .out <- list()

  
  
  .scenarios <- readxl::excel_sheets(.data_file)  
  # .scenarios <- validate_data_file(.data_file)
  
  .out <- .scenarios  %>% map2(1:length(.),~{
    print(paste0("##### ",.x," #####"))
    if(is_function(updateProgress)){
      text <- paste0("Processing scenario: ",.x)
      updateProgress(value=.y/length(.scenarios) ,detail=text)
    }
    .r <- try(
      withCallingHandlers(
        process_scenario(.data_file=.data_file,
                         .sheet=.x,
                         .model_definition_file = .model_definition_file,
                         .drop_correlated_indicators = .drop_correlated_indicators,
                         .normalize = .normalize,
                         .na.rm=.na.rm),
        normalization_limits_missing_error=function(cnd){
          
          warning(normalization_limits_missing_warning(.missing_limits = cnd$missing_limits,.scenario = .x))
          
        },
        
        error=function(cnd){
          
          
          warning(unknown_error_processing_scenario(.scenario=.x,.cnd = cnd))
          
        }
      )
    )
    
    if(inherits(.r,"try-error")){
      NULL
    }else{
      .r
    }
  })
  
  .out %<>% set_names(.scenarios)
  
  
  .out
}

##### PLOT FUNCTIONS #####

.dimensions_abrv <- c("Social sensitivity"="SS","Ecological vulnerability"="EV","Ecological sensitivity"="ES","Exposure"="EX","Adaptive capacity"="AC","Total"="Total")

.dimensions_color_scale <- data.frame(group=forcats::as_factor(c("Social sensitivity","Ecological vulnerability","Adaptive capacity")),color=c("#0064B4","#5DA446","#FFB92E"),stringsAsFactors = FALSE)

.ecological_vul_color_scale <- data.frame(group=forcats::as_factor(c("Ecological sensitivity","Exposure","Adaptive capacity")),color=c("#467C35","#559841","#80B373"),stringsAsFactors = FALSE)

.low_vul_color <- "#5DA446"
.mod_vul_color <- "#E4E2E2"
.hi_vul_color <-  "#4890CC" 
.ext_vul_color <- "#0064B4"

.ext_vul_label <- "Extreme vulnerability"
.hi_vul_label <- "High vulnerability"
.mod_vul_label <- "Moderate vulnerability"
.low_vul_label <- "Low vulnerability"

.vul_labels <- c("Low vulnerability","Moderate vulnerability","High vulnerability","Extreme vulnerability")

.vulnerability_color_scale <- data.frame(group=forcats::as_factor(c(.ext_vul_label,.hi_vul_label,.mod_vul_label,.low_vul_label)),
                                         color=c(.ext_vul_color,.hi_vul_color,.mod_vul_color,.low_vul_color),stringsAsFactors = FALSE) 

.low_vul_upper_limit <- 0.25
.mod_vul_upper_limit <- 0.5
.hi_vul_upper_limit <- 0.75
#' Returns a color for the vulnerability score
.vulnerability_group <- function(.score) case_when(.score <=.low_vul_upper_limit ~ .low_vul_label,
                                                   .score <=.mod_vul_upper_limit ~ .mod_vul_label,
                                                   .score <=.hi_vul_upper_limit ~ .hi_vul_label,
                                                   TRUE ~ .ext_vul_label)


.vulnerability_color <- function(.score) case_when(.score <=.low_vul_upper_limit ~ .low_vul_color,
                                                   .score <=.mod_vul_upper_limit ~ .mod_vul_color,
                                                   .score <=.hi_vul_upper_limit ~ .hi_vul_color,
                                                   TRUE ~ .ext_vul_color)

.combine_lists <- function(default,override){
  
  result <- default
  
  elements_to_keep <- names(result) %>% keep(~!.x %in% names(override) )
  result <- c(result[elements_to_keep],override)
  
  result
}


#' Given a set of vulnerability scores and a MPA name
#' produce a donut graph showing the vulverabilities for each 
#' group.
#' 
#' @param .df (data.frame) with columns, bar_title, score, color
#' @param .center_score (numeric) score to be shown in the middle of the graph.
#' @param .title (character) Graph title.
#' @param .central_score_color (character) used for the centralscore color.
#' @param .central_score_subtitle (character) text to plot under the central score
#' @return (ggplot graph)
#donut_graph <- function(.df, 
#                        .center_score, 
#                        .title,
#                        .central_score_color,
#                        .central_score_subtitle,
#                        ...){
#  args <- list(
#    .donut_graph_central_score_size=45,
#    .donut_graph_central_score_subtitle_size=18,
#    .donut_graph_bar_labels_size=9,
#    .donut_graph_lengend_text_size=20,
#    .donut_graph_legend_spacing=1.0,
#    .donut_graph_plot_title_size=30)
  
  
  
#  arguments <- list(...)
  
# for(arg in names(arguments)){
#    args[arg] <- arguments[arg]
    
#  }
  
#  for(arg in names(args)){
#    assign(arg,args[[arg]])
    
#  }
  
  
  
  
#  .to.plot <- .df %>% mutate(fraction=score/sum(score), 
#                             ymax=cumsum(fraction),
#                             ymin=c(0,head(ymax,n=-1))) %>%
#    mutate(bar_title=factor(bar_title,levels=bar_title))
  
#  .labels <- .to.plot %>% mutate(label=.to.plot$score,x=4,y=ymin+fraction/2)
  
#  .central_label <- data.frame(label=.center_score,x=0,y=0)
#  .central_subtitle <- data.frame(label=.central_score_subtitle,x=0,y=0)
  
#  ggplot(.to.plot,aes(ymax=ymax,ymin=ymin,xmax=4.75,xmin=3.25,fill=bar_title)) +
#    geom_rect(color="white")+
#    coord_polar(theta = "y")+
#    geom_text(data=.labels,aes(x=x,y=y,label=label),inherit.aes = FALSE,color="black",fontface="bold",family="Calibri",size=.donut_graph_bar_labels_size)+
#    geom_text(data=.central_label,aes(x=x,y=y,label=label),inherit.aes = FALSE,color=.central_score_color,size=.donut_graph_central_score_size,nudge_x = 0.4,fontface="bold",family="Calibri")+
#    geom_text(data=.central_subtitle,aes(x=x,y=y,label=label),inherit.aes = FALSE,color=.central_score_color,size=.donut_graph_central_score_subtitle_size,nudge_x = -1,vjust=2,fontface="bold",family="Calibri")+
#    scale_fill_manual(values=.to.plot$color)+
#    theme_void()+
#    xlim(c(-1,5))+
#    ggtitle(.title) +
#    theme(legend.title = element_blank(),
#          plot.title = element_text(hjust=0.5,vjust=-4,size = .donut_graph_plot_title_size,face="bold",family="Calibri"),
#          legend.position = "bottom",
#          legend.margin = margin(),
#          legend.box.margin = margin(-20),
#          legend.text = element_text(size=.donut_graph_lengend_text_size),
#          legend.spacing.x = unit(.donut_graph_legend_spacing, 'cm'))
  
  
  
#}

#' Plots a circle and inside of it the value of an indicator and its units. It also shows a title.
#' 
#' @param .indicator (quality.indicator.display.definition) indicator to display
#' @return (ggplot)
#quality_indicator_display <- function(.indicator){
  
  
#  .out <- ggplot() + 
#    geom_circle(aes(x0=0,y0=0,r=1),color=.indicator$circle_color,size=2) + 
#    do.call(geom_text,c(list(mapping=aes(x=0,y=0.3,label=.indicator$indicator$value)),.indicator$indicator$text_properties))+
#    do.call(geom_text,c(list(mapping=aes(x=0,y=-0.5,label=.indicator$subtitle$value)),.indicator$subtitle$text_properties))+
#    ggtitle(.indicator$title$value)+
#    theme_void()+
#    theme(plot.title=do.call(element_text,c(.indicator$title$text_properties)))
  
  
  
#  .out
  
#}


## BY XAN ####################################################################################################
donut_graph <- function (Perspective="Ecological", ## 
                         MPA.wanted="Baix Emporda", 
                         Scenario.wanted="RCP85_2050")
{
  ## Perspective: categorical, choose between "Ecological" or "Social"
  ## MPA: categorical, name of the MPA to plot. Ej. "Baix Emporda"
  ## Scenario: categorical, RCP_year. Ej. "RCP85_2050"
  
  ## Read data for the donut
  if (Perspective == "Ecological") {df <- readxl::read_excel (path="inputs/EcologicalVulnerability.xlsx")}
  if (Perspective == "Social")     {df <- readxl::read_excel (path="inputs/SocioEcologicalVulnerability.xlsx")}
  
  ## Transform tibble to data frame
  df <- as.data.frame (df)
  
  df [df == "Low"] <- "Low (percentile < 20)"
  df [df == "Intermediate"] <- "Intermediate (percentile 20 - 40)"
  df [df == "High"] <- "High (percentile 40 - 60)"
  df [df == "Very high"] <- "Very high (percentile 60 - 80)"
  df [df == "Extreme"] <- "Extreme (percentile > 80)"
  
  
  ## Subset the data frame to the MPA and scenario wanted
  df1 <- subset (x=df, MPA==MPA.wanted)
  df2 <- subset (x=df1, Scenario==Scenario.wanted)
  
  ## Extract values for the plot and prepare df for the ggplot
  AC   <- df2$EcologicalAdaptiveCapacity
  S    <- df2$EcologicalSensitivity
  E    <- df2$Exposure
  
  Q_AC <- df2$Q_EcologicalAdaptiveCapacity
  Q_S  <- df2$Q_EcologicalSensitivity
  Q_E  <- df2$Q_Exposure
  
  
  if (Index == "LOW (PERCENTILE < 20)") col.Index <- "olivedrab4"
  if (Index == "INTERMEDIATE (PERCENTILE 20 - 40)") col.Index <- "#0073C2FF"
  if (Index == "HIGH (PERCENTILE 40 - 60)") col.Index <- "#EFC000FF"
  if (Index == "VERY HIGH (PERCENTILE 60 - 80)") col.Index <- "darkorange2"
  if (Index == "EXTREME (PERCENTILE > 80)") col.Index <- "#CD534CFF"
  
  Index    <- toupper (df2$Index)
  Index_short <- substr(Index, start=1, stop= unlist(gregexpr(pattern=" \\(", Index)) -1 )
  
  df.plot <- data.frame ("Dimension"=c("AC", "S", "E"), "Value"=c(AC, S, E), "Category"=c(Q_AC, Q_S, Q_E))
  
  ## Color for the categories
  df.plot$Color <- rep(NA, 3)
  df.plot$Color [which (df.plot$Category == "Low (percentile < 20)")] <- "olivedrab4" 
  df.plot$Color [which (df.plot$Category == "Intermediate (percentile 20 - 40)")] <- "#0073C2FF" #blue
  df.plot$Color [which (df.plot$Category == "High (percentile 40 - 60)")] <- "#EFC000FF" #yellow
  df.plot$Color [which (df.plot$Category == "Very high (percentile 60 - 80)")] <- "darkorange2"
  df.plot$Color [which (df.plot$Category == "Extreme (percentile > 80)")] <- "#CD534CFF" #red
  
  if ( length(grep ("RCP85", Scenario.wanted)) == 1) {.plot.title1 <- gsub ("RCP85_", "RCP 8.5 - ", Scenario.wanted)}
  if ( length(grep ("RCP45", Scenario.wanted)) == 1) {.plot.title1 <- gsub ("RCP45_", "RCP 4.5 - ", Scenario.wanted)}
  
  
  .plot.title <- paste (paste (paste (MPA.wanted, .plot.title1, sep=", "), Perspective, sep =" - "), "vulnerability", paste =" ")
  
  
  ##############################################################################
  library(dplyr)
  #breaks       = c( 0, 0.1, 0.2, 0.3, 0.4,0.6)
  breaks       = c( "Low (percentile < 20)", "Intermediate (percentile 20 - 40)", "High (percentile 40 - 60)", "Very high (percentile 60 - 80)", "Extreme (percentile > 80)")
  df.plot$bins <- match (df.plot$Category, breaks)
  Colors <- c("olivedrab4", "#0073C2FF", "#EFC000FF", "darkorange2", "#CD534CFF")
  df.plot$bins <- factor(df.plot$Category, levels=breaks)
  df.plot$Category_short <- substr(df.plot$Category, start=1, stop= unlist(gregexpr(pattern=" \\(", df.plot$Category)) )
  ##############################################################################
  ## The plot
  library(ggplot2)
  
  plot.name <- paste (paste("figures/", .plot.title, sep=""), ".png", sep="")
  png (plot.name, width=2400, height=2400, unit="px", res=350)
  
  
  donut <- ggplot (data=df.plot, aes(x=2, y=Value, fill=bins)) + ## values of the donut
    geom_bar(stat = "identity", color = "white") +
    coord_polar(theta = "y", start = 0) +
    geom_text(aes(label = Dimension), position=position_stack(vjust=0.5), color = "white", fontface="bold", size=8) +
    geom_text(data=df.plot, aes(x=0.6, y=0.0, label="Vulnerability"), inherit.aes = FALSE, color="black",   fontface="bold", size=8) +
    geom_text(data=df.plot, aes(x=0.1, y=0.0, label=Index_short)          , inherit.aes = FALSE, color=col.Index, fontface="bold", size=10) +
    scale_fill_manual("Categories", values = Colors, drop = FALSE) + 
    theme_void() + ## clean the background
    xlim(0.1, 2.5) +
    ggtitle(.plot.title) +
    theme (plot.title = element_text(hjust=0.5, vjust=-4, size = 15, face="bold"), 
           legend.position="right",
           legend.text = element_text(size=12),
           legend.title = element_text(size=16))
  print (donut)
  
  
  dev.off()  
}

## CALL THE FUNCTION
# donut_xan(Perspective = "Ecological", MPA.wanted = "Baix Emporda", Scenario.wanted = "RCP85_2050")
# donut_xan(Perspective = "Ecological", MPA.wanted = "Baix Emporda", Scenario.wanted = "RCP85_2100")



#' Given data for 4 indicators, it plots 4 indicators displays created with quality_indicator_display
#' and puts them together in a group.
#' 
#' @param .indicators_data (quality.indicator.display.definition list)
#' @return (gtable)
quality_indicators_group_display <- function(.indicators_data){
  
  
  .out <- ggplot()
  
  .indicators_display_list <- .indicators_data %>% map(quality_indicator_display)
  
  .layout=matrix(c(1:8),byrow = TRUE,nrow = 2)
  
  if(length(.indicators_display_list) !=8){
    .out <-withRestarts(
      stop(quality_indicators_group_display_wrong_number_of_indicators_error(.n_indicators_required = 8, .indicators_display_list = .indicators_display_list)),
      new_layout = function(.cnd,.indicators_display_list,.layout,...){
        
        arrangeGrob(grobs=.indicators_display_list,layout_matrix=.layout,...)
      }
    )
  }else{
    
    .out <- arrangeGrob(grobs=.indicators_display_list,layout_matrix = .layout)
  }
  
  .out
}

#' Given a data.frame with coverage information and color for each bar, plot
#' an horizontal barplot showing the coverage.
#' @param .df (data.frame) columns: label, coverage (in range 0-1), color
#' @param .xbreaks (numeric) x axis breaks
#' @return (ggplot)
coverage_barplot <- function(.df,.xbreaks=c(seq(0,1,0.2)),.x_axis_text_size=14,.y_axis_text_size=14){
  
  .df %<>% mutate(label=factor(label,levels=rev(label)))
  
  .out <- .df %>% ggplot(aes(x=label,y=coverage,color=color,fill=color))+
    geom_col() +
    scale_color_manual(values = as.character(.df$color),breaks = as.character(.df$color))+
    scale_fill_manual(values = as.character(.df$color),breaks = as.character(.df$color))+
    coord_flip(clip="off")+
    scale_y_continuous(expand = expansion(0,0),limits = c(0,1),breaks = .xbreaks,labels = scales::percent)+
    ggtitle("Data Coverage")+
    ylab(NULL)+
    theme_void() +
    theme(axis.line = element_line(color="black",size=1,linetype = "solid"),
          axis.text.y = element_text(color="gray40",size=.y_axis_text_size,margin=margin(r=0.1,unit="cm")),
          axis.text.x = element_text(color="gray40",size=.x_axis_text_size,margin = margin(r=0.2,t=0.2,unit="cm")),
          title = element_text(color="gray40",size=16,margin = margin(r=0.2,t=0.2,unit="cm")),
          panel.grid.major.x = element_line(color="gray",linetype="dashed"),
          legend.position = "none",
          plot.margin = margin(0.5,1.0,0.5,0.7,"cm")
          
    )
  
  
  .out
}


#' Given coverage and quality indicators data,it plots a quality panel.
#' 
#' @param .coverage (data.frame) same as for coverage_barplot
#' @param .xbreaks (numeric) x axis breaks for the coverage barplot
#' @param .indicators_data (quality.indicator.display.definition list) same as for quality_indicatos_group_display
#' @return (gtable)
quality_panel <- function(.coverage,.indicators_data,.xbreaks=c(seq(0,1,0.2)),.coverage_x_axis_text_size=14,.coverage_y_axis_text_size=14){
  
  coverage_graph <- coverage_barplot(.df = .coverage,
                                     .xbreaks = .xbreaks,
                                     .x_axis_text_size=.coverage_x_axis_text_size,
                                     .y_axis_text_size=.coverage_y_axis_text_size)
  
  indicators_group <- quality_indicators_group_display(.indicators_data = .indicators_data)
  
  .out <-arrangeGrob(coverage_graph,indicators_group,nullGrob(),layout_matrix = matrix(c(1,1,1,1,1,1,
                                                                                         3,2,2,2,2,2),byrow = TRUE,nrow=2),heights = c(1/2,1/2))
  
  .out
  
}

#' Given a set of labels and their scores, plots a horizontal bar graph.
#' Each label has a color based on a scale and optionally an icon.
#' 
#' @param .df (data.frame) columns labels, score, group, icon (the path of an image to plot next to the bar)
#' @param .color_scale (data.frame) group colors
#' @param .x_axis_titel (character)
#' @param .show_icons (logical) whether to show the icons in the icon column
#' @param .xlimits (numeric) 2 numeric vector for the x axis limit
#' @param .xbreaks (numeric) x axis breaks
#' @param .icon_asp (numeric) icons aspecto ratio
#' @param .icon_size (numeric) icon size
#' @return (ggplot graph)
horizontal_bar_plot <- function(.df,.color_scale,.x_axis_title,.show_icons=FALSE,.xlimits=c(0,1),.xbreaks=c(seq(0,1,0.2)),.icon_asp=1,.icon_size=0.1,...){
  
  draw_key_polygon3 <- function(data, params, size) {
    lwd <- min(data$size, min(size) / 4)
    
    grid::rectGrob(
      width = grid::unit(0.6, "npc"),
      height = grid::unit(0.6, "npc"),
      gp = grid::gpar(
        col = data$colour,
        fill = alpha(data$fill, data$alpha),
        lty = data$linetype,
        lwd = lwd * .pt,
        linejoin = "mitre"
      ))
  }
  
  # register new key drawing function, 
  # the effect is global & persistent throughout the R session
  GeomBar$draw_key = draw_key_polygon3
  
  
  g <-  .df %>% mutate(label=forcats::fct_rev(label)) %>% ggplot(aes(x=label,y=score,color=group,fill=group)) +
    geom_bar(stat="identity",width = 0.7)+
    scale_color_manual(values = as.character(.color_scale$color),breaks = as.character(.color_scale$group),drop=FALSE)+
    scale_fill_manual(values = as.character(.color_scale$color),breaks = as.character(.color_scale$group),drop=FALSE)+
    coord_flip(clip="off")+
    scale_y_continuous(expand = expansion(0,0),limits = .xlimits,breaks = .xbreaks)+
    ylab(.x_axis_title)+
    theme_void() +
    theme(axis.line = element_line(color="black",size=1,linetype = "solid"),
          axis.text = element_text(color="gray40",size=22,margin = margin(r=0.2,t=0.2,unit="cm")),
          axis.title.x  = element_text(color="gray40",size=24,margin = margin(t=0.3,unit="cm")),
          panel.grid.major.x = element_line(color="gray",linetype="dashed"),
          legend.position = "right",
          legend.box.spacing  = unit(1,"cm"),
          legend.text = element_text(size=18),
          legend.key = element_rect(color = NA, fill = NA),
          legend.key.size = unit(1.5, "cm"),
          legend.title = element_blank(),
          plot.margin = margin(0.5,0.5,0.5,0.5,"cm")
          
    )
  
  if(.show_icons  & "icon" %in% names(.df)){
    g <- g + geom_image(aes(image=icon),nudge_y = 0.1,size=.icon_size,asp=.icon_asp,color="black")
  }
  
  g
  
}



#' Given a set of labels and their scores, plots a vertical bar graph.
#' Each label has a color based on a scale and optionally an icon.
#' 
#' @param .df (data.frame) columns labels, score, group, icon (the path of an image to plot next to the bar)
#' @param .color_scale (data.frame) group colors
#' @param .y_axis_title (character)
#' @param .show_icons (logical) whether to show the icons in the icon column
#' @param .stack_by (character) column in .df used to group the data and produce a stacked bar chart.
#' @param .ylimits (numeric) 2 numeric vector for the y axis limit
#' @param .ybreaks (numeric) y axis breaks
#' @param .icon_offset (numeric) nudget for icons
#' @param .icon_asp (numeric) icons aspecto ratio
#' @param .icon_size (numeric) icon size
#' @return (ggplot graph)
vertical_bar_plot <- function(.df,.color_scale,.y_axis_title,.show_icons=FALSE,.stack_by=NULL,.ylimits=c(0,1),.ybreaks=c(seq(0,1,0.2)),.icon_offset=0.1,.icon_asp=1,.icon_size=0.1,...){
  draw_key_polygon3 <- function(data, params, size) {
    lwd <- min(data$size, min(size) / 4)
    
    grid::rectGrob(
      width = grid::unit(0.6, "npc"),
      height = grid::unit(0.6, "npc"),
      gp = grid::gpar(
        col = data$colour,
        fill = alpha(data$fill, data$alpha),
        lty = data$linetype,
        lwd = lwd * .pt,
        linejoin = "mitre"
      ))
  }
  
  # register new key drawing function, 
  # the effect is global & persistent throughout the R session
  GeomBar$draw_key = draw_key_polygon3
  
  x_var <- "label"
  if(!is.null(.stack_by)){
    x_var <- .stack_by
    
  }
  
  
  g <-  .df  %>% ggplot(aes_string(x=x_var,y="score",color="group",fill="group")) +
    geom_bar(stat="identity",width = 0.7)+
    scale_color_manual(values = as.character(.color_scale$color),breaks = as.character(.color_scale$group),drop=FALSE)+
    scale_fill_manual(values = as.character(.color_scale$color),breaks = as.character(.color_scale$group),drop=FALSE)+
    scale_y_continuous(expand = expansion(0,0),limits = .ylimits,breaks = .ybreaks)+
    ylab(.y_axis_title)+
    coord_cartesian(clip = "off")+
    theme_void() +
    theme(axis.line = element_line(color="black",size=1,linetype = "solid"),
          axis.text.x = element_text(color="gray40",size=22,margin = margin(t=0.2,unit="cm"),angle=90),
          axis.text.y = element_text(color="gray40",size=22,margin = margin(r=0.2,unit="cm")),
          axis.title.y  = element_text(color="gray40",size=24,angle = 90,margin = margin(r=0.5,unit = "cm")),
          panel.grid.major.y = element_line(color="gray",linetype="dashed"),
          legend.position = "right",
          legend.box.spacing  = unit(1,"cm"),
          legend.text = element_text(size=18),
          legend.key = element_rect(color = NA, fill = NA),
          legend.key.size = unit(1.5, "cm"),
          legend.title = element_blank(),
          plot.margin = margin(0.5,0.5,0.5,0.5,"cm")
          
    )
  
  if(.show_icons & "icon" %in% names(.df)){
    
    if(!is.null(.stack_by)){
      x_var <- .stack_by
      img_data <- .df %>% group_by_at(vars(one_of(c(.stack_by,"icon")))) %>% summarise(score=sum(score,na.rm = TRUE)) %>% ungroup
    }else{
      img_data<- .df
    }
    
    g <- g + geom_image(data=img_data,aes_string(x=x_var,y="score",image="icon"),inherit.aes = FALSE,nudge_y = .icon_offset,size=.icon_size,asp=.icon_asp,color="black")
  }
  
  g
}

#' Given some defaults, a plot function and some data, run the plot function
#' 
#' 
#' @param .df (data.frame) data.frame
#' @param .plot_function (function) plot function
#' @param .default_plot_params (list) passed to the plot function
#' @param ... used to override plot params
#' @return (ggplot) graph
run_plot_function <- function(.df,.plot_function,.default_plot_params,...){
  
  .override_param <- list(...)
  
  .plot_parameters <- .combine_lists(.default_plot_params,.override_param)
  
  
  .plot_parameters <- c(list(.df=.df),.plot_parameters)
  
  g <- do.call(.plot_function,.plot_parameters)
  
  g
  
}

#' Plots a key indicators graph
#' 
#' @param .df (data.frame) data.frame with the same structure as .df in horizontal_bars_plot. No icon column required.
#' @param ... parameters passed to horizontal_bars_plot
#' @return (ggplot) graph
key_indicators_graph <- function(.df,...){
  
  .key_indicators_color_scale <- .dimensions_color_scale
  
  
  graph_parameters <- list(.color_scale=.key_indicators_color_scale,
                           .show_icons=FALSE,
                           .x_axis_title="Indicators scores")
  
  g <- run_plot_function(.df=.df,
                         .plot_function = horizontal_bar_plot,
                         .default_plot_params = graph_parameters,
                         ...)
  
  g
  
}



#' Plots a habitats vulnerability graph
#' 
#' @param .df (data.frame) data.frame with the same structure as .df in vertical_bars_plot. group
#' is computed from the score
#' @param ... parameters passed to vertical_bars_plot
#' @return (ggplot) graph
habitats_vulnerability_graph <- function(.df,...){
  
  .habitats_vulnerability_color_scale <- .vulnerability_color_scale
  
  .to.plot <- .df %>% mutate(group=.vulnerability_group(score), group=factor(group,levels=.vul_labels))
  
  .to.plot %<>% filter(label!="Other")
  
  graph_parameters <- list(.color_scale=.habitats_vulnerability_color_scale,
                           .show_icons=TRUE,
                           .y_axis_title="VUL scores"
  )
  
  
  g <- run_plot_function(.df=.to.plot,
                         .plot_function = vertical_bar_plot,
                         .default_plot_params = graph_parameters,
                         ...)
  
  g
  
  
}


#' Plots a species vulnerability graph
#' 
#' @param .df (data.frame) data.frame with the same structure as .df in vertical_bars_plot. group
#' is computed from the score
#' @param ... parameters passed to horizontal_bars_plot
#' @return (ggplot) graph
species_vulnerability_graph <- function(.df,...){
  
  .species_vulnerability_color_scale <- .vulnerability_color_scale
  
  .to.plot <- .df %>% mutate(group=.vulnerability_group(score), group=factor(group,levels=.vul_labels))
  
  .to.plot %<>% filter(label!="Other")
  
  graph_parameters <- list(.color_scale=.species_vulnerability_color_scale,
                           .show_icons=TRUE,
                           .x_axis_title="Vulnerability scores"
  )
  
  
  g <- run_plot_function(.df=.to.plot,
                         .plot_function = horizontal_bar_plot,
                         .default_plot_params = graph_parameters,
                         ...)
  
  g
  
  
}

#' Plots a user group vulnerability graph
#' 
#' @param .df (data.frame) data.frame with the same structure as .df in vertical_bars_plot. group
#' is computed from the score
#' @param ... parameters passed to vertical_bars_plot
#' @return (ggplot) graph
user_groups_vulnerability_graph <- function(.df,...){
  
  .user_groups_vulnerability_color_scale <- .vulnerability_color_scale
  
  .to.plot <- .df %>% mutate(group=.vulnerability_group(score), group=factor(group,levels=.vul_labels))
  
  .to.plot %<>% filter(label!="Other")
  
  graph_parameters <- list(.color_scale=.user_groups_vulnerability_color_scale,
                           .show_icons=TRUE,
                           .y_axis_title="VUL scores")
  
  
  g <- run_plot_function(.df=.to.plot,
                         .plot_function = vertical_bar_plot,
                         .default_plot_params = graph_parameters,
                         ...)
  
  g
  
  
}



#' Plots a species groups vulnerability graph
#' 
#' @param .df (data.frame) data.frame with the same structure as .df in vertical_bars_plot. Species are stacked
#' using the species_group column
#' @param ... parameters passed to vertical_bars_plot
#' @return (ggplot) graph
species_groups_vulnerability_graph <- function(.df,...){
  
  .species_groups_vulnerability_color_scale <- .vulnerability_color_scale
  
  .to.plot <- .df 
  
  .to.plot %<>% filter(label!="Other")
  
  
  
  .to.plot %<>% select(-species_vul) %>% distinct()
  
  .to.plot %<>% group_by(species_group) %>%
    mutate(n=n(),score=score/n,total_group=sum(score,na.rm=TRUE))  %>% ungroup
  
  
  
  .to.plot %<>% arrange(desc(total_group)) %>% mutate(group=factor(group,levels=rev(.vul_labels)),species_group=factor(species_group,levels=unique(species_group)))
  
  # .upper_limit <- .to.plot %>% 
  #   group_by(species_group) %>% 
  #   summarise(total=sum(score,na.rm = TRUE)) %>% 
  #   pull(total) %>% 
  #   max %>%
  #   add(2)
  # 
  
  
  .upper_limit <- 1
  
  graph_parameters <- list(.color_scale=.species_groups_vulnerability_color_scale,
                           .show_icons=TRUE,
                           .y_axis_title="VUL scores",
                           .stack_by="species_group",
                           .ylimits=c(0,.upper_limit),
                           .ybreaks=seq(0,.upper_limit,0.1)
                           
  )
  
  
  g <- run_plot_function(.df=.to.plot,
                         .plot_function = vertical_bar_plot,
                         .default_plot_params = graph_parameters,
                         ...)
  
  g
  
  
}


#' Plots a MPA donut vulnerability graph with a quality_panel
#' 
#' @param .df (data.frame) data.frame with the same structure as .df in donut_graph
#' @param .MPA_name (character) graph title
#' @param .MPA_vulnerability (numeric) MPA vulnerability score
#' @param .MPA_vul_color (character) color of the vulnerability score
#' @param .coverage (data.frame) same as for coverage_barplot
#' @param .indicators_data (quality.indicator.display.definition list) same as for quality_indicatos_group_display
#' @param .xbreaks (numeric) x axis breaks for the coverage barplot
#' @param .donut_graph_params (list) parameters passed to donut_graph
#' @return (gtable)
MPA_donut_graph_and_quality_panel <- function(.df,.MPA_name,.MPA_vulnerability,.MPA_vul_color,.coverage,.indicators_data,.xbreaks=c(seq(0,1,0.2)),.donut_graph_params=list(),.coverage_x_axis_text_size=14,.coverage_y_axis_text_size=14){
  
  .donut_graph <- do.call(MPA_donut_graph,c(list(.df,.MPA_name,.MPA_vulnerability,.MPA_vul_color),.donut_graph_params))
  
  .donut_graph <- .donut_graph + theme(legend.position = c(0.8,0),
                                       legend.direction = "horizontal",
                                       plot.margin = margin(r=3,unit = "in"))
  
  .quality_panel <- quality_panel(.coverage,.indicators_data,.xbreaks,
                                  .coverage_x_axis_text_size=.coverage_x_axis_text_size,
                                  .coverage_y_axis_text_size=.coverage_y_axis_text_size)
  
  
  
  
  .out <- qplot(x=c(0,12),y=c(0,7),geom = "blank") + 
    theme_void() + 
    theme(plot.margin = margin(0,0,0,0,"cm"))+
    scale_y_continuous(expand = expansion(0,0))+
    scale_x_continuous(expand = expansion(0,0))+
    annotation_custom(ggplotGrob(.donut_graph),xmin=0,xmax=11,ymin=0,ymax=7)+
    annotation_custom(.quality_panel,xmin=6.5,xmax=12,ymin=0.75,ymax=7)
  # .out <- arrangeGrob(.donut_graph,.quality_panel,layout_matrix = matrix(c(1,2),byrow = TRUE,nrow = 1),widths = c(7/10,3/10))
  
  .out
  
}



#' Plots a MPA donut vulnerability graph
#' 
#' @param .df (data.frame) data.frame with the same structure as .df in donut_graph
#' @param .MPA_name (character) graph title
#' @param .MPA_vulnerability (numeric) MPA vulnerability score
#' @param .MPA_vul_color (character) color of the vulnerability score
#' @param ... parameters passed to donut_graph
#' @return (ggplot) graph
MPA_donut_graph <- function(.df,.MPA_name,.MPA_vulnerability,.MPA_vul_color,...){
  
  
  .to.plot <- .df
  
  .MPA_vul_subtitle <- .vulnerability_group(.score=.MPA_vulnerability) %>% str_split(" ") %>% extract2(1) %>% extract2(1)
  
  
  
  graph_parameters <- list(.center_score=.MPA_vulnerability, 
                           .title=.MPA_name,
                           .central_score_color=.MPA_vul_color,
                           .central_score_subtitle=.MPA_vul_subtitle
  )
  
  
  g <- run_plot_function(.df=.to.plot,
                         .plot_function = donut_graph,
                         .default_plot_params = graph_parameters,
                         ...)
  
  # draw_key_polygon3 <- function(data, params, size) {
  #   lwd <- min(data$size, min(size) / 4)
  #   
  #   grid::rectGrob(
  #     width = grid::unit(0.6, "npc"),
  #     height = grid::unit(0.6, "npc"),
  #     gp = grid::gpar(
  #       col = data$colour,
  #       fill = alpha(data$fill, data$alpha),
  #       lty = data$linetype,
  #       lwd = lwd * .pt,
  #       linejoin = "mitre"
  #     ))
  # }
  # 
  # # register new key drawing function, 
  # # the effect is global & persistent throughout the R session
  # GeomBar$draw_key = draw_key_polygon3
  # 
  # 
  # legend_g <-.vulnerability_color_scale %>% mutate(x=1,y=1) %>% ggplot(aes(x=x,y=y,color=group,fill=group))+ geom_polygon() +
  #   scale_color_manual(guide=guide_legend(title="Vulnerability"),values = as.character(.vulnerability_color_scale$color),breaks = as.character(.vulnerability_color_scale$group))+
  #   scale_fill_manual(guide=guide_legend(title="Vulnerability"),values = as.character(.vulnerability_color_scale$color),breaks = as.character(.vulnerability_color_scale$group))+
  #   theme(legend.margin = margin(),
  #         legend.text = element_text(size=18),
  #         legend.title = element_text(size=22),
  #         legend.position = c(2,0.7))
  # 
  # 
  # legend_grobs <- ggplotGrob(legend_g)$grobs
  # # get the legend
  # legend <- legend_grobs[[which(sapply(legend_grobs, function(x) x$name) == "guide-box")]]
  # 
  # #get legend dimensions
  # lheight <- sum(legend$height)
  # lwidth <- sum(legend$width)
  # 
  # 
  
  # g <- arrangeGrob(
  #   legend,
  #   g,
  #   ncol = 2,
  #   widths = c(0.1,0.9))
  
  g
  
}



#' Plots a MPA social-ecological vulnerability graph
#' 
#' @param .df (data.frame) data.frame with the same structure as .df in donut_graph, except for color
#' @param .MPA_name (character) graph title
#' @param .MPA_vulnerability (numeric) MPA vulnerability score
#' @param .coverage (data.frame) same as for coverage_barplot
#' @param .indicators_data (quality.indicator.display.definition list) same as for quality_indicatos_group_display
#' @param .donut_graph_params (list) parameters passed to donut_graph
#' @return (ggplot) graph
MPA_social_ecological_vulnerability_graph <- function(.df,.MPA_name,.MPA_vulnerability,.coverage,.indicators_data,.donut_graph_params=list()){
  
  
  .to.plot <- .df
  
  .to.plot %<>% left_join(.dimensions_color_scale %>% mutate(group=as.character(group)),by=c(bar_title="group"))
  
  .coverage %<>% left_join(.dimensions_color_scale %>% mutate(group=as.character(group)),by=c(label="group"))
  
  .coverage %<>% mutate(label=case_when(label %in% names(.dimensions_abrv)~ .dimensions_abrv[label],TRUE~label ),
                        color=case_when(label=="Total" ~"gray40",TRUE~color))
  
  
  MPA_donut_graph_and_quality_panel(.df=.to.plot,
                                    .MPA_name = .MPA_name,
                                    .MPA_vulnerability=.MPA_vulnerability,
                                    .MPA_vul_color = "gray20",
                                    .coverage=.coverage,
                                    .indicators_data=.indicators_data,
                                    .donut_graph_params=.donut_graph_params)
  
  
  
}



#' Plots a MPA ecological vulnerability graph
#' 
#' @param .df (data.frame) data.frame with the same structure as .df in donut_graph, except for color
#' @param .MPA_name (character) graph title
#' @param .MPA_vulnerability (numeric) MPA vulnerability score
#' @param .coverage (data.frame) same as for coverage_barplot
#' @param .indicators_data (quality.indicator.display.definition list) same as for quality_indicatos_group_display
#' @param .donut_graph_params (list) parameters passed to donut_graph
#' @return (ggplot) graph
MPA_ecological_vulnerability_graph <- function(.df,.MPA_name,.MPA_vulnerability,.coverage,.indicators_data,.donut_graph_params=list()){
  
  
  
  
  .to.plot <- .df
  
  .to.plot %<>% left_join(.ecological_vul_color_scale %>% mutate(group=as.character(group)),by=c(bar_title="group"))
  
  .coverage %<>% left_join(.ecological_vul_color_scale %>% mutate(group=as.character(group)),by=c(label="group"))
  
  .coverage %<>% mutate(label=case_when(label %in% names(.dimensions_abrv)~ .dimensions_abrv[label],TRUE~label ),
                        color=case_when(label=="EV" ~"gray40",TRUE~color))
  
  
  MPA_donut_graph_and_quality_panel(.df=.to.plot,
                                    .MPA_name = .MPA_name,
                                    .MPA_vulnerability=.MPA_vulnerability,
                                    .MPA_vul_color = "gray20",
                                    .coverage=.coverage,
                                    .indicators_data=.indicators_data,
                                    .donut_graph_params=.donut_graph_params)
  
}


##### SHINY MODULES #####


data_file_input_UI <- function(id){
  
  ns <- NS(id)
  fileInput(ns("input_file"),"Please, Upload Your Data",multiple=FALSE,accept = "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
}

data_file_input_server <- function(id,.model_definition_file,.column_definitions,.model_structure=NULL,.drop_correlated_indicators=FALSE){
  moduleServer(
    id,
    function(input,output,session){
      
      scenarios <- reactiveVal()
      
      observeEvent(input$input_file,{
        
        .data_file <- input$input_file$datapath[1]
        progress <- AsyncProgress$new()
        progress$set(message = "Computing scenarios", value = 0)
        
        future({
          # Create a Progress object
          
          
          # Close the progress when this reactive exits (even if there's an error)
          on.exit(progress$close())
          
          # Create a callback function to update progress.
          
          updateProgress <- function(value = NULL, detail = NULL) {
            if (is.null(value)) {
              value <- progress$getValue()
              value <- value + (progress$getMax() - value) / 5
            }
            progress$set(value = value, detail = detail)
          }
          
          
          # browser()
          withCallingHandlers(.result <- process_data_file(.data_file=.data_file,
                                                           .model_definition_file=.model_definition_file,
                                                           .column_definitions=.column_definitions,
                                                           .normalize=TRUE,
                                                           .model_structure = .model_structure,
                                                           .drop_correlated_indicators = .drop_correlated_indicators,
                                                           updateProgress = updateProgress ),
                              data_file_column_missing=function(cond){
                                # browser()
                                text <- paste0("Columns ",paste0(cond$columns,collapse=", ")," missing in scenario ",cond$scenario,". Skipping")
                                showModal(modalDialog(
                                  title=paste0("Error in scenario ",cond$scenario),
                                  text
                                ))
                              },
                              normalization_limits_missing_warning=function(cond){
                                # browser()
                                text <- cond$message
                                showModal(modalDialog(
                                  title=paste0("Error in scenario ",cond$scenario),
                                  text
                                ))
                              },
                              unknown_error_processing_scenario=function(cond){
                                # browser()
                                text <- cond$message
                                showModal(modalDialog(
                                  title=paste0("Error in scenario ",cond$scenario),
                                  text
                                ))
                              })
          .result
        })%...>%(function(promis){
          if(!is.null(promis)){
            
            
            text <- promis %>% map2(names(.),~{
              
              if(length(.x$log)>0){
                
                c(list(tags$p(tags$b(.y))),
                  list(tags$ul(.x$log %>% unique %>% map(~tags$li(.x))))
                )
                
                
                
              }else{
                NULL
              }
              
            }) %>% compact()
            
            if(length(text) >0){
              
              showModal(modalDialog(wellPanel(text,title="We have found some issues in your data",style = "overflow-y:scroll; max-height: 600px")))
              
            }
            
          }
          
          
          
          
          scenarios(promis) 
          
          
          
        })
        
        
        
      })
      scenarios
    }
  )
}

scenarios_download_report_button_UI <- function(id){
  
  ns <- NS(id)
  
  downloadButton(ns("download_report"),label = "Download report")
  
  
}

scenarios_download_report_button_server <- function(id,.template_path,
                                                    .scenarios_data,
                                                    .model_structure,
                                                    .species_groups,
                                                    .report_title,
                                                    .indicator_labels,
                                                    .img_width=8,
                                                    .img_height=5.1,
                                                    .res=300,
                                                    .icons=NULL){
  moduleServer(
    id,
    function(input,output,session){
      output$download_report <- downloadHandler(
        filename=function(){
          paste0("MPA Vulnerability Report.zip")
        },
        content = function(file){
          
          
          # Create a Progress object
          progress <- AsyncProgress$new()
          progress$set(message = "Creating report", value = 0)
          .scen_data <- .scenarios_data()
          future({
            # Close the progress when this reactive exits (even if there's an error)
            on.exit(progress$close())
            
            # Create a callback function to update progress.
            # Each time this is called:
            # - If `value` is NULL, it will move the progress bar 1/5 of the remaining
            #   distance. If non-NULL, it will set the progress to that value.
            # - It also accepts optional detail text.
            updateProgress <- function(value = NULL, detail = NULL) {
              if (is.null(value)) {
                value <- progress$getValue()
                value <- value + (progress$getMax() - value) / 5
              }
              progress$set(value = value, detail = detail)
            }
            
            zip_report(.zip_file_path=file,
                       .data=.scen_data,
                       .model_structure=.model_structure,
                       .species_groups=.species_groups,
                       .indicator_labels=.indicator_labels,
                       .icons=.icons,
                       .template_path = .template_path,
                       .img_width=.img_width,
                       .img_height=.img_height,
                       .res=.res,
                       updateProgress = updateProgress)
          })
        }
      )
      
    })
}

scenarios_dropdown_menu_UI <- function(id,.scenarios){
  
  ns <- NS(id)
  
  selectInput(ns("scenario_selected"),"Scenarios",.scenarios,selectize = FALSE)
  
  
  
  
}

scenarios_dropdown_menu_server <- function(id,.scenarios_data){
  moduleServer(
    id,
    function(input,output,session){
      
      scenario_names <- reactive({
        
        .scenarios_data() %>% names %>% sort
        
      })
      
      observeEvent(scenario_names(),{
        
        updateSelectInput(session,"scenario_selected",choices =scenario_names(),selected = scenario_names()[1] )
        
      },ignoreNULL = TRUE)
      
      scenario_selected <- reactiveVal({
        
        
        if(!is.null(isolate(.scenarios_data()))){
          isolate(.scenarios_data()[[1]])
        }else{
          NULL
        }
        
      })
      
      
      
      observeEvent(input$scenario_selected,{
        if(!is.null(.scenarios_data())){
          
          if(!is.null(.scenarios_data())){
            scenario_selected(.scenarios_data()[[input$scenario_selected]])
          }
          
        }
      },ignoreNULL = TRUE)
      
      scenario_selected
    }
  )
  
}

model_output_tabs_UI <- function(.id,.class){
  ns = NS(.id)
  
  tags$div(
    tabsetPanel(
      
      tabPanel("General",
               column(12,
                      div(withSpinner(imageOutput(ns("social_ecological_vul"),width = "100%")),style="height:600px"),
                      htmlOutput(ns("social_ecological_explanation")),
                      div(withSpinner(imageOutput(ns("ecological_vul"),width="100%")),style="height:600px"),
                      htmlOutput(ns("ecological_explanation")),
                      div(withSpinner(imageOutput(ns("indicators_rank"),width="100%")),style="height:600px"),
                      htmlOutput(ns("indicators_rank_explanation"))
               )
      ),
      tabPanel("Habitats",
               
               div(withSpinner(imageOutput(ns("habitats_vul"),width="100%")),style="height:600px"),
               htmlOutput(ns("habitats_explanation")),
      ),
      tabPanel("Species",
               
               div(withSpinner(imageOutput(ns("species_vul"),width="100%")),style="height:600px"),
               htmlOutput(ns("species_explanation")),
               div(withSpinner(imageOutput(ns("species_groups_vul"),width="100%")),style="height:600px"),
               htmlOutput(ns("species_groups_explanation"))),
      tabPanel("User Groups",
               
               div(withSpinner(imageOutput(ns("user_groups_vul"),width="100%")),style="height:600px"),
               htmlOutput(ns("user_groups_explanation")))
    ),
    class = .class
  )
  
  
  
}

app_output_ecological_vuln_donut_graph <- function(.data,.filepath,.width,.height,.res,.donut_graph_params){
  
  .labels_equiv <- c("ECOLOGICAL VULNERABILITY"="Ecological vulnerability","ECOLOGICAL SENSITIVITY"="Ecological sensitivity","EXPOSURE"="Exposure","ECOLOGICAL ADAPTIVE CAPACITY"="Adaptive capacity")
  
  # Index data for dimensions
  
  .index_data <- .data$MPA_index %>% 
    select(`ECOLOGICAL SENSITIVITY`,`EXPOSURE`,`ECOLOGICAL ADAPTIVE CAPACITY`) %>%
    pivot_longer(cols=everything(),names_to="bar_title",values_to="score") %>%
    mutate(bar_title=.labels_equiv[bar_title],score=round(score,1))
  
  # MPA name
  
  .MPA_name <- .data$MPA_index %>% pull("MPA")
  
  # MPA vulnerability score
  
  .MPA_vulnerability_score <- .data$MPA_index %>% pull("ECOLOGICAL VULNERABILITY") %>% round(1)
  
  # Coverage data
  
  .coverage_df <- .data$MPA_coverage %>% 
    select(`ECOLOGICAL VULNERABILITY`,`ECOLOGICAL SENSITIVITY`,`EXPOSURE`,`ECOLOGICAL ADAPTIVE CAPACITY`) %>%
    pivot_longer(cols=everything(),names_to="label",values_to="coverage") %>%
    mutate(label=.labels_equiv[label])
  
  # Years
  .MPA_years <- .data$MPA_years %>% select(`ECOLOGICAL VULNERABILITY`,`ECOLOGICAL SENSITIVITY`,`EXPOSURE`,`ECOLOGICAL ADAPTIVE CAPACITY`) %>%
    pivot_longer(cols=everything(),names_to="label",values_to="years")
  
  .MPA_years %<>% mutate(label=.labels_equiv[label],years=sprintf("%d",as.integer(years))) %>% mutate(years=str_replace(years,"NA","-"))
  
  
  .MPA_quantitative <- .data$MPA_quantitative_percent %>% select(`ECOLOGICAL VULNERABILITY`,`ECOLOGICAL SENSITIVITY`,`EXPOSURE`,`ECOLOGICAL ADAPTIVE CAPACITY`) %>%
    pivot_longer(cols=everything(),names_to="label",values_to="quantitative")
  
  .MPA_quantitative %<>% mutate(label=.labels_equiv[label], quantitative=percent(quantitative,accuracy = 1))  %>% mutate(quantitative=str_replace(quantitative,"NA","-"))
  
  
  .years_indicators <- .MPA_years %$% map2(label,years,~{
    
    subtitble <- .dimensions_abrv[.x]
    if(.x=="Ecological vulnerability"){
      c_color <- "gray40"
    }else{
      c_color <- .ecological_vul_color_scale %>% filter(group==.x) %>% pull("color")  
    }
    
    
    ind_val <- .y
    
    quality.indicator.display.definition.constructor(.title_value = "",.title_properties = list(size=20,color="black"),
                                                     .subtitle_value = subtitble,.subtitle_properties = list(size=4,color="black"),
                                                     .indicator_value = ind_val,.indicator_properties = list(size=10,color="black"),
                                                     .circle_color = c_color)
  })
  
  .years_indicators[[1]]$title$value <- "Years measured"
  .years_indicators[[1]]$title$text_properties$size <- 20
  
  .quant_indicators <- .MPA_quantitative %$% map2(label,quantitative,~{
    
    subtitble <- .dimensions_abrv[.x]
    if(.x=="Ecological vulnerability"){
      c_color <- "gray40"
    }else{
      c_color <- .ecological_vul_color_scale %>% filter(group==.x) %>% pull("color")  
    }
    
    ind_val <- .y
    
    
    
    quality.indicator.display.definition.constructor(.title_value = "",.title_properties = list(size=20,color="black"),
                                                     .subtitle_value = subtitble,.subtitle_properties = list(size=4,color="black"),
                                                     .indicator_value = ind_val,.indicator_properties = list(size=6,color="black"),
                                                     .circle_color = c_color)
  })
  
  .quant_indicators[[1]]$title$value <- "Quantitative indicators"
  .quant_indicators[[1]]$title$text_properties$size <- 20
  
  .indicators <- c(.years_indicators,.quant_indicators)
  
  
  
  
  png(.filepath,width = .width, height = .height,units = "in",res = .res)
  g <- MPA_ecological_vulnerability_graph(.df=.index_data,
                                          .MPA_name=.MPA_name,
                                          .MPA_vulnerability=.MPA_vulnerability_score,
                                          .coverage = .coverage_df,
                                          .indicators_data = .indicators,
                                          .donut_graph_params=.donut_graph_params)
  
  grid.arrange(g)
  dev.off()
  
  
  
}

app_output_social_ecological_donut_graph <- function(.data,.filepath,.width,.height,.res,.donut_graph_params){
  
  .labels_equiv <- c("Vulnerability"="Total","SOCIAL SENSITIVITY"="Social sensitivity","ECOLOGICAL VULNERABILITY"="Ecological vulnerability","SOCIAL ADAPTIVE CAPACITY"="Adaptive capacity")
  
  # Index data for dimensions
  
  .index_data <- .data$MPA_index %>% 
    select(`SOCIAL SENSITIVITY`,`ECOLOGICAL VULNERABILITY`,`SOCIAL ADAPTIVE CAPACITY`) %>%
    pivot_longer(cols=everything(),names_to="bar_title",values_to="score") %>%
    mutate(bar_title=.labels_equiv[bar_title],score=round(score,1))
  
  # MPA name
  
  .MPA_name <- .data$MPA_index %>% pull("MPA")
  
  # MPA vulnerability score
  
  .MPA_vulnerability_score <- .data$MPA_index %>% pull("Vulnerability") %>% round(1)
  
  # Coverage data
  
  .coverage_df <- .data$MPA_coverage %>% 
    select(Vulnerability,`SOCIAL SENSITIVITY`,`ECOLOGICAL VULNERABILITY`,`SOCIAL ADAPTIVE CAPACITY`) %>%
    pivot_longer(cols=everything(),names_to="label",values_to="coverage") %>%
    mutate(label=.labels_equiv[label])
  
  # Years
  .MPA_years <- .data$MPA_years %>% select(Vulnerability,`SOCIAL SENSITIVITY`,`ECOLOGICAL VULNERABILITY`,`SOCIAL ADAPTIVE CAPACITY`) %>%
    pivot_longer(cols=everything(),names_to="label",values_to="years")
  
  .MPA_years %<>% mutate(label=.labels_equiv[label],years=sprintf("%d",as.integer(years))) %>% mutate(years=str_replace(years,"NA","-"))
  
  
  .MPA_quantitative <- .data$MPA_quantitative_percent %>% select(Vulnerability,`SOCIAL SENSITIVITY`,`ECOLOGICAL VULNERABILITY`,`SOCIAL ADAPTIVE CAPACITY`) %>%
    pivot_longer(cols=everything(),names_to="label",values_to="quantitative")
  
  .MPA_quantitative %<>% mutate(label=.labels_equiv[label], quantitative=percent(quantitative,accuracy = 1))  %>% mutate(quantitative=str_replace(quantitative,"NA","-"))
  
  
  .years_indicators <- .MPA_years %$% map2(label,years,~{
    
    subtitble <- .dimensions_abrv[.x]
    if(.x=="Total"){
      c_color <- "gray40"
    }else{
      c_color <- .dimensions_color_scale %>% filter(group==.x) %>% pull("color")  
    }
    
    
    ind_val <- .y
    
    quality.indicator.display.definition.constructor(.title_value = "",.title_properties = list(size=20,color="black"),
                                                     .subtitle_value = subtitble,.subtitle_properties = list(size=4,color="black"),
                                                     .indicator_value = ind_val,.indicator_properties = list(size=10,color="black"),
                                                     .circle_color = c_color)
  })
  
  .years_indicators[[1]]$title$value <- "Years measured"
  .years_indicators[[1]]$title$text_properties$size <- 20
  
  .quant_indicators <- .MPA_quantitative %$% map2(label,quantitative,~{
    
    subtitble <- .dimensions_abrv[.x]
    if(.x=="Total"){
      c_color <- "gray40"
    }else{
      c_color <- .dimensions_color_scale %>% filter(group==.x) %>% pull("color")  
    }
    
    ind_val <- .y
    
    
    
    quality.indicator.display.definition.constructor(.title_value = "",.title_properties = list(size=20,color="black"),
                                                     .subtitle_value = subtitble,.subtitle_properties = list(size=4,color="black"),
                                                     .indicator_value = ind_val,.indicator_properties = list(size=6,color="black"),
                                                     .circle_color = c_color)
  })
  
  .quant_indicators[[1]]$title$value <- "Quantitative indicators"
  .quant_indicators[[1]]$title$text_properties$size <- 20
  
  .indicators <- c(.years_indicators,.quant_indicators)
  
  
  
  
  png(.filepath,width = .width, height = .height,units = "in",res = .res)
  g <- MPA_social_ecological_vulnerability_graph(.df=.index_data,
                                                 .MPA_name=.MPA_name,
                                                 .MPA_vulnerability=.MPA_vulnerability_score,
                                                 .coverage = .coverage_df,
                                                 .indicators_data = .indicators,
                                                 .donut_graph_params=.donut_graph_params)
  
  grid.arrange(g)
  dev.off()
  
  
  
}


app_output_key_indicators_graph <- function(.data,.filepath,.model_structure,.indicator_labels,.width,.height,.res){
  
  .dimensions_to_labels <- c("ECOLOGICAL VULNERABILITY"="Ecological vulnerability",
                             "SOCIAL ADAPTIVE CAPACITY"="Adaptive capacity",
                             "SOCIAL SENSITIVITY"="Social sensitivity")
  
  .indicators <- .model_structure %>%  set_names(NULL) %>% flatten %>%
    map(~.x   %>% set_names(NULL) %>% flatten %>%  set_names(NULL)%>% flatten  %>% set_names(NULL) %>% flatten %>%names %>% as.list) %>% 
    map2(names(.),~{
      
      rep(.y,length(.x)) %>% set_names(.x)
    }) %>% reduce(c)
  
  .indicators <- .dimensions_to_labels[.indicators] %>% set_names(names(.indicators))
  
  .to.plot <- .data$MPA_indicators_contributions$contributions[[1]]$Vulnerability %>% 
    pivot_longer(cols=everything(),names_to = "Indicator",values_to="score") %>%
    mutate(group=.indicators[Indicator]) 
  
  .to.plot %<>% left_join(.indicator_labels,by=c("Indicator"="name")) %>% select(label,score,group) %>% mutate(score=abs(score))
  
  .to.plot %<>%  arrange(desc(score)) %>% slice(1:10)  %>% mutate(label=factor(label,levels=label))
  
  png(.filepath,width = .width, height = .height,units = "in",res = .res)
  
  g <- key_indicators_graph(.to.plot,.xlimits=c(0,max(.to.plot$score,na.rm=TRUE)),.xbreaks=waiver())
  g <- g + theme(legend.position = c(0.2,-0.3),
                 legend.direction = "horizontal",
                 legend.text = element_text(size=14),
                 legend.key.size = unit(1, "cm"),
                 legend.box.spacing  = unit(1,"cm"),
                 axis.text.y = element_text(size=14,hjust=1),
                 axis.text.x = element_text(size=20),
                 axis.title.x  = element_text(size=22),
                 plot.margin = margin(t=0.5,r=0.5,b=1.5,l=0.5,"cm")
  )
  grid.arrange(g)
  dev.off()
  
  
  
}

app_output_habitat_vuln_graph <- function(.data,.filepath,.width,.height,.res,.icons=NULL){
  
  .to.plot <- .data$MPA_HB_index %>% filter(!is.na(HB)) %>% select(label=HB,score=Vulnerability)%>% 
    mutate(group=.vulnerability_group(score)) %>% arrange(desc(score))  
  
  if(!is.null(.icons)){
    
    .to.plot %<>% left_join(.icons,by=c(label="name"))
    
  }
  
  .to.plot%<>% mutate(label=factor(label,levels=label))
  
  png(.filepath,width = .width, height = .height,units = "in",res = .res)
  
  g <- habitats_vulnerability_graph(.to.plot,.icon_asp=2.2,.icon_size=0.1,.icon_offset=0.15)
  g <- g + theme(legend.position = c(0.45,1.2),
                 legend.direction = "horizontal",
                 legend.text = element_text(size=14),
                 legend.key.size = unit(0.5, "cm"),
                 legend.box.spacing  = unit(0.5,"cm"),
                 axis.text.x = element_text(size=16,angle = 30,hjust = 1,vjust=1),
                 axis.title.y  = element_text(size=20),
                 plot.margin = margin(t=2,r=1.5,b=0.5,l=0.5,"cm")
  )
  grid.arrange(g)
  dev.off()
  
  
}

app_output_species_vuln_graph <- function(.data,.filepath,.width,.height,.res,.icons=NULL){
  
  .to.plot <- .data$MPA_SP_index %>% 
    filter(!is.na(SP)) %>% 
    select(label=SP,score=Vulnerability)%>% 
    mutate(group=.vulnerability_group(score)) %>% 
    filter(label!="Other") %>%
    arrange(desc(score)) %>% 
    ungroup %>% 
    slice(1:10) 
  
  if(!is.null(.icons)){
    
    .to.plot %<>% left_join(.icons,by=c(label="name"))
    
  }
  
  .to.plot %<>% 
    mutate(label=factor(label,levels=label))
  
  png(.filepath,width = .width, height = .height,units = "in",res = .res)
  
  g <- species_vulnerability_graph(.to.plot,.icon_asp=1.2,.icon_size=0.08,.icon_offset=0.11)
  g <- g + theme(legend.position = c(0.35,1.2),
                 legend.direction = "horizontal",
                 legend.text = element_text(size=14),
                 legend.key.size = unit(0.5, "cm"),
                 legend.box.spacing  = unit(0.5,"cm"),
                 axis.text.y = element_text(size=14,hjust=1),
                 axis.title.x  = element_text(size=20),
                 plot.margin = margin(t=2,r=3.5,b=0.5,l=0.5,"cm")
  )
  grid.arrange(g)
  dev.off()
  
  
}


app_output_user_groups_vuln_graph <- function(.data,.filepath,.width,.height,.res,.icons=NULL){
  
  
  .to.plot <- .data$MPA_UG_index %>% filter(!is.na(UG)) %>% select(label=UG,score=Vulnerability)%>% 
    mutate(group=.vulnerability_group(score)) %>% arrange(desc(score)) 
  
  if(!is.null(.icons)){
    
    .to.plot %<>% left_join(.icons,by=c(label="name"))
    
  }
  
  .to.plot  %<>% mutate(label=factor(label,levels=label))
  
  png(.filepath,width = .width, height = .height,units = "in",res = .res)
  
  g <- user_groups_vulnerability_graph(.to.plot,.icon_asp=2.2,.icon_size=0.08,.icon_offset=0.15)
  g <- g + theme(legend.position = c(0.45,1.2),
                 legend.direction = "horizontal",
                 legend.text = element_text(size=14),
                 legend.key.size = unit(0.5, "cm"),
                 legend.box.spacing  = unit(0.5,"cm"),
                 axis.text.x = element_text(size=16,angle = 30,hjust = 1,vjust=1),
                 axis.title.y  = element_text(size=20),
                 plot.margin = margin(t=2,r=1.5,b=0.5,l=0.5,"cm")
  )
  grid.arrange(g)
  dev.off()
  
}


app_output_species_groups_vuln_graph <- function(.data,.filepath,.species_groups,.width,.height,.res,.icons=NULL){
  
  
  
  .to.plot <-  .data$MPA_SP_index %>% filter(!is.na(SP)) %>% select(label=SP,score=Vulnerability)%>% 
    mutate(group=.vulnerability_group(score)) %>%
    left_join(.species_groups,by=c(label="species"))  %>%
    filter(!is.na(species_group))
  
  
  if(!is.null(.icons)){
    
    .to.plot %<>% left_join(.icons,by=c(species_group="name"))
    
  }
  
  
  .species_groups_levels <- .to.plot %>% group_by(species_group) %>% 
    summarise(score_total=sum(score),.groups="drop") %>% 
    filter(!is.na(species_group)) %>% 
    arrange(desc(score_total)) %>% 
    pull("species_group")
  
  
  .to.plot %<>%  mutate(species_group=factor(species_group,levels=.species_groups_levels)) 
  
  
  png(.filepath,width = .width, height = .height,units = "in",res = .res)
  
  g <- species_groups_vulnerability_graph(.to.plot,.icon_asp=2.2,.icon_size=0.08,.icon_offset=0.1)
  g <- g + theme(legend.position = c(0.45,1.2),
                 legend.direction = "horizontal",
                 legend.text = element_text(size=14),
                 legend.key.size = unit(0.5, "cm"),
                 legend.box.spacing  = unit(0.5,"cm"),
                 axis.text.x = element_text(size=12,angle = 30,hjust = 1,vjust=1),
                 axis.title.y  = element_text(size=20),
                 plot.margin = margin(t=2,r=2,b=0.5,l=1.5,"cm")
  )
  grid.arrange(g)
  dev.off()
  
}

#' @param .data (reactiveValues) should have the following data:
#' - MPA_index (data.frame) 1 row df containing the normalized index and with columns SOCIAL SENSITIVITY, 
#' ECOLOGICAL VULNERABILITY, SOCIAL ADAPTIVECAPACITY and index
#' - MPA_coverage (data.frame) 1 row df containing de coverage in the range 0-1 and with columns SOCIAL SENSITIVITY, 
#' ECOLOGICAL VULNERABILITY, SOCIAL ADAPTIVECAPACITY and index
#' - MPA_years (data.frame) 1 row df containing de average number of years of measurements and with columns SOCIAL SENSITIVITY, 
#' ECOLOGICAL VULNERABILITY, SOCIAL ADAPTIVECAPACITY and index
model_output_tabs_server <- function(id,.data,.model_structure,.indicator_labels,.species_groups,.img_width=8,.img_height=5.1,.res=300,.icons=NULL){
  moduleServer(
    id,
    
    function(input,output,session){
      
      
      donut_graph_params <- list(.donut_graph_central_score_size=30,
                                 .donut_graph_central_score_subtitle_size=12,
                                 .donut_graph_bar_labels_size=8,
                                 .donut_graph_lengend_text_size=16,
                                 .donut_graph_legend_spacing=0.2)
      
      
      # Plots
      
      output$social_ecological_vul <- renderImage({
        req(.data())
        if(!is.null(.data())){
          outfile <- tempfile(fileext = ".png")
          app_output_social_ecological_donut_graph(.data(),
                                                   .filepath=outfile,
                                                   .width=.img_width,
                                                   .height=.img_height,
                                                   .res=.res,
                                                   .donut_graph_params=donut_graph_params)
          
          list(src = outfile,
               height=500,
               contentType = 'image/png',
               alt = "Social Ecological Vulnerability")
        }else{
          list()
        }
      },deleteFile = TRUE)
      
      output$ecological_vul <- renderImage({
        req(.data())
        if(!is.null(.data())){
          outfile <- tempfile(fileext = ".png")
          app_output_ecological_vuln_donut_graph(.data(),
                                                 .filepath=outfile,
                                                 .width=.img_width,
                                                 .height=.img_height,
                                                 .res=.res,
                                                 .donut_graph_params=donut_graph_params)
          
          list(src = outfile,
               contentType = 'image/png',
               height=500,
               alt = "Ecological Vulnerability")
        }
      },deleteFile=TRUE)
      
      
      output$indicators_rank <- renderImage({
        req(.data())
        if(!is.null(.data())){
          outfile <- tempfile(fileext = ".png")
          app_output_key_indicators_graph(.data=.data(),
                                          .filepath=outfile,
                                          .model_structure=.model_structure,
                                          .indicator_labels = .indicator_labels,
                                          .width=.img_width,
                                          .height=.img_height,
                                          .res=.res)
          
          list(src = outfile,
               contentType = 'image/png',
               height=500,
               alt = "Ten top indicators contributing to the vulnerability index")
        }
        
        
      },deleteFile=TRUE)
      
      output$habitats_vul <- renderImage({
        req(.data())
        if(!is.null(.data())){
          
          outfile <- tempfile(fileext = ".png")
          
          app_output_habitat_vuln_graph(.data=.data(),.filepath=outfile,.width=.img_width,.height=.img_height,.res=.res,.icons=.icons)
          
          list(src = outfile,
               contentType = 'image/png',
               height=500,
               alt = "Habitats vulnerability index")
        }
        
      },deleteFile=TRUE)
      
      output$species_vul <-  renderImage({
        req(.data())
        if(!is.null(.data())){
          
          outfile <- tempfile(fileext = ".png")
          
          app_output_species_vuln_graph(.data=.data(),.filepath=outfile,.width=.img_width*1.05,.height=.img_height*1.05,.res=.res,.icons=.icons)
          
          
          list(src = outfile,
               contentType = 'image/png',
               height=500,
               alt = "Species vulnerability index")
        }
        
      },deleteFile=TRUE)
      
      output$species_groups_vul <- renderImage({
        req(.data())
        if(!is.null(.data())){
          
          outfile <- tempfile(fileext = ".png")
          
          app_output_species_groups_vuln_graph(.data=.data(),.filepath=outfile,.species_groups = .species_groups,.width=.img_width*1.2,.height=.img_height,.res=.res,.icons=.icons)
          
          
          
          list(src = outfile,
               contentType = 'image/png',
               height=500,
               alt = "Habitats vulnerability index")
        }
        
      },deleteFile=TRUE)
      
      
      
      output$user_groups_vul <- renderImage({
        req(.data())
        if(!is.null(.data())){
          
          
          outfile <- tempfile(fileext = ".png")
          
          app_output_user_groups_vuln_graph(.data=.data(),.filepath=outfile,.width=.img_width,.height=.img_height,.res=.res,.icons=.icons)
          
          
          
          list(src = outfile,
               contentType = 'image/png',
               height=500,
               alt = "User groups vulnerability index")
          
        }
      },deleteFile=TRUE)
      
      
      # Texts
      
      
      
      try(output$social_ecological_explanation <- renderUI(HTML(paste0(read_lines("web_texts/social_ecological_donut_graph.html"),collapse="\n"))))
      
      
      
      try(output$ecological_explanation <- renderUI(HTML(paste0(read_lines("web_texts/ecological_donut_graph.html"),collapse="\n"))))
      
      
      try(output$indicators_rank_explanation <- renderUI(HTML(paste0(read_lines("web_texts/key_indicators_graph.html"),collapse="\n"))))
      
      try(output$habitats_explanation <-renderUI(HTML(paste0(read_lines("web_texts/habitat_vuln_graph.html"),collapse="\n"))))
      try(output$species_explanation <- renderUI(HTML(paste0(read_lines("web_texts/species_vuln_graph.html"),collapse="\n"))))
      try(output$species_groups_explanation <- renderUI(HTML(paste0(read_lines("web_texts/species_groups_vuln_graph.html"),collapse="\n"))))
      try(output$user_groups_explanation <- renderUI(HTML(paste0(read_lines("web_texts/user_groups_vuln_graph.html"),collapse="\n"))))
      
      
    }
    
    
  )}

##### REPORT ######

donut_graph_scenarios_and_quality_panel <- function(.data,.filepath,.width,.height,.res,.donut_graph_params,.labels_equiv,.columns_selection_donut,.colums_selection_indicators,.color_scale,.vulnerability_measure_column,.total_label){
  
  # Index data for dimensions
  
  grid_titles <- names(.data) %>% map(~str_match(.x,"(\\d\\.\\d).*(\\d{4})") %>% as.data.frame) %>% bind_rows() %>% select(2:3) %>% set_names(c("RCP","Year"))
  
  .dat_order <- grid_titles %>% mutate(i=1:nrow(.))  %>% arrange(Year,RCP) %>% pull("i")
  
  .data <- .data[.dat_order]
  
  
  .donuts <- .data %>% compact %>% map(~{
    
    .index_data <- .x$MPA_index %>% 
      select(one_of(.columns_selection_donut)) %>%
      pivot_longer(cols=everything(),names_to="bar_title",values_to="score") %>%
      mutate(bar_title=.labels_equiv[bar_title],score=round(score,1))
    
    # MPA vulnerability score
    
    .MPA_vulnerability_score <- .x$MPA_index %>% pull(.vulnerability_measure_column) %>% round(1)
    
    
    .index_data %<>% left_join(.color_scale %>% mutate(group=as.character(group)),by=c(bar_title="group"))
    
    .donut_graph <- do.call(MPA_donut_graph,c(list(.index_data,"",.MPA_vulnerability_score,"gray20"),.donut_graph_params)) 
    
  }) %>% map(~ .x + theme(legend.position = "none",
                          legend.margin = margin(0,0,0,0),
                          legend.box.margin = margin(0), 
                          plot.margin = unit(c(-2,-2,-2,-0),"lines")))
  
  
  if(length(.donuts)>0){
    .g <- ggplotGrob(.donuts[[1]] + theme(legend.position = "bottom"))$grobs
    
    # get the legend
    legend <- .g[[which(sapply(.g, function(x) x$name) == "guide-box")]]
  }else{
    legend  <- nullGrob()
  }
  
  .donuts <- names(.data) %>% map(~{
    .out <- .donuts[[.x]]
    if(is.null(.out)){
      .out <- nullGrob()
    }
    
    .out
  }) %>% set_names(names(.data))
  
  .donuts_grobs <- c(.donuts,list(legend))
  
  
  
  # Coverage data
  
  .coverage_df <- .data %>% compact %>% map(~.x$MPA_coverage %>% 
                                              select(one_of(.colums_selection_indicators)) %>%
                                              pivot_longer(cols=everything(),names_to="label",values_to="coverage") %>%
                                              mutate(label=.labels_equiv[label])) 
  
  if(length(.coverage_df)>0){
    .coverage_df <- .coverage_df[-1]%>%
      reduce(~{
        full_join(.x,.y%>% rename(coverage2=coverage),by="label") %>% 
          mutate(coverage=coverage+coverage2,n=n+as.numeric(!is.na(coverage2))) %>% 
          select(-coverage2)
      },.init=.coverage_df[[1]] %>% mutate(n=as.numeric(!is.na(coverage)))) %>% 
      mutate(coverage=coverage/n) %>% select(-n)
    
    
    .coverage_df %<>% left_join(.color_scale %>% mutate(group=as.character(group)),by=c(label="group"))
    
    .coverage_df %<>% mutate( color=case_when(label==.total_label ~"gray40",TRUE~color),
                              label=case_when(label %in% names(.dimensions_abrv)~ .dimensions_abrv[label],TRUE~label )
    )
  }else{
    .coverage_df <- NULL
  }
  # Years
  
  .MPA_years <- .data %>% compact() %>% map(~{
    
    .years <- .x$MPA_years %>% select(one_of(.colums_selection_indicators)) %>%
      pivot_longer(cols=everything(),names_to="label",values_to="years")
    .years
    
  })
  
  if(length(.MPA_years)>0){
    .MPA_years <- .MPA_years[-1]%>%
      reduce(~{
        full_join(.x,.y%>% rename(years2=years),by="label") %>% 
          mutate(years=years+years2,n=n+as.numeric(!is.na(years2))) %>% 
          select(-years2)
      },.init=.MPA_years[[1]] %>% mutate(n=as.numeric(!is.na(years)))) %>% 
      mutate(years=years/n) %>% select(-n)
    
    
    
    .MPA_years %<>% mutate(label=.labels_equiv[label],years=sprintf("%d",as.integer(years))) %>% mutate(years=str_replace(years,"NA","-"))
  }else{
    .MPA_years <- NULL
  }
  
  
  .MPA_quantitative <- .data %>% compact %>% map(~{
    
    .x$MPA_quantitative_percent %>% select(one_of(.colums_selection_indicators)) %>%
      pivot_longer(cols=everything(),names_to="label",values_to="quantitative")
    
    
  })
  
  
  if(length(.MPA_quantitative)>0){
    .MPA_quantitative <- .MPA_quantitative[-1]%>%
      reduce(~{
        full_join(.x,.y%>% rename(quantitative2=quantitative),by="label") %>% 
          mutate(quantitative=quantitative+quantitative2,n=n+as.numeric(!is.na(quantitative2))) %>% 
          select(-quantitative2)
      },.init=.MPA_quantitative[[1]] %>% mutate(n=as.numeric(!is.na(quantitative)))) %>% 
      mutate(quantitative=quantitative/n) %>% select(-n)
    
    
    
    .MPA_quantitative %<>% mutate(label=.labels_equiv[label], quantitative=percent(quantitative,accuracy = 1))  %>% mutate(quantitative=str_replace(quantitative,"NA","-"))
    
  }else{
    .MPA_quantitative <- NULL
  }
  
  
  if(!is.null(.MPA_years) & !is.null(.coverage_df) & !is.null(.MPA_quantitative)){
    
    .years_indicators <- .MPA_years %$% map2(label,years,~{
      
      subtitble <- .dimensions_abrv[.x]
      if(.x==.total_label){
        c_color <- "gray40"
      }else{
        c_color <- .color_scale %>% filter(group==.x) %>% pull("color")  
      }
      
      
      ind_val <- .y
      
      quality.indicator.display.definition.constructor(.title_value = "",.title_properties = list(size=18,color="black"),
                                                       .subtitle_value = subtitble,.subtitle_properties = list(size=4,color="black"),
                                                       .indicator_value = ind_val,.indicator_properties = list(size=10,color="black"),
                                                       .circle_color = c_color)
    })
    
    .years_indicators[[1]]$title$value <- "Years measured"
    .years_indicators[[1]]$title$text_properties$size <- 18
    
    .quant_indicators <- .MPA_quantitative %$% map2(label,quantitative,~{
      
      subtitble <- .dimensions_abrv[.x]
      if(.x==.total_label){
        c_color <- "gray40"
      }else{
        c_color <- .color_scale %>% filter(group==.x) %>% pull("color")  
      }
      
      ind_val <- .y
      
      
      
      quality.indicator.display.definition.constructor(.title_value = "",.title_properties = list(size=18,color="black"),
                                                       .subtitle_value = subtitble,.subtitle_properties = list(size=4,color="black"),
                                                       .indicator_value = ind_val,.indicator_properties = list(size=5,color="black"),
                                                       .circle_color = c_color)
    })
    
    .quant_indicators[[1]]$title$value <- "Quantitative indicators"
    .quant_indicators[[1]]$title$text_properties$size <- 18
    
    .indicators <- c(.years_indicators,.quant_indicators)
    
    
    .qual_panel <- quality_panel(.coverage = .coverage_df,.indicators_data = .indicators,.coverage_x_axis_text_size=12)
    
  }else{
    .qual_panel <- nullGrob()
  }
  # grid_titles <- names(.data) %>% map(~str_match(.x,"(\\d\\.\\d).*(\\d{4})") %>% as.data.frame) %>% bind_rows() %>% select(2:3) %>% set_names(c("RCP","Year"))
  
  .RCP_text_grobs <- grid_titles$RCP %>% unique() %>% map(~textGrob(paste("RCP",.x),gp=gpar(fontface="bold"),vjust=2,hjust=0.1))
  
  .Year_text_grobs <- grid_titles$Year %>% unique() %>% map(~textGrob(.x,gp=gpar(fontface="bold"),vjust=2,hjust=0))
  
  .grobs <- c(.donuts_grobs,list(.qual_panel),.RCP_text_grobs,.Year_text_grobs,list(nullGrob()))
  
  .donuts_grid <- arrangeGrob(grobs = .grobs,layout_matrix = matrix(c(14,12,13,14,
                                                                      9,1,2,14,
                                                                      9,1,2,8,
                                                                      10,3,4,8,
                                                                      11,5,6,8,
                                                                      11,5,6,14,
                                                                      14,14,14,14,
                                                                      7,7,7,7),ncol = 4,byrow = TRUE),
                              widths = c(0.08,0.33,0.33,0.46),heights = c(0.03,0.09,0.19,0.28,0.19,0.09,0.03,0.10))
  
  png(.filepath,width = .width, height = .height,units = "in",res = .res)
  
  
  grid.arrange(.donuts_grid)
  dev.off()
  
  
}

app_output_social_ecological_donut_graph_scenarios <- function(.data,.filepath,.width,.height,.res,.donut_graph_params){
  
  .labels_equiv <- c("Vulnerability"="Total","SOCIAL SENSITIVITY"="Social sensitivity","ECOLOGICAL VULNERABILITY"="Ecological vulnerability","SOCIAL ADAPTIVE CAPACITY"="Adaptive capacity")
  
  .columns_selection_donut <- c("SOCIAL SENSITIVITY","ECOLOGICAL VULNERABILITY","SOCIAL ADAPTIVE CAPACITY")
  
  .columns_selection_indicators <- c("Vulnerability","SOCIAL SENSITIVITY","ECOLOGICAL VULNERABILITY","SOCIAL ADAPTIVE CAPACITY")
  
  donut_graph_scenarios_and_quality_panel(.data=.data,
                                          .filepath=.filepath,
                                          .width=.width,
                                          .height=.height,
                                          .re=.res,
                                          .donut_graph_params=.donut_graph_params,
                                          .labels_equiv=.labels_equiv,
                                          .columns_selection_donut=.columns_selection_donut,
                                          .colums_selection_indicators=.columns_selection_indicators,
                                          .color_scale=.dimensions_color_scale,
                                          .vulnerability_measure_column="Vulnerability",
                                          .total_label="Total")
  
}


app_output_social_ecological_donut_graph_scenarios_by_HZ <- function(.data,.filepath,.width,.height,.res,.donut_graph_params){
  
  .labels_equiv <- c("Vulnerability"="Total","SOCIAL SENSITIVITY"="Social sensitivity","ECOLOGICAL VULNERABILITY"="Ecological vulnerability","SOCIAL ADAPTIVE CAPACITY"="Adaptive capacity")
  
  .columns_selection_donut <- c("SOCIAL SENSITIVITY","ECOLOGICAL VULNERABILITY","SOCIAL ADAPTIVE CAPACITY")
  
  .columns_selection_indicators <- c("Vulnerability","SOCIAL SENSITIVITY","ECOLOGICAL VULNERABILITY","SOCIAL ADAPTIVE CAPACITY")
  
  .color_scale <- .dimensions_color_scale
  .vulnerability_measure_column <- "Vulnerability"
  .total_label <- "Total"
  
  
  
  grid_titles <- names(.data) %>% map(~str_match(.x,"(\\d\\.\\d).*(\\d{4})") %>% as.data.frame) %>% bind_rows() %>% select(2:3) %>% set_names(c("RCP","Year"))
  
  .dat_order <- grid_titles %>% mutate(i=1:nrow(.))%>% arrange(Year,RCP) %>% pull("i")
  
  .data <- .data[.dat_order]
  
  
  
  .donuts <- .data %>% compact %>% map(~{
    
    .index_data <- .x$MPA_HZ_index %>% as.data.frame%>% nest(data_HZ=-c(HZ))
    
    .index_data %<>% mutate(data_HZ=map(data_HZ,~.x %>% 
                                          select(one_of(.columns_selection_donut)) %>%
                                          pivot_longer(cols=everything(),names_to="bar_title",values_to="score") %>%
                                          mutate(bar_title=.labels_equiv[bar_title],score=round(score,1))
    ) %>% set_names(HZ)) %>% pull("data_HZ")
    
    # MPA vulnerability score
    
    .MPA_vulnerability_score <- .x$MPA_HZ_index %>% pull(.vulnerability_measure_column) %>% round(1) %>% set_names(.x$MPA_HZ_index$HZ)
    
    
    .index_data %<>% map(~.x %>% left_join(.color_scale %>% mutate(group=as.character(group)),by=c(bar_title="group")))
    
    .donut_graph <- map2(.index_data,.MPA_vulnerability_score,~do.call(MPA_donut_graph,c(list(.x,"",.y,"gray20"),.donut_graph_params)))
    
  }) %>% map(~{
    .x %>% map(~.x + theme(legend.position = "none",
                           legend.margin = margin(0,0,0,0),
                           legend.box.margin = margin(0), 
                           plot.margin = unit(c(-2,-1,-2,-0),"lines")))})
  
  
  if(length(.donuts)>1){
    .g <- ggplotGrob(.donuts[[1]][[1]] + theme(legend.position = "bottom"))$grobs
    # get the legend
    legend <- .g[[which(sapply(.g, function(x) x$name) == "guide-box")]]
  }else{
    legend <- nullGrob()
  }
  
  
  # grid_titles <- names(.data) %>% map(~str_match(.x,"(\\d\\.\\d).*(\\d{4})") %>% as.data.frame) %>% bind_rows() %>% select(2:3) %>% set_names(c("RCP","Year"))
  
  Years <- grid_titles %>% pull("Year") %>% unique
  
  .donuts_panels <- Years %>% map(~{
    
    
    
    .years_donuts <- .donuts[grid_titles$Year==.x]
    
    
    
    .years_donuts %<>% compact %>% map(~list(.x$SST,.x$MHW)) 
    
    if(length(.years_donuts)==3){
      .years_donuts %<>% reduce(c)
    }else{
      .years_donuts <- c(rep(list(nullGrob()),6)) 
    }  
    HZ_labels <- c("SST","MHW") %>%map(~textGrob(.x,gp=gpar(fontface="bold",cex=1.1),vjust=2,hjust=0.3))
    
    .Year_text_grobs <- .x %>% unique() %>% map(~textGrob(.x,gp=gpar(fontface="bold",cex=1.5),vjust=1,hjust=0.3))
    line <- pathGrob(c(0.25,0.75),c(0,0))
    .grobs <- c(.years_donuts,.Year_text_grobs,HZ_labels,list(line))
    
    .donuts_grid <- arrangeGrob(grobs = .grobs,layout_matrix = matrix(c(7,7,
                                                                        10,10,
                                                                        8,9,
                                                                        1,2,
                                                                        3,4,
                                                                        5,6
    ),ncol = 2,byrow = TRUE),
    widths = c(0.5,0.5),heights = c(0.025,0.05,0.03,0.29,0.29,0.29))
    .donuts_grid
    
    
  })
  
  
  .RCP_labels <- grid_titles %>%  pull("RCP") %>% unique()
  .RCP_text_grobs <- .RCP_labels %>% unique() %>% map(~textGrob(paste("RCP",.x),gp=gpar(fontface="bold"),vjust=1.5,hjust=0.3))
  .out <- arrangeGrob(grobs = c(.donuts_panels,list(legend),.RCP_text_grobs,list(nullGrob())),
                      layout_matrix = matrix(c(11,1,2,
                                               8,1,2,
                                               9,1,2,
                                               10,1,2,
                                               7,7,7),ncol=3,byrow = TRUE),
                      heights = c(0.0945,0.27,0.27,0.27,0.1),widths = c(0.1,0.46,0.46))
  
  
  png(.filepath,width = .width, height = .height,units = "in",res = .res)
  
  
  grid.arrange(.out)
  dev.off()
  
  
}
app_output_ecological_vulnerability_donut_graph_scenarios <- function(.data,.filepath,.width,.height,.res,.donut_graph_params){
  
  .labels_equiv <- c("ECOLOGICAL VULNERABILITY"="Ecological vulnerability","ECOLOGICAL SENSITIVITY"="Ecological sensitivity","EXPOSURE"="Exposure","ECOLOGICAL ADAPTIVE CAPACITY"="Adaptive capacity")
  
  .columns_selection_donut <- c("ECOLOGICAL SENSITIVITY","EXPOSURE","ECOLOGICAL ADAPTIVE CAPACITY")
  
  .columns_selection_indicators <- c("ECOLOGICAL VULNERABILITY","ECOLOGICAL SENSITIVITY","EXPOSURE","ECOLOGICAL ADAPTIVE CAPACITY")
  
  donut_graph_scenarios_and_quality_panel(.data=.data,
                                          .filepath=.filepath,
                                          .width=.width,
                                          .height=.height,
                                          .re=.res,
                                          .donut_graph_params=.donut_graph_params,
                                          .labels_equiv=.labels_equiv,
                                          .columns_selection_donut=.columns_selection_donut,
                                          .colums_selection_indicators=.columns_selection_indicators,
                                          .color_scale=.ecological_vul_color_scale,
                                          .vulnerability_measure_column="ECOLOGICAL VULNERABILITY",
                                          .total_label="Ecological vulnerability")
  
}

#' Converts a data.frame to a flextable formatted according to the MPA
#' engagement guidelines
#' 
#' @param .data (data.frame) table
#' @return (flextable)
mpa_engage_table <- function(.data){
  
  .out <- .data %>% flextable()
  
  .out %<>% bg(part="header",bg="#7EB639") %>% 
    bg(i=seq(1,nrow(.data),2),bg="#E4F3CE",part="body") %>% 
    hline(border=fp_border(color="#B2DA77"),part="all") %>%
    border(border.top=fp_border(color="#B2DA77"),part="header") %>%
    font(i=1:nrow(.data),fontname = "Cambria",part="body") %>%
    fontsize(i=1:nrow(.data),size = 11,part="body") %>%
    font(i=1,fontname = "Cambria",part="header") %>%
    fontsize(i=1,size = 11,part="header")%>%
    bold(part="header")
  
  .out
}


#' Returns a flextable with all the user groups vulnerability graph data for
#' one scenario
#' 
#' @param .scenario_data (list of data.frame) scenario data computed by compute scenario
#' @return (flextable)
report_user_groups_vul_table <- function(.scenario_data){
  
  .to.plot <- .scenario_data$MPA_UG_index %>% filter(!is.na(UG)) %>% select(Users=UG,Vulnerability)%>% 
    mutate(group=.vulnerability_group(Vulnerability)) %>% arrange(desc(Vulnerability))  %>% 
    mutate(Users=factor(Users,levels=Users),
           Vulnerability=sprintf("%0.3f",Vulnerability)) %>%
    unite(Vulnerability,Vulnerability,group,sep=" ")
  
  
  .out <- mpa_engage_table(.to.plot) %>% 
    align(j=2,align = "left",part="body") %>%
    align(j=2,align = "center",part="header") %>%
    align(j=1,align = "left")
  
  .out
}

#' Returns a flextable with all the species groups vulnerability graph data for
#' one scenario
#' 
#' @param .scenario_data (list of data.frame) scenario data computed by compute scenario
#' @param species_groups (data.frame)
#' @return (flextable)
report_species_groups_vul_table <- function(.scenario_data,.species_groups){
  
  .species_groups_levels <- .species_groups$species_group %>% unique
  
  .to.plot <- .scenario_data$MPA_SP_index %>% filter(!is.na(SP)) %>% select(Species=SP,Vulnerability)%>% 
    mutate(group=.vulnerability_group(Vulnerability)) %>% arrange(desc(Vulnerability))  %>% 
    mutate(Species=factor(Species,levels=Species))  %>% 
    left_join(.species_groups,by=c(Species="species")) %>%
    mutate(species_group=factor(species_group,levels=.species_groups_levels)) %>%
    filter(!is.na(species_group)) %>% select(species_group,Vulnerability) %>% 
    group_by(species_group) %>% summarise(Vulnerability=sum(Vulnerability,na.rm=TRUE),.groups="drop") %>%
    arrange(desc(Vulnerability)) %>% mutate(Vulnerability=sprintf("%0.3f",Vulnerability))
  
  .out <- mpa_engage_table(.to.plot) %>% 
    set_header_labels(species_group="Species Group",Vulnerability="Species accumulated\nvulnerability") %>%
    align(j=2,align = "center",part="body") %>%
    align(j=2,align = "center",part="header") %>%
    align(j=1,align = "left")
  
  .out
}


#' Returns a flextable with all the species vulnerability graph data for
#' one scenario
#' 
#' @param .scenario_data (list of data.frame) scenario data computed by compute scenario
#' @return (flextable)
report_species_vul_table <- function(.scenario_data){
  
  .to.plot <- .scenario_data$MPA_SP_index %>% filter(!is.na(SP)) %>% select(Species=SP,Vulnerability)%>% 
    mutate(group=.vulnerability_group(Vulnerability)) %>% arrange(desc(Vulnerability))  %>% 
    mutate(Species=factor(Species,levels=Species),
           Vulnerability=sprintf("%0.3f",Vulnerability)) %>%
    unite(Vulnerability,Vulnerability,group,sep=" ")
  
  
  .out <- mpa_engage_table(.to.plot) %>% 
    align(j=2,align = "left",part="body") %>%
    align(j=2,align = "center",part="header") %>%
    align(j=1,align = "left")
  
  .out
}


#' Returns a flextable with all the habitat vulnerability graph data for
#' one scenario
#' 
#' @param .scenario_data (list of data.frame) scenario data computed by compute scenario
#' @return (flextable)
report_habitat_vul_table <- function(.scenario_data){
  
  .to.plot <- .scenario_data$MPA_HB_index %>% filter(!is.na(HB)) %>% select(Habitat=HB,Vulnerability)%>% 
    mutate(group=.vulnerability_group(Vulnerability)) %>% arrange(desc(Vulnerability))  %>% 
    mutate(Habitat=factor(Habitat,levels=Habitat),
           Vulnerability=sprintf("%0.3f",Vulnerability)) %>%
    unite(Vulnerability,Vulnerability,group,sep=" ")
  
  
  .out <- mpa_engage_table(.to.plot) %>% 
    align(j=2,align = "left",part="body") %>%
    align(j=2,align = "center",part="header") %>%
    align(j=1,align = "left")
  
  .out
}


#' Returns a flextable with all the key indicators graph data for
#' one scenario
#' 
#' @param .scenario_data (list of data.frame) scenario data computed by compute scenario
#' @param .model_structure (model.structure)
#' @param .indicator_labels (data.frame) produced by read_labels
#' @return (flextable)
report_key_indicators_table <- function(.scenario_data,.model_structure,.indicator_labels){
  
  
  
  .dimensions_to_labels <- c("EXPOSURE"="Ecological vulnerability",
                             "ECOLOGICAL VULNERABILITY"="Ecological vulnerability",
                             "ECOLOGICAL SENSITIVITY"="Ecological vulnerability",
                             "SOCIAL ADAPTIVE CAPACITY"="Adaptive capacity",
                             "SOCIAL SENSITIVITY"="Social sensitivity")
  
  
  
  .indicators <- .model_structure %>% 
    map(~.x%>%  set_names(NULL) %>% flatten  %>% set_names(NULL) %>%  flatten %>% names %>% as.list) %>% 
    map2(names(.),~{
      
      rep(.y,length(.x)) %>% set_names(.x)
    }) %>% reduce(c)
  
  .indicators <- .dimensions_to_labels[.indicators] %>% set_names(names(.indicators))
  
  .to.plot <- .scenario_data$MPA_indicators_contributions$contributions[[1]]$Vulnerability %>% 
    pivot_longer(cols=everything(),names_to = "Indicator",values_to="score") %>%
    mutate(group=.indicators[Indicator]) %>% arrange(desc(score)) %>% slice(1:10) 
  
  .to.plot %<>% left_join(.indicator_labels,by=c("Indicator"="name")) %>% 
    select(Label=label,score,group)%>% mutate(Label=factor(Label,levels=Label)) %>% 
    mutate(score=sprintf("%0.3f",round(score,3)))
  
  .out <- mpa_engage_table(.to.plot) %>% 
    set_header_labels(Label="Indicator",score="Contribution to vulnerability",group="Dimension") %>%
    align(j=2:3,align = "center",part="all") %>%
    align(j=1,align = "left")
  
  .out
}

#' Returns a flextable with all the ecological vulnerability donut graph data for
#' one scenario
#' 
#' @param .scenario_data (list of data.frame) scenario data computed by compute scenario
#' @return (flextable)
report_ecological_vuln_table <- function(.scenario_data){
  
  .labels_equiv <- c("ECOLOGICAL SENSITIVITY"="Ecological sensitivity","EXPOSURE"="Exposure","ECOLOGICAL ADAPTIVE CAPACITY"="Adaptive capacity","ECOLOGICAL VULNERABILITY"="Ecological vulnerability")
  
  
  #  scores
  
  .scores_df <- .scenario_data$MPA_index %>% select(`ECOLOGICAL SENSITIVITY`,`ECOLOGICAL ADAPTIVE CAPACITY`,`EXPOSURE`,`ECOLOGICAL VULNERABILITY`) %>%
    pivot_longer(cols=everything(),names_to="bar_title",values_to="score")
  
  .scores_df %<>% mutate(bar_title=.labels_equiv[bar_title],score=sprintf("%0.2f",round(score,1)))
  
  #  coverage
  
  .coverage_df <- .scenario_data$MPA_coverage %>% select(`ECOLOGICAL SENSITIVITY`,`ECOLOGICAL ADAPTIVE CAPACITY`,`EXPOSURE`,`ECOLOGICAL VULNERABILITY`) %>%
    pivot_longer(cols=everything(),names_to="bar_title",values_to="coverage")
  
  
  .coverage_df %<>% mutate(bar_title=.labels_equiv[bar_title],coverage=scales::percent(coverage,accuracy = 1))
  
  #  years
  
  .years_df <- .scenario_data$MPA_years %>% select(`ECOLOGICAL SENSITIVITY`,`ECOLOGICAL ADAPTIVE CAPACITY`,`EXPOSURE`,`ECOLOGICAL VULNERABILITY`) %>%
    pivot_longer(cols=everything(),names_to="bar_title",values_to="years")
  
  
  .years_df %<>% mutate(bar_title=.labels_equiv[bar_title],years=sprintf("%d",as.integer(years)))
  
  # Quantitative perc
  
  .quantitative_perc_df <- .scenario_data$MPA_quantitative_percent %>% select(`ECOLOGICAL SENSITIVITY`,`ECOLOGICAL ADAPTIVE CAPACITY`,`EXPOSURE`,`ECOLOGICAL VULNERABILITY`) %>%
    pivot_longer(cols=everything(),names_to="bar_title",values_to="quantitative")
  
  
  .quantitative_perc_df %<>% mutate(bar_title=.labels_equiv[bar_title],quantitative=scales::percent(quantitative,accuracy = 1))
  
  # Join dfs
  
  .to_plot <- .scores_df%>% left_join(.coverage_df,by="bar_title") %>%  left_join(.years_df,by="bar_title") %>% left_join(.quantitative_perc_df,by="bar_title")
  
  .to_plot %<>% mutate(across(everything(),~str_replace(.,"NA","-")))
  
  # Plot table
  
  .out <- mpa_engage_table(.to_plot) %>% 
    set_header_labels(bar_title=" ",score="Vulnerability",coverage="Data\ncoverage",years="Years",quantitative="Quantitative\npercentage") %>%
    align(j=2:5,align = "center",part="all") %>%
    align(j=1,align = "left")
  
  .out
}

#' Returns a flextable with all the social-ecological donut graph data for
#' one scenario
#' 
#' @param .scenario_data (list of data.frame) scenario data computed by compute scenario
#' @return (flextable)
report_social_ecological_table <- function(.scenario_data){
  
  .labels_equiv <- c("SOCIAL SENSITIVITY"="Social sensitivity","ECOLOGICAL VULNERABILITY"="Ecological vulnerability","SOCIAL ADAPTIVE CAPACITY"="Adaptive capacity","Vulnerability"="Vulnerability index")
  
  
  #  scores
  
  .scores_df <- .scenario_data$MPA_index %>% select(`SOCIAL SENSITIVITY`,`SOCIAL ADAPTIVE CAPACITY`,`ECOLOGICAL VULNERABILITY`,Vulnerability) %>%
    pivot_longer(cols=everything(),names_to="bar_title",values_to="score")
  
  .scores_df %<>% mutate(bar_title=.labels_equiv[bar_title],score=sprintf("%0.2f",round(score,1)))
  
  #  coverage
  
  .coverage_df <- .scenario_data$MPA_coverage %>% select(`SOCIAL SENSITIVITY`,`SOCIAL ADAPTIVE CAPACITY`,`ECOLOGICAL VULNERABILITY`,Vulnerability) %>%
    pivot_longer(cols=everything(),names_to="bar_title",values_to="coverage")
  
  
  .coverage_df %<>% mutate(bar_title=.labels_equiv[bar_title],coverage=scales::percent(coverage,accuracy = 1))
  
  #  years
  
  .years_df <- .scenario_data$MPA_years %>% select(`SOCIAL SENSITIVITY`,`SOCIAL ADAPTIVE CAPACITY`,`ECOLOGICAL VULNERABILITY`,Vulnerability) %>%
    pivot_longer(cols=everything(),names_to="bar_title",values_to="years")
  
  
  .years_df %<>% mutate(bar_title=.labels_equiv[bar_title],years=sprintf("%d",as.integer(years)))
  
  # Quantitative perc
  
  .quantitative_perc_df <- .scenario_data$MPA_quantitative_percent %>% select(`SOCIAL SENSITIVITY`,`SOCIAL ADAPTIVE CAPACITY`,`ECOLOGICAL VULNERABILITY`,Vulnerability) %>%
    pivot_longer(cols=everything(),names_to="bar_title",values_to="quantitative")
  
  
  .quantitative_perc_df %<>% mutate(bar_title=.labels_equiv[bar_title],quantitative=scales::percent(quantitative,accuracy = 1))
  
  # Join dfs
  
  .to_plot <- .scores_df%>% left_join(.coverage_df,by="bar_title") %>%  left_join(.years_df,by="bar_title") %>% left_join(.quantitative_perc_df,by="bar_title")
  
  .to_plot %<>% mutate(across(everything(),~str_replace(.,"NA","-")))
  
  # Plot table
  
  .out <- mpa_engage_table(.to_plot) %>% 
    set_header_labels(bar_title=" ",score="Vulnerability",coverage="Data\ncoverage",years="Years",quantitative="Quantitative\npercentage") %>%
    align(j=2:5,align = "center",part="all") %>%
    align(j=1,align = "left")
  
  .out
}

#' Creates a MS Office report for the scenario data and saves it in the path provided.
#' It uses a template.
#' 
#' @param .outfile (character) path forthe output file
#' @param .template_path (character) path for the doc template
#' @param .scenario_data (list of data.frame) scenario data computed by compute scenario
#' @param .model_structure (model.structure)
#' @param species_groups (data.frame)
#' @param .indicator_labels (data.frame) produced by read_labels
#' @return NULL
scenario_report <- function(.outfile,
                            .template_path,
                            .scenario_data,
                            .model_structure,
                            .species_groups,
                            .indicator_labels,
                            .icons
){
  
  .donut_graph_params <- list(.donut_graph_central_score_size=30,
                              .donut_graph_central_score_subtitle_size=12,
                              .donut_graph_bar_labels_size=8,
                              .donut_graph_lengend_text_size=16,
                              .donut_graph_legend_spacing=0.2)
  
  
  .width <- 8
  .height <- 5.1
  .res <- 300
  
  .out <- read_docx(.template_path)
  
  # Title
  
  
  .out %<>% cursor_bookmark("rep_scenario_title")
  .out %<>%  body_replace_all_text(old_value  = "rep_scenario",new_value  =toupper(.scenario_data$scenario))
  
  
  # Social-ecological vulnerability donut graph
  
  filepath <- tempfile(fileext = ".png")
  
  app_output_social_ecological_donut_graph(.data=.scenario_data,
                                           .filepath=filepath,
                                           .width=.width,
                                           .height=.height,
                                           .res=.res,
                                           .donut_graph_params=.donut_graph_params)
  
  .out %<>% cursor_bookmark("rep_image_social_ecological_vul")
  
  .out %<>% body_add_img(filepath,width=7,height=4.5,pos = "on")  
  
  
  .social_ecological_donut_graph_table <- report_social_ecological_table(.scenario_data) %>% autofit()
  
  .out %<>% cursor_bookmark("rep_table_social_ecological_vul")
  
  .out %<>% body_add_flextable(.social_ecological_donut_graph_table,pos="on")
  
  # Ecological vulnerability donut graphs
  
  .temp_filepath <- tempfile(fileext = ".png")
  
  app_output_ecological_vuln_donut_graph(.data=.scenario_data,
                                         .filepath=.temp_filepath,
                                         .width=.width,
                                         .height=.height,
                                         .res=.res,
                                         .donut_graph_params=.donut_graph_params)
  
  .out %<>% cursor_bookmark("rep_image_ecological_vul")
  
  .out %<>% body_add_img(.temp_filepath,width=7,height=4.5,pos = "on")  
  
  
  .ecological_vuln_donut_graph_table <- report_ecological_vuln_table(.scenario_data) %>% autofit()
  
  .out %<>% cursor_bookmark("rep_table_ecological_vul")
  
  .out %<>% body_add_flextable(.ecological_vuln_donut_graph_table,pos="on")
  
  
  # Key indicators  graph
  
  .temp_filepath <- tempfile(fileext = ".png")
  
  app_output_key_indicators_graph(.data=.scenario_data,
                                  .filepath=.temp_filepath,
                                  .model_structure = .model_structure,
                                  .indicator_labels = .indicator_labels,
                                  .width=.width,
                                  .height=.height,
                                  .res=.res)
  
  .out %<>% cursor_bookmark("rep_image_key_indicators")
  
  .out %<>% body_add_img(.temp_filepath,width=7,height=4.5,pos = "on")  
  
  
  .key_indicators_graph_table <- report_key_indicators_table(.scenario_data,.model_structure = .model_structure,.indicator_labels = .indicator_labels) %>% autofit()
  
  .out %<>% cursor_bookmark("rep_table_key_indicators")
  
  .out %<>% body_add_flextable(.key_indicators_graph_table,pos="on")
  
  # Habitat vuln graph
  
  .temp_filepath <- tempfile(fileext = ".png")
  
  app_output_habitat_vuln_graph(.data=.scenario_data,
                                .filepath=.temp_filepath,
                                .width=.width,
                                .height=.height,
                                .res=.res,
                                .icons = .icons)
  
  .out %<>% cursor_bookmark("rep_image_habitat_vul")
  
  .out %<>% body_add_img(.temp_filepath,width=7,height=4.5,pos = "on")  
  
  
  .habitat_vuln_graph_table <- report_habitat_vul_table(.scenario_data) %>% autofit()
  
  .out %<>% cursor_bookmark("rep_table_habitat_vul")
  
  .out %<>% body_add_flextable(.habitat_vuln_graph_table,pos="on")
  
  # Species vuln graph
  
  .temp_filepath <- tempfile(fileext = ".png")
  
  app_output_species_vuln_graph(.data=.scenario_data,
                                .filepath=.temp_filepath,
                                .width=.width,
                                .height=.height,
                                .res=.res,
                                .icons = .icons)
  
  .out %<>% cursor_bookmark("rep_image_species_vul")
  
  .out %<>% body_add_img(.temp_filepath,width=7,height=4.5,pos = "on") 
  
  .species_vuln_graph_table <- report_species_vul_table(.scenario_data) %>% autofit()
  
  .out %<>% cursor_bookmark("rep_table_species_vul")
  
  .out %<>% body_add_flextable(.species_vuln_graph_table,pos="on")
  
  
  # Species groups graph
  
  .temp_filepath <- tempfile(fileext = ".png")
  
  app_output_species_groups_vuln_graph(.data=.scenario_data,
                                       .filepath=.temp_filepath,
                                       .species_groups = .species_groups,
                                       .width=.width,
                                       .height=.height,
                                       .res=.res,
                                       .icons = .icons)
  
  .out %<>% cursor_bookmark("rep_image_species_groups_vul")
  
  .out %<>% body_add_img(.temp_filepath,width=7,height=4.5,pos = "on") 
  
  
  .species_groups_vuln_graph_table <- report_species_groups_vul_table(.scenario_data,.species_groups = .species_groups) %>% autofit()
  
  .out %<>% cursor_bookmark("rep_table_species_groups_vul")
  
  .out %<>% body_add_flextable(.species_groups_vuln_graph_table,pos="on")
  
  
  # User groups graph
  
  .temp_filepath <- tempfile(fileext = ".png")
  
  app_output_user_groups_vuln_graph(.data=.scenario_data,
                                    .filepath=.temp_filepath,
                                    .width=.width,
                                    .height=.height,
                                    .res=.res,
                                    .icons = .icons)
  
  .out %<>% cursor_bookmark("rep_image_user_groups_vul")
  
  .out %<>% body_add_img(.temp_filepath,width=7,height=4.5,pos = "on") 
  
  
  .user_groups_vuln_graph_table <- report_user_groups_vul_table(.scenario_data) %>% autofit()
  
  .out %<>% cursor_bookmark("rep_table_user_groups_vul")
  
  .out %<>% body_add_flextable(.user_groups_vuln_graph_table,pos="on")
  
  
  # Data issues
  
  if(length(.scenario_data$log) >0){
    
    .out %<>% cursor_bookmark("rep_data_issues")
    
    .out %<>% body_add_par(.scenario_data$log[1],pos = "on",style="bullet")  
    
    if(length(.scenario_data$log) >1){
      for(issue in .scenario_data$log[2:length(.scenario_data$log)]){
        
        .out %<>% body_add_par(issue,pos = "after",style="bullet")  
        
      }
    }
    
  }else{
    
    .out %<>% cursor_bookmark("rep_data_issues")
    
    .out %<>% body_add_par("No issues found",pos = "on")  
    
    
  }
  
  
  # Save
  
  .out %>% print(target=.outfile)
  
  
  NULL
}



#' Creates a MS Office report for the scenario data and saves it in the path provided.
#' It uses a template.
#' 
#' @param .outfile (character) path forthe output file
#' @param .template_path (character) path for the doc template
#' @param .scenario_data (list of data.frame) scenario data computed by compute scenario
#' @param .model_structure (model.structure)
#' @param species_groups (data.frame)
#' @param .indicator_labels (data.frame) produced by read_labels
#' @return NULL
general_report <- function(.outfile,
                           .template_path,
                           .scenarios_data,
                           .model_structure,
                           .species_groups,
                           .indicator_labels,
                           .icons
){
  
  
  
  .grid_titles <- names(.scenarios_data) %>% map(~str_match(.x,"(\\d\\.\\d).*(\\d{4})") %>% as.data.frame) %>% bind_rows() %>% select(2:3) %>% set_names(c("RCP","Year"))
  
  .RCP_2_8_2100 <- .scenarios_data[[which(.grid_titles$RCP=="8.5" & .grid_titles$Year == "2100")]]
  
  .donut_graph_params <- list(.donut_graph_central_score_size=14,
                              .donut_graph_central_score_subtitle_size=6,
                              .donut_graph_bar_labels_size=4,
                              .donut_graph_lengend_text_size=16,
                              .donut_graph_legend_spacing=0.1)
  
  .width <- 8
  .height <- 6
  .res <- 300
  
  .out <- read_docx(.template_path)
  
  
  
  # Social-ecological vulnerability donut graph
  
  filepath <- tempfile(fileext = ".png")
  
  app_output_social_ecological_donut_graph_scenarios(.data=.scenarios_data,
                                                     .filepath=filepath,
                                                     .width=.width,
                                                     .height=.height,
                                                     .res=.res,
                                                     .donut_graph_params=.donut_graph_params)
  
  .out %<>% cursor_bookmark("rep_image_social_ecological_vul")
  
  .out %<>% body_add_img(filepath,width=5.728346,height=4.212598,pos = "on")  
  
  
  
  
  # Ecological vulnerability donut graphs
  
  .temp_filepath <- tempfile(fileext = ".png")
  
  app_output_ecological_vulnerability_donut_graph_scenarios(.data=.scenarios_data,
                                                            .filepath=.temp_filepath,
                                                            .width=.width,
                                                            .height=.height,
                                                            .res=.res,
                                                            .donut_graph_params=.donut_graph_params)
  
  .out %<>% cursor_bookmark("rep_image_ecological_vul")
  
  .out %<>% body_add_img(.temp_filepath,width=5.728346,height=4.212598,pos = "on")  
  
  if(!is.null(.RCP_2_8_2100)){
    
    .height <- 5.1
    
    # Key indicators  graph
    
    .temp_filepath <- tempfile(fileext = ".png")
    
    app_output_key_indicators_graph(.data=.RCP_2_8_2100,
                                    .filepath=.temp_filepath,
                                    .model_structure = .model_structure,
                                    .indicator_labels = .indicator_labels,
                                    .width=.width,
                                    .height=.height,
                                    .res=.res)
    
    .out %<>% cursor_bookmark("rep_image_key_indicators")
    
    .out %<>% body_add_img(.temp_filepath,width=4.594488,height=2.952756,pos = "on")  
    
    
    
    # Habitat vuln graph
    
    .temp_filepath <- tempfile(fileext = ".png")
    
    app_output_habitat_vuln_graph(.data=.RCP_2_8_2100,
                                  .filepath=.temp_filepath,
                                  .width=.width,
                                  .height=.height,
                                  .res=.res,
                                  .icons = .icons)
    
    .out %<>% cursor_bookmark("rep_image_habitat_vul")
    
    .out %<>% body_add_img(.temp_filepath,width=4.594488,height=2.952756,pos = "on")  
    
    
    
    # Species vuln graph
    
    .temp_filepath <- tempfile(fileext = ".png")
    
    app_output_species_vuln_graph(.data=.RCP_2_8_2100,
                                  .filepath=.temp_filepath,
                                  .width=11,
                                  .height=7,
                                  .res=.res,
                                  .icons = .icons)
    
    .out %<>% cursor_bookmark("rep_image_species_vul")
    
    .out %<>% body_add_img(.temp_filepath,width=5.74311,height=4.052802353,pos = "on") 
    
    
    # User groups graph
    
    .temp_filepath <- tempfile(fileext = ".png")
    
    app_output_user_groups_vuln_graph(.data=.RCP_2_8_2100,
                                      .filepath=.temp_filepath,
                                      .width=.width,
                                      .height=.height,
                                      .res=.res,
                                      .icons = .icons)
    
    .out %<>% cursor_bookmark("rep_image_user_groups_vul")
    
    .out %<>% body_add_img(.temp_filepath,width=4.594488,height=2.952756,pos = "on") 
    
  }
  
  # Hazards
  
  .height <- 7
  
  filepath <- tempfile(fileext = ".png")
  
  app_output_social_ecological_donut_graph_scenarios_by_HZ(.data=.scenarios_data,
                                                           .filepath=filepath,
                                                           .width=.width,
                                                           .height=.height,
                                                           .res=.res,
                                                           .donut_graph_params=.donut_graph_params)
  
  .out %<>% cursor_bookmark("rep_image_hazard_vul")
  
  .out %<>% body_add_img(filepath,width=4.826772,height=4.224409,pos = "on")  
  
  # Save
  
  .out %>% print(target=.outfile)
  
  
  NULL
}


zip_report <-  function(.zip_file_path,.data,.model_structure,.species_groups,.indicator_labels,.template_path,.img_width=8,.img_height=5.1,.res=300,.icons=NULL,updateProgress=NULL){
  
  
  
  .temp_dir <-tempfile(pattern = "tempdir",tmpdir = ".")
  
  on.exit(unlink(.temp_dir,recursive = TRUE))
  
  .base_path <- file.path(.temp_dir,"MPA_report")
  
  unlink(.base_path,recursive = TRUE)
  if(!dir.exists(.base_path)){
    dir.create(.base_path,recursive = TRUE)
  }
  
  
  
  if(is_function(updateProgress)){
    text <- paste0("Processing report document")
    updateProgress(value=1/(length(.data)+1) ,detail=text)
  }
  .doc_filepath <- file.path(.base_path,"MPA Vulnerability Report.docx")
  
  suppressWarnings(general_report(.outfile=.doc_filepath,
                                  .template_path=.template_path,
                                  .scenarios_data=.data,
                                  .model_structure=.model_structure,
                                  .species_groups=.species_groups,
                                  .indicator_labels=.indicator_labels,
                                  .icons=.icons
  ))
  
  
  .donut_graph_params <- list(.donut_graph_central_score_size=30,
                              .donut_graph_central_score_subtitle_size=12,
                              .donut_graph_bar_labels_size=8,
                              .donut_graph_lengend_text_size=16,
                              .donut_graph_legend_spacing=0.2)
  
  
  .data %>% compact %>% walk2(1:length(.),~{
    
    if(is_function(updateProgress)){
      text <- paste0("Processing graphs for scenario: ",.x$scenario)
      updateProgress(value=.y/(length(.data)+1) ,detail=text)
    }
    
    .image_path <- file.path(.base_path,"graphs",.x$scenario)
    
    if(!dir.exists(.image_path)){
      dir.create(.image_path,recursive = TRUE)
    }
    
    .img_path <- file.path(.image_path,"ecological vulnerability.png")
    
    app_output_ecological_vuln_donut_graph(.data=.x,
                                           .filepath=.img_path,
                                           .width=.img_width,
                                           .height=.img_height,
                                           .res=.res,
                                           .donut_graph_params=.donut_graph_params)
    
    
    .img_path <- file.path(.image_path,"social ecological vulnerability.png")
    
    app_output_social_ecological_donut_graph(.data=.x,
                                             .filepath=.img_path,
                                             .width=.img_width,
                                             .height=.img_height,
                                             .res=.res,
                                             .donut_graph_params=.donut_graph_params)
    
    
    .img_path <- file.path(.image_path,"habitat vulnerability.png")
    
    app_output_habitat_vuln_graph(.data=.x,
                                  .filepath=.img_path,
                                  .width=.img_width,
                                  .height=.img_height,
                                  .res=.res,
                                  .icons = .icons)
    
    
    .img_path <- file.path(.image_path,"key indicators.png")
    
    app_output_key_indicators_graph(.data=.x,
                                    .filepath=.img_path,
                                    .model_structure = .model_structure,
                                    .indicator_labels=.indicator_labels,
                                    .width=.img_width,
                                    .height=.img_height,
                                    .res=.res)
    
    
    .img_path <- file.path(.image_path,"species groups vulnerability.png")
    
    app_output_species_groups_vuln_graph(.data=.x,
                                         .filepath=.img_path,
                                         .species_groups = .species_groups,
                                         .width=.img_width,
                                         .height=.img_height,
                                         .res=.res,
                                         .icons = .icons)
    
    .img_path <- file.path(.image_path,"species vulnerability.png")
    
    
    app_output_species_vuln_graph(.data=.x,
                                  .filepath=.img_path,
                                  .width=.img_width,
                                  .height=.img_height,
                                  .res=.res,
                                  .icons = .icons)
    
    
    .img_path <- file.path(.image_path,"user groups vulnerability.png")
    
    app_output_user_groups_vuln_graph(.data=.x,
                                      .filepath=.img_path,
                                      .width=.img_width,
                                      .height=.img_height,
                                      .res=.res,
                                      .icons = .icons)
    
    
  })
  
  
  
  cur_dir <- getwd()
  on.exit(setwd(cur_dir))
  
  setwd(.temp_dir)
  
  .zip_file <- "report.zip"
  
  
  .files <- list.files("MPA_report",recursive = TRUE,full.names = TRUE)
  
  # .files <- str_remove(.files,paste0(.temp_dir,"/"))
  zip(zipfile = .zip_file,files = .files  )
  
  setwd(cur_dir)
  
  file.copy(file.path(.temp_dir,.zip_file),.zip_file_path)  
  
  
}
##### CONDITION HANDLING #####




condition <- function(subclass, message, call = sys.call(-1), ...) {
  structure(
    class = c(subclass, "condition"),
    list(message = message, call = call, ...)
  )
}

weight_missing_in_model_definition <- function(.missing_weights){
  
  msg <- paste0("Weight for '",paste0(.missing_weights,collapse = ","),"' missing. Please check your model definition file.")
  condition(c("weight_missing_in_model_definition","error"),
            message = msg,
            missing_weights=.missing_weights)
  
  
}


normalization_limits_missing_error <- function(.missing_limits){
  
  msg <- paste0("Normalization limit for '",paste0(.missing_limits,collapse = ","),"' missing. Please check your model definition file.")
  condition(c("normalization_limits_missing_error","error"),
            message = msg,
            missing_limits=.missing_limits)
  
  
}


normalization_limits_missing_warning <- function(.missing_limits,.scenario){
  
  msg <- paste0("No normalization definitions for indicators '",paste0(.missing_limits,collapse = ","),"'. Please check your data.")
  condition(c("normalization_limits_missing_warning","warning"),
            message = msg,
            missing_limits=.missing_limits,
            scenario=.scenario)
  
  
}
species_not_listed_in_model <- function(.unknown_species){
  
  msg <- paste0("Species '",paste0(.unknown_species,collapse = ","),"' found, but not listed in our model Please check your data.")
  condition(c("species_not_listed_in_model","warning"),
            message = msg,
            species=.unknown_species)
  
  
}

user_groups_not_listed_in_model <- function(.unknown_user_groups){
  
  msg <- paste0("User group(s) '",paste0(.unknown_user_groups,collapse = ","),"' found, but not listed in our model. Please check your data.")
  condition(c("user_groups_not_listed_in_model","warning"),
            message = msg,
            species=.unknown_user_groups)
  
  
}

habitats_not_listed_in_model <- function(.unknown_habitats){
  
  msg <- paste0("Habitat(s) '",paste0(.unknown_habitats,collapse = ","),"' found, but not listed in our model. Please check your data.")
  condition(c("habitats_not_listed_in_model","warning"),
            message = msg,
            species=.unknown_habitats)
  
  
}



read_model_definition_normalization_sheet_error <- function(.file,.cond){
  
  msg <- paste0("Error reading normalization sheet on file: ",.file,"\n",.cond)
  condition(c("read_model_definition_normalization_sheet_error","error"),
            message = msg,
            file=.file)
  
}
# Recoveries
return_empty_normalization_data <- function(.file) invokeRestart("return_empty_normalization_data",.file)

empty_normalization_data <- data.frame(name=character(0),min=integer(0),max=integer(0),rev=integer(0),quantitative=integer(0),stringsAsFactors = FALSE)


normalization.definitions.data.frame_not_valid <- function(.msg,.type,.df){
  msg <- paste0("Normalization definitions not valid: ",.msg)
  condition(c("normalization.definitions.data.frame_not_valid"),
            message = msg,
            normalization.definitions.data.frame_not_valid_type=.type,
            df=.df)
}

normalization.definitions.data.frame_columns_missing_error <- function(.msg,.type,.df){
  msg <- paste0("Normalization definitions not valid: ",.msg)
  condition(c("normalization.definitions.data.frame_columns_missing_error","error"),
            message = msg,
            normalization.definitions.data.frame_not_valid_type=.type,
            df=.df)
}


use_value <- function(x) invokeRestart("use_value",x)

fix_normalization.definitions.data.frame_columns <- function(.df) invokeRestart("fix_normalization.definitions.data.frame_columns",.df)

fix_normalization.definitions.data.frame_columns_classes <- function(.df) invokeRestart("fix_normalization.definitions.data.frame_columns_classes",.df)

normalization_factor_values_empty <- function(.df){
  msg <- paste0("No values for normalization factors where found.")
  
  
  condition(c("normalization_factor_values_empty","warning"),
            message = msg,
            df=.df)
}

return_empty_list <- function() invokeRestart("return_empty_list")



computing_function_column_does_not_exist <- function(.df,.col_definition,.col_name,.computing_function){
  
  msg <- paste0("Column ",.col_name," not found when using ",.computing_function)
  condition(c("computing_function_column_does_not_exist"),
            message=msg,
            df=.df,
            col_definition=.col_definition,
            computing_function=.computing_function,
            missing_column=.col_name
  )
  
}



quality_indicators_group_display_wrong_number_of_indicators_error <- function(.n_indicators_required,.indicators_display_list){
  
  msg <- paste0("quality_indicators_group_display requires ",.n_indicators_required,", but only found",length(.indicators_display_list))
  condition(c("quality_indicators_group_display_wrong_number_of_indicators_error","error"),
            message=msg,
            n_indicators_required=.n_indicators_required,
            indicators_display_list=.indicators_display_list
  )
  
}


read_MPA_unknown_indicator <- function(.indicator){
  msg <- paste0("Indicator '",.indicator,"' found in data file, but not defined in model. Removing.")
  
  condition(c("read_MPA_unknown_indicator","warning"),
            message=msg,
            indicator=.indicator
            
  )
  
}


read_MPA_scale_missing_for_indicator <- function(.indicator,.scales){
  msg <- paste0("According to the model structure, values in indicator ",.indicator," are expected to depend on the scale ",paste0(.scales,collapse=","),", but some/all do not. Please check your data and model definition.")
  
  condition(c("read_MPA_scale_missing_for_indicator","warning"),
            message=msg,
            indicator=.indicator,
            scales=.scales
            
  )
}


read_MAP_unexpected_scale_for_indicator <- function(.indicator,.scales){
  
  msg <- paste0("According to the model structure, values in indicator ",.indicator," should not depend on the scale ",paste0(.scales,collapse=","),", dropping that scale in that column.")
  
  condition(c("read_MAP_unexpected_scale_for_indicator","warning"),
            message=msg,
            indicator=.indicator,
            scales=.scales
            
  )
  
}


indicator_factors_to_numeric_indicator_missing <- function(.indicator){
  
  msg <- paste0("indicator_factors_to_numeric: Column missing ",.indicator)
  
  condition(c("indicator_factors_to_numeric_indicator_missing","warning"),
            message=msg,
            indicator=.indicator
            
  )
  
}


unexpected_qualitative_values_for_indicator <- function(.indicator,.values){
  msg <- paste0("'",paste0(.values,collapse = ","),"' did not match any factor normalization definition for column ",.indicator,". Plase check your data and your normalization factor definitions.")
  
  condition(c("unexpected_qualitative_values_for_indicator","warning"),
            message=msg,
            indicator=.indicator,
            values=.values
            
  )
}


data_outside_normalization_limits <- function(.indicator,.values,.range){
  msg <- paste0("Data in column ",.indicator," is outside its normalization limits.")
  
  condition(c("data_outside_normalization_limits","warning"),
            message=msg,
            indicator=.indicator,
            values=.values,
            range=.range
            
            
  )
}




data_file_column_missing <- function(.data_file,.columns,.scenario){
  
  msg <- paste0("Column(s) ",paste0(.columns,collapse=", ")," missing in scenario '",.scenario,"'")
  
  condition(c("data_file_column_missing","warning"),
            message=msg,
            data_file=.data_file,
            columns=.columns,
            scenario=.scenario
            
            
  )
}


unknown_error_processing_scenario <- function(.scenario,.cnd){
  
  msg <- paste0("An unknown error happenend while processing scenario '",.scenario,"'. Skipping.\\n Error:",as.character(.cnd$message))
  
  condition(c("unknown_error_processing_scenario","warning"),
            message=msg,
            scenario=.scenario
            
            
  )
}


# Handlers

return_nas <- function() invokeRestart("return_nas")

use_nas <- function() invokeRestart("use_nas")

use_zeros <- function() invokeRestart("use_zeros")

use_constant <- function(x) invokeRestart("use_constant",x)

drop_column <- function(cond,...) invokeRestart("drop_column",cond,...)

cut_to_range <- function(cond) invokeRestart("cut_to_range",cond)

new_layout <- function(.cnd,.indicators_display_list,.layout,...) invokeRestart("new_layout",.cnd,.indicators_display_list,.layout,...)
